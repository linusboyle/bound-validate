@inproceedings{bourke_formally_2017,
	abstract = {The correct compilation of block diagram languages like Lustre, Scade, and a discrete subset of Simulink is important since they are used to program critical embedded control software. We describe the specification and verification in an Interactive Theorem Prover of a compilation chain that treats the key aspects of Lustre: sampling, nodes, and delays. Building on CompCert, we show that repeated execution of the generated assembly code faithfully implements the dataflow semantics of source programs.   We resolve two key technical challenges. The first is the change from a synchronous dataflow semantics, where programs manipulate streams of values, to an imperative one, where computations manipulate memory sequentially. The second is the verified compilation of an imperative language with encapsulated state to C code where the state is realized by nested records. We also treat a standard control optimization that eliminates unnecessary conditional statements.},
	address = {New York, NY, USA},
	author = {Bourke, Timothy and Brun, L{\'e}lio and Dagand, Pierre-{\'E}variste and Leroy, Xavier and Pouzet, Marc and Rieg, Lionel},
	booktitle = {Proceedings of the 38th {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	doi = {10.1145/3062341.3062358},
	file = {BourkeT et al-A Formally Verified Compiler for Lustre.pdf:/home/linusboyle/Zotero/storage/8CPXEMF8/BourkeT et al-A Formally Verified Compiler for Lustre.pdf:application/pdf},
	isbn = {978-1-4503-4988-8},
	keywords = {Interactive Theorem Proving (Coq), Synchronous Languages (Lustre), Verified Compilation, Lustre, Velus},
	note = {event-place: Barcelona, Spain},
	pages = {586--601},
	publisher = {ACM},
	series = {{PLDI} 2017},
	title = {A {Formally} {Verified} {Compiler} for {Lustre}},
	url = {http://doi.acm.org/10.1145/3062341.3062358},
	urldate = {2019-06-29},
	year = {2017}
}

@inproceedings{caspi_lustre:_1987,
	abstract = {LUSTRE is a synchronous data-flow language for programming systems which interact with their environments in real-time. After an informal presentation of the language, we describe its semantics by means of structural inference rules. Moreover, we show how to use this semantics in order to generate efficient sequential code, namely, a finite state automaton which represents the control of the program. Formal rules for program transformation are also presented.},
	address = {New York, NY, USA},
	author = {Caspi, P. and Pilaud, D. and Halbwachs, N. and Plaice, J. A.},
	booktitle = {Proceedings of the 14th {ACM} {SIGACT}-{SIGPLAN} {Symposium} on {Principles} of {Programming} {Languages}},
	doi = {10.1145/41625.41641},
	file = {CaspiP et al-LUSTRE - A declarative language for programming synchronous systems.pdf:/home/linusboyle/Zotero/storage/SMRMXJSR/CaspiP et al-LUSTRE - A declarative language for programming synchronous systems.pdf:application/pdf},
	isbn = {978-0-89791-215-0},
	keywords = {Lustre},
	note = {event-place: Munich, West Germany},
	pages = {178--188},
	publisher = {ACM},
	series = {{POPL} '87},
	shorttitle = {{LUSTRE}},
	title = {{LUSTRE}: {A} declarative language for programming synchronous systems},
	url = {http://doi.acm.org/10.1145/41625.41641},
	urldate = {2019-06-29},
	year = {1987}
}

@inproceedings{krishnaswami_ultrametric_2011,
	abstract = {We describe a denotational model of higher-order functional reactive programming using ultra metric spaces and non expansive maps, which provide a natural Cartesian closed generalization of causal stream functions and guarded recursive definitions. We define a type theory corresponding to this semantics and show that it satisfies normalization. Finally, we show how to efficiently implement reactive programs written in this language using an imperatively updated data flow graph, and give a separation logic proof that this low-level implementation is correct with respect to the high-level semantics.},
	author = {Krishnaswami, N. R. and Benton, N.},
	booktitle = {2011 {IEEE} 26th {Annual} {Symposium} on {Logic} in {Computer} {Science}},
	doi = {10.1109/LICS.2011.38},
	file = {KrishnaswamiN&BentonN-Ultrametric Semantics of Reactive Programs.html:/home/linusboyle/Zotero/storage/4RECX6SY/KrishnaswamiN&BentonN-Ultrametric Semantics of Reactive Programs.html:text/html; KrishnaswamiN&BentonN-Ultrametric Semantics of Reactive Programs.pdf:/home/linusboyle/Zotero/storage/JI94HA2N/KrishnaswamiN&BentonN-Ultrametric Semantics of Reactive Programs.pdf:application/pdf},
	keywords = {Bismuth, Calculus, causal stream function, data flow graphs, dataflow graph, Delay, functional programming, high level semantics, higher order functional reactive programming, low level implementation, Mathematical model, natural cartesian closed generalization, nonexpansive map, reactive program, Semantics, separation logic proof, Syntactics, type theory, ultrametric semantics},
	month = jun,
	pages = {257--266},
	title = {Ultrametric {Semantics} of {Reactive} {Programs}},
	year = {2011}
}

@inproceedings{r._krishnaswami_semantic_2011,
	abstract = {We give a denotational model for graphical user interface (GUI) programming using the Cartesian closed category of ultrametric spaces. The ultrametric structure enforces causality restrictions on reactive systems and allows well-founded recursive definitions by a generalization of guardedness. We capture the arbitrariness of user input (e.g., a user gets to decide the stream of clicks she sends to a program) by making use of the fact that the closed subsets of an ultrametric space themselves form an ultrametric space, allowing us to interpret nondeterminism with a "powerspace" monad.
Algebras for the powerspace monad yield a model of intuitionistic linear logic, which we exploit in the definition of a mixed linear/non-linear domain-specific language for writing GUI programs. The non-linear part of the language is used for writing reactive stream-processing functions whilst the linear sublanguage naturally captures the generativity and usage constraints on the various linear objects in GUIs, such as the elements of a DOM or scene graph.
We have implemented this DSL as an extension to OCaml, and give examples demonstrating that programs in this style can be short and readable.},
	author = {{R. Krishnaswami}, Neelakantan and Benton, Nick},
	doi = {10.1145/2034574.2034782},
	file = {R. KrishnaswamiN&BentonN-A Semantic Model for Graphical User Interfaces.pdf:/home/linusboyle/Zotero/storage/Z7S69J9U/R. KrishnaswamiN&BentonN-A Semantic Model for Graphical User Interfaces.pdf:application/pdf},
	month = sep,
	pages = {45--57},
	title = {A {Semantic} {Model} for {Graphical} {User} {Interfaces}},
	volume = {46},
	year = {2011}
}

@inproceedings{auger_formalization_2012,
	abstract = {This paper presents a formalization of the compilation of a synchronous data-flow language into an imperative sequential language. We consider MiniLS, a minimalistic yet full-featured synchronous language reminiscent of Lustre. It provides original constructs such as a reset and an n-ary merge operator. These constructs play a central role in generating efficient code and in making the language suitable as a backend for compiling advanced features such as hierarchical state machines. We introduce a generic imperative language to represent transition functions and a clockdirected translation from the source into this language. This translation is modular : every synchronous function is translated into a single transition function. We address the target code generation phase by presenting code emitters to Java and C. The paper comes with a precise description of each compilation step, a formal semantics for the source and destination languages and a proof that whenever the compilation succeeds, it produces a sequential program which is semantically equivalent to the source. To our knowledge, this is the first synchronous realistic compiler with such a correctness property. The formalization of the compilation process and its proof of correctness are an important step toward fully-verified implementations of compilers for languages based on synchronous block diagrams.},
	author = {Auger, C{\'e}dric and Colaco, J. L. and Hamon, Gr{\'e}goire and Pouzet, Marc},
	file = {AugerC et al-A Formalization and Proof of a Modular Lustre Compiler.pdf:/home/linusboyle/Zotero/storage/IEL8QCIT/AugerC et al-A Formalization and Proof of a Modular Lustre Compiler.pdf:application/pdf},
	keywords = {Code generation (compiler), Compiler, Correctness (computer science), Dataflow, Diagram, Imperative programming, Lustre, Merge algorithm, Programming Languages, Semantics (computer science), Synchronous Data Flow, UML state machine, ⛔ No DOI found, Velus},
	title = {A {Formalization} and {Proof} of a {Modular} {Lustre} {Compiler}},
	year = {2012}
}

@inproceedings{jiang_towards_2019,
	author = {Jiang, Hanru and Liang, Hongjin and Xiao, Siyang and Zha, Junpeng and Feng, Xinyu},
	doi = {10.1145/3314221.3314595},
	file = {JiangH et al-Towards certified separate compilation for concurrent programs.pdf:/home/linusboyle/Zotero/storage/HIP6MN9H/JiangH et al-Towards certified separate compilation for concurrent programs.pdf:application/pdf},
	isbn = {978-1-4503-6712-7},
	keywords = {CompCert},
	month = aug,
	pages = {111--125},
	publisher = {ACM},
	title = {Towards certified separate compilation for concurrent programs},
	url = {http://dl.acm.org/citation.cfm?id=3314221.3314595},
	urldate = {2019-08-08},
	year = {2019}
}

@inproceedings{benton_mixed_1994,
	abstract = {Intuitionistic linear logic regains the expressive power of intuitionistic logic through the ! (of course) modality. Benton, Bierman, Hyland and de Paiva have given a term assignment system for ILL and an associated notion of categorical model in which the ! modality is modelled by a comonad satisfying certain extra conditions. Ordinary intuitionistic logic is then modelled in a cartesian closed category which arises as a full subcategory of the category of coalgebras for the comonad.
This paper attempts to explain the connection between ILL and IL more directly and symmetrically by giving a logic, term calculus and categorical model for a system in which the linear and non-linear worlds exist on an equal footing, with operations allowing one to pass in both directions. We start from the categorical model of ILL given by Benton, Bierman, Hyland and de Paiva and show that this is equivalent to having a symmetric monoidal adjunction between a symmetric monoidal closed category and a cartesian closed category. We then derive both a sequent calculus and a natural deduction presentation of the logic corresponding to the new notion of model.},
	author = {Benton, Nick},
	doi = {10.1007/BFb0022251},
	file = {BentonN-A Mixed Linear and Non-Linear Logic - Proofs, Terms and Models.pdf:/home/linusboyle/Zotero/storage/FUKZYGWP/BentonN-A Mixed Linear and Non-Linear Logic - Proofs, Terms and Models.pdf:application/pdf},
	month = sep,
	pages = {121--135},
	shorttitle = {A {Mixed} {Linear} and {Non}-{Linear} {Logic}},
	title = {A {Mixed} {Linear} and {Non}-{Linear} {Logic}: {Proofs}, {Terms} and {Models}},
	year = {1994}
}

@inproceedings{gordon_operational_1993,
	abstract = {I/O mechanisms are needed if functional languages are to be suitable for general purpose programming and several implementations exist. But little is known about semantic methods for specifying and proving properties of lazy functional programs engaged in I/O. As a step towards formal methods of reasoning about realistic I/O we investigate three widely implemented mechanisms in the setting of teletype I/O: synchronised-stream (primitive in Haskell), continuationpassing (derived in Haskell) and Landin-stream I/O (where programs map an input stream to an output stream of characters). Using methods from Milner's CCS we give a labelled transition semantics for the three mechanisms. We adopt bisimulation equivalence as equality on programs engaged in I/O and give functions to map between the three kinds of I/O. The main result is the rst formal proof of semantic equivalence of the three mechanisms, generalising an informal argument of the Haskell committee.},
	address = {Copenhagen, Denmark},
	author = {Gordon, Andrew D.},
	booktitle = {Proceedings of the conference on {Functional} programming languages and computer architecture - {FPCA} '93},
	doi = {10.1145/165180.165199},
	file = {GordonA-An operational semantics for I-O in a lazy functional language.pdf:/home/linusboyle/Zotero/storage/8T96WPM8/GordonA-An operational semantics for I-O in a lazy functional language.pdf:application/pdf},
	isbn = {978-0-89791-595-3},
	language = {en},
	pages = {136--145},
	publisher = {ACM Press},
	title = {An operational semantics for {I}/{O} in a lazy functional language},
	url = {http://portal.acm.org/citation.cfm?doid=165180.165199},
	urldate = {2019-08-24},
	year = {1993}
}

@inproceedings{dasgupta_complete_2019,
	abstract = {We present the most complete and thoroughly tested formal semantics of x86-64 to date. Our semantics faithfully formalizes all the non-deprecated, sequential user-level instructions of the x86-64 Haswell instruction set architecture. This totals 3155 instruction variants, corresponding to 774 mnemonics. The semantics is fully executable and has been tested against more than 7,000 instruction-level test cases and the GCC torture test suite. This extensive testing paid off, revealing bugs in both the x86-64 reference manual and other existing semantics. We also illustrate potential applications of our semantics in different formal analyses, and discuss how it can be useful for processor verification.},
	address = {Phoenix, AZ, USA},
	author = {Dasgupta, Sandeep and Park, Daejun and Kasampalis, Theodoros and Adve, Vikram S. and Ro{\c s}u, Grigore},
	booktitle = {Proceedings of the 40th {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation} - {PLDI} 2019},
	doi = {10.1145/3314221.3314601},
	file = {DasguptaS et al-A complete formal semantics of x86-64 user-level instruction set architecture.pdf:/home/linusboyle/Zotero/storage/6H2IEF38/DasguptaS et al-A complete formal semantics of x86-64 user-level instruction set architecture.pdf:application/pdf},
	isbn = {978-1-4503-6712-7},
	language = {en},
	pages = {1133--1148},
	publisher = {ACM Press},
	title = {A complete formal semantics of x86-64 user-level instruction set architecture},
	url = {http://dl.acm.org/citation.cfm?doid=3314221.3314601},
	urldate = {2019-08-24},
	year = {2019}
}

@inproceedings{kang_lightweight_2016,
	abstract = {Major compiler verification efforts, such as the CompCert project, have traditionally simplified the verification problem by restricting attention to the correctness of whole-program compilation, leaving open the question of how to verify the correctness of separate compilation. Recently, a number of sophisticated techniques have been proposed for proving more flexible, compositional notions of compiler correctness, but these approaches tend to be quite heavyweight compared to the simple "closed simulations" used in verifying whole-program compilation. Applying such techniques to a compiler like CompCert, as Stewart et al. have done, involves major changes and extensions to its original verification. In this paper, we show that if we aim somewhat lower---to prove correctness of separate compilation, but only for a *single* compiler---we can drastically simplify the proof effort. Toward this end, we develop several lightweight techniques that recast the compositional verification problem in terms of whole-program compilation, thereby enabling us to largely reuse the closed-simulation proofs from existing compiler verifications. We demonstrate the effectiveness of these techniques by applying them to CompCert 2.4, converting its verification of whole-program compilation into a verification of separate compilation in less than two person-months. This conversion only required a small number of changes to the original proofs, and uncovered two compiler bugs along the way. The result is SepCompCert, the first verification of separate compilation for the full CompCert compiler.},
	address = {New York, NY, USA},
	author = {Kang, Jeehoon and Kim, Yoonseung and Hur, Chung-Kil and Dreyer, Derek and Vafeiadis, Viktor},
	booktitle = {Proceedings of the 43rd {Annual} {ACM} {SIGPLAN}-{SIGACT} {Symposium} on {Principles} of {Programming} {Languages}},
	doi = {10.1145/2837614.2837642},
	file = {KangJ et al-Lightweight Verification of Separate Compilation.pdf:/home/linusboyle/Zotero/storage/EH75JNNR/KangJ et al-Lightweight Verification of Separate Compilation.pdf:application/pdf},
	isbn = {978-1-4503-3549-2},
	keywords = {CompCert, Compositional compiler verification, separate compilation},
	note = {event-place: St. Petersburg, FL, USA},
	pages = {178--190},
	publisher = {ACM},
	series = {{POPL} '16},
	title = {Lightweight {Verification} of {Separate} {Compilation}},
	url = {http://doi.acm.org/10.1145/2837614.2837642},
	urldate = {2019-09-10},
	year = {2016}
}

@inproceedings{sun_big_2019,
	abstract = {The analytical formulation has been shown to be the most effective for cir---cuit placement. A key ingredient of analytical placement is its wirelength model, which needs to be differentiable and can accurately approximate a golden wirelength model such as half---perimeter wirelength. Existing wirelength models derive gradient from differentiating smooth maximum (minimum) functions, such as the log---sum---exp and weighted---average models. In this paper, we propose a novel bivariate gradient---based wire---length model, namely BiG, which directly derives a gradient with any bivariate smooth maximum (minimum) function without any differentia---tion. Our wirelength model can effectively combine the advantages of both multivariate and bivariate functions. Experimental results show that our BiG model effectively and efficiently improves placement solutions.},
	address = {Las Vegas, NV, USA},
	author = {Sun, Fan-Keng and Chang, Yao-Wen},
	booktitle = {Proceedings of the 56th {Annual} {Design} {Automation} {Conference} 2019 on - {DAC} '19},
	doi = {10.1145/3316781.3317782},
	file = {SunF&ChangY-BiG - A Bivariate Gradient-Based Wirelength Model for Analytical Circuit Placement.pdf:/home/linusboyle/Zotero/storage/W6TR7CHJ/SunF&ChangY-BiG - A Bivariate Gradient-Based Wirelength Model for Analytical Circuit Placement.pdf:application/pdf},
	isbn = {978-1-4503-6725-7},
	language = {en},
	pages = {1--6},
	publisher = {ACM Press},
	shorttitle = {{BiG}},
	title = {{BiG}: {A} {Bivariate} {Gradient}-{Based} {Wirelength} {Model} for {Analytical} {Circuit} {Placement}},
	url = {http://dl.acm.org/citation.cfm?doid=3316781.3317782},
	urldate = {2020-02-21},
	year = {2019}
}

@inproceedings{hsu_dag-based_2019,
	abstract = {As clock frequencies increase, topology-matching bus routing is desired to provide an initial routing result which facilitates the following buffer insertion to meet the timing constraints. Our algorithm consists of three main techniques: (1) a bus clustering method to reduce the routing complexity, (2) a DAG-based algorithm to connect a bus in the specific topology, and (3) a rip---up and re-route scheme to alleviate the routing congestion. Experimental results show that our proposed algorithm outperforms all the participating teams of the 2018 CAD Contest at ICCAD, where the top-3 routers result in 145\%, 158\%, and 420\% higher costs than ours.},
	address = {Las Vegas, NV, USA},
	author = {Hsu, Chen-Hao and Hung, Shao-Chun and Chen, Hao and Sun, Fan-Keng and Chang, Yao-Wen},
	booktitle = {Proceedings of the 56th {Annual} {Design} {Automation} {Conference} 2019 on - {DAC} '19},
	doi = {10.1145/3316781.3317740},
	file = {HsuC et al-A DAG-Based Algorithm for Obstacle-Aware Topology-Matching On-Track Bus Routing.pdf:/home/linusboyle/Zotero/storage/LA8T8VFB/HsuC et al-A DAG-Based Algorithm for Obstacle-Aware Topology-Matching On-Track Bus Routing.pdf:application/pdf},
	isbn = {978-1-4503-6725-7},
	language = {en},
	pages = {1--6},
	publisher = {ACM Press},
	title = {A {DAG}-{Based} {Algorithm} for {Obstacle}-{Aware} {Topology}-{Matching} {On}-{Track} {Bus} {Routing}},
	url = {http://dl.acm.org/citation.cfm?doid=3316781.3317740},
	urldate = {2020-02-21},
	year = {2019}
}

@inproceedings{islam_predicting_2019,
	abstract = {At leading technology nodes, the industry is facing a stiff challenge to make profitable ICs. One of the primary issues is the design rule checking (DRC) violation. In this research, we cohort with the DARPA IDEA program that aims for ``no-human-in-the-loop'' and 24-hour turnaround time to implement an IC from design specifications. In order to reduce human effort, we introduce the ensemble random forest algorithm to predict DRC violations before global routing, which is considered the most time-consuming step in an IC design flow. In addition, we identified features that critically impact the DRC violations. The algorithm has a 5.8\% better F1-score compared to the existing SVM classifiers.},
	address = {Las Vegas, NV, USA},
	author = {Islam, Riadul and Shahjalal, Md Asif},
	booktitle = {Proceedings of the 56th {Annual} {Design} {Automation} {Conference} 2019 on - {DAC} '19},
	doi = {10.1145/3316781.3322478},
	file = {IslamR&ShahjalalM-Predicting DRC Violations Using Ensemble Random Forest Algorithm.pdf:/home/linusboyle/Zotero/storage/6KL4GJ6S/IslamR&ShahjalalM-Predicting DRC Violations Using Ensemble Random Forest Algorithm.pdf:application/pdf},
	isbn = {978-1-4503-6725-7},
	language = {en},
	pages = {1--2},
	publisher = {ACM Press},
	title = {Predicting {DRC} {Violations} {Using} {Ensemble} {Random} {Forest} {Algorithm}},
	url = {http://dl.acm.org/citation.cfm?doid=3316781.3322478},
	urldate = {2020-02-21},
	year = {2019}
}

@inproceedings{lin_dreamplace_2019,
	abstract = {Placement for very-large-scale integrated (VLSI) circuits is one of the most important steps for design closure. This paper proposes a novel GPU-accelerated placement framework DREAMPlace, by casting the analytical placement problem equivalently to training a neural network. Implemented on top of a widely-adopted deep learning toolkit PyTorch, with customized key kernels for wirelength and density computations, DREAMPlace can achieve over 30{\texttimes} speedup in global placement without quality degradation compared to the state-of-theart multi-threaded placer RePlAce. We believe this work shall open up new directions for revisiting classical EDA problems with advancement in AI hardware and software.},
	address = {Las Vegas, NV, USA},
	author = {Lin, Yibo and Dhar, Shounak and Li, Wuxi and Ren, Haoxing and Khailany, Brucek and Pan, David Z.},
	booktitle = {Proceedings of the 56th {Annual} {Design} {Automation} {Conference} 2019 on - {DAC} '19},
	doi = {10.1145/3316781.3317803},
	file = {LinY et al-DREAMPlace - Deep Learning Toolkit-Enabled GPU Acceleration for Modern VLSI Placement.pdf:/home/linusboyle/Zotero/storage/ARU2NEGF/LinY et al-DREAMPlace - Deep Learning Toolkit-Enabled GPU Acceleration for Modern VLSI Placement.pdf:application/pdf},
	isbn = {978-1-4503-6725-7},
	keywords = {软件前沿报告, Read, 数字系统设计课, EDA与AI},
	language = {en},
	pages = {1--6},
	publisher = {ACM Press},
	shorttitle = {{DREAMPlace}},
	title = {{DREAMPlace}: {Deep} {Learning} {Toolkit}-{Enabled} {GPU} {Acceleration} for {Modern} {VLSI} {Placement}},
	url = {http://dl.acm.org/citation.cfm?doid=3316781.3317803},
	urldate = {2020-02-21},
	year = {2019}
}

@inproceedings{yu_painting_2019,
	abstract = {Physical design process commonly consumes hours to days for large designs, and routing is known as the most critical step. Demands for accurate routing quality prediction raise to a new level to accelerate hardware innovation with advanced technology nodes. This work presents an approach that forecasts the density of all routing channels over the entire floorplan, with features collected up to placement, using conditional GANs. Specifically, forecasting the routing congestion is constructed as an image translation (colorization) problem. The proposed approach is applied to a) placement exploration for minimum congestion, b) constrained placement exploration and c) forecasting congestion in real-time during incremental placement, using eight designs targeting a fixed FPGA architecture.},
	address = {Las Vegas, NV, USA},
	author = {Yu, Cunxi and Zhang, Zhiru},
	booktitle = {Proceedings of the 56th {Annual} {Design} {Automation} {Conference} 2019 on - {DAC} '19},
	doi = {10.1145/3316781.3317876},
	file = {YuC&ZhangZ-Painting on Placement - Forecasting Routing Congestion using Conditional Generative Adversarial Nets.pdf:/home/linusboyle/Zotero/storage/YR9P5NNA/YuC&ZhangZ-Painting on Placement - Forecasting Routing Congestion using Conditional Generative Adversarial Nets.pdf:application/pdf},
	isbn = {978-1-4503-6725-7},
	keywords = {EDA与AI},
	language = {en},
	pages = {1--6},
	publisher = {ACM Press},
	shorttitle = {Painting on {Placement}},
	title = {Painting on {Placement}: {Forecasting} {Routing} {Congestion} using {Conditional} {Generative} {Adversarial} {Nets}},
	url = {http://dl.acm.org/citation.cfm?doid=3316781.3317876},
	urldate = {2020-02-21},
	year = {2019}
}

@inproceedings{ma_high_2019,
	abstract = {Applications of deep learning to electronic design automation (EDA) have recently begun to emerge, although they have mainly been limited to processing of regular structured data such as images. However, many EDA problems require processing irregular structures, and it can be non-trivial to manually extract important features in such cases. In this paper, a high performance graph convolutional network (GCN) model is proposed for the purpose of processing irregular graph representations of logic circuits. A GCN classifier is firstly trained to predict observation point candidates in a netlist. The GCN classifier is then used as part of an iterative process to propose observation point insertion based on the classification results. Experimental results show the proposed GCN model has superior accuracy to classical machine learning models on difficult-to-observation nodes prediction. Compared with commercial testability analysis tools, the proposed observation point insertion flow achieves similar fault coverage with an 11\% reduction in observation points and a 6\% reduction in test pattern count.},
	address = {Las Vegas, NV, USA},
	author = {Ma, Yuzhe and Ren, Haoxing and Khailany, Brucek and Sikka, Harbinder and Luo, Lijuan and Natarajan, Karthikeyan and Yu, Bei},
	booktitle = {Proceedings of the 56th {Annual} {Design} {Automation} {Conference} 2019 on - {DAC} '19},
	doi = {10.1145/3316781.3317838},
	file = {MaY et al-High Performance Graph Convolutional Networks with Applications in Testability Analysis.pdf:/home/linusboyle/Zotero/storage/ZF7RX976/MaY et al-High Performance Graph Convolutional Networks with Applications in Testability Analysis.pdf:application/pdf},
	isbn = {978-1-4503-6725-7},
	language = {en},
	pages = {1--6},
	publisher = {ACM Press},
	title = {High {Performance} {Graph} {Convolutional} {Networks} with {Applications} in {Testability} {Analysis}},
	url = {http://dl.acm.org/citation.cfm?doid=3316781.3317838},
	urldate = {2020-02-21},
	year = {2019}
}

@inproceedings{chen_faster_2019,
	abstract = {As the circuit feature size continuously shrinks down, hotspot detection has become a more challenging problem in modern DFM flows. Developed deep learning techniques have recently shown their advantages on hotspot detection tasks. However, existing hotspot detectors only accept small layout clips as input with potential defects occurring at a center region of each clip, which will be time consuming and waste lots of computational resources when dealing with large full-chip layouts. In this paper, we develop a new end-to-end framework that can detect multiple hotspots in a large region at a time and promise a better hotspot detection performance. We design a joint auto-encoder and inception module for efficient feature extraction. A two-stage classification and regression flow is proposed to efficiently locate hotspot regions roughly and conduct final prediction with better accuracy and false alarm penalty. Experimental results show that our framework enables a significant speed improvement over existing methods with higher accuracy and fewer false alarms.},
	address = {Las Vegas, NV, USA},
	author = {Chen, Ran and Zhong, Wei and Yang, Haoyu and Geng, Hao and Zeng, Xuan and Yu, Bei},
	booktitle = {Proceedings of the 56th {Annual} {Design} {Automation} {Conference} 2019 on - {DAC} '19},
	doi = {10.1145/3316781.3317824},
	file = {ChenR et al-Faster Region-based Hotspot Detection.pdf:/home/linusboyle/Zotero/storage/C3IUANK2/ChenR et al-Faster Region-based Hotspot Detection.pdf:application/pdf},
	isbn = {978-1-4503-6725-7},
	language = {en},
	pages = {1--6},
	publisher = {ACM Press},
	title = {Faster {Region}-based {Hotspot} {Detection}},
	url = {http://dl.acm.org/citation.cfm?doid=3316781.3317824},
	urldate = {2020-02-21},
	year = {2019}
}

@inproceedings{jiang_efficient_2019,
	abstract = {Layout hotspot detection is of great importance in the physical verification flow. Deep neural network models have been applied to hotspot detection and achieved great successes. The layouts can be viewed as binary images. The binarized neural network can thus be suitable for the hotspot detection problem. In this paper we propose a new deep learning architecture based on binarized neural networks (BNNs) to speed up the neural networks in hotspot detection. A new binarized residual neural network is carefully designed for hotspot detection. Experimental results on ICCAD 2012 Contest benchmarks show that our architecture outperforms all previous hotspot detectors in detecting accuracy and has an 8x speedup over the best deep learning-based solution.},
	address = {Las Vegas, NV, USA},
	author = {Jiang, Yiyang and Yang, Fan and Zhu, Hengliang and Yu, Bei and Zhou, Dian and Zeng, Xuan},
	booktitle = {Proceedings of the 56th {Annual} {Design} {Automation} {Conference} 2019 on - {DAC} '19},
	doi = {10.1145/3316781.3317811},
	file = {JiangY et al-Efficient Layout Hotspot Detection via Binarized Residual Neural Network.pdf:/home/linusboyle/Zotero/storage/Z2GVVH65/JiangY et al-Efficient Layout Hotspot Detection via Binarized Residual Neural Network.pdf:application/pdf},
	isbn = {978-1-4503-6725-7},
	language = {en},
	pages = {1--6},
	publisher = {ACM Press},
	title = {Efficient {Layout} {Hotspot} {Detection} via {Binarized} {Residual} {Neural} {Network}},
	url = {http://dl.acm.org/citation.cfm?doid=3316781.3317811},
	urldate = {2020-02-21},
	year = {2019}
}

@inproceedings{barboza_machine_2019,
	abstract = {Optimizations at placement stage need to be guided by timing estimation prior to routing. To handle timing uncertainty due to the lack of routing information, people tend to make very pessimistic predictions such that performance specification can be ensured in the worst case. Such pessimism causes over-design that wastes chip resources or design effort. In this work, a machine learning-based pre-routing timing prediction approach is introduced. Experimental results show that it can reach accuracy near post-routing sign-off analysis. Compared to a commercial pre-routing timing estimation tool, it reduces false positive rate by about 2/3 in reporting timing violations.},
	address = {Las Vegas, NV, USA},
	author = {Barboza, Erick Carvajal and Shukla, Nishchal and Chen, Yiran and Hu, Jiang},
	booktitle = {Proceedings of the 56th {Annual} {Design} {Automation} {Conference} 2019 on - {DAC} '19},
	doi = {10.1145/3316781.3317857},
	file = {BarbozaE et al-Machine Learning-Based Pre-Routing Timing Prediction with Reduced Pessimism.pdf:/home/linusboyle/Zotero/storage/4NKCZRCN/BarbozaE et al-Machine Learning-Based Pre-Routing Timing Prediction with Reduced Pessimism.pdf:application/pdf},
	isbn = {978-1-4503-6725-7},
	keywords = {软件前沿报告, Read, EDA与AI},
	language = {en},
	pages = {1--6},
	publisher = {ACM Press},
	title = {Machine {Learning}-{Based} {Pre}-{Routing} {Timing} {Prediction} with {Reduced} {Pessimism}},
	url = {http://dl.acm.org/citation.cfm?doid=3316781.3317857},
	urldate = {2020-02-21},
	year = {2019}
}

@inproceedings{chen_march_2019,
	abstract = {The continuous development of modern VLSI technology has brought new challenges for on-chip interconnections. Different from classic net-by-net routing, bus routing requires all the nets (bits) in the same bus to share similar or even the same topology, besides considering wire length, via count, and other design rules. In this paper, we present MARCH, an efficient maze routing method under a concurrent and hierarchical scheme for buses. In MARCH, to achieve the same topology, all the bits in a bus are routed concurrently like marching in a path. For efficiency, our method is hierarchical, consisting of a coarse-grained topology-aware path planning and a fine-grained track assignment for bits. Additionally, an effective rip-up and reroute scheme is applied to further improve the solution quality. In experimental results, MARCH significantly outperforms the first place at 2018 IC/CAD Contest in both quality and runtime.},
	address = {Las Vegas, NV, USA},
	author = {Chen, Jingsong and Liu, Jinwei and Chen, Gengjie and Zheng, Dan and Young, Evangeline F. Y.},
	booktitle = {Proceedings of the 56th {Annual} {Design} {Automation} {Conference} 2019 on - {DAC} '19},
	doi = {10.1145/3316781.3317860},
	file = {ChenJ et al-MARCH - MAze Routing Under a Concurrent and Hierarchical Scheme for Buses.pdf:/home/linusboyle/Zotero/storage/DHPENA6B/ChenJ et al-MARCH - MAze Routing Under a Concurrent and Hierarchical Scheme for Buses.pdf:application/pdf},
	isbn = {978-1-4503-6725-7},
	language = {en},
	pages = {1--6},
	publisher = {ACM Press},
	shorttitle = {{MARCH}},
	title = {{MARCH}: {MAze} {Routing} {Under} a {Concurrent} and {Hierarchical} {Scheme} for {Buses}},
	url = {http://dl.acm.org/citation.cfm?doid=3316781.3317860},
	urldate = {2020-02-21},
	year = {2019}
}

@inproceedings{yu_pin_2019,
	address = {Las Vegas, NV, USA},
	author = {Yu, Tao-Chun and Fang, Shao-Yun and Chiu, Hsien-Shih and Hu, Kai-Shun and Tai, Philip Hui-Yuh and Shen, Cindy Chin-Fang and Sheng, Henry},
	booktitle = {Proceedings of the 56th {Annual} {Design} {Automation} {Conference} 2019 on - {DAC} '19},
	doi = {10.1145/3316781.3317882},
	file = {YuT et al-Pin Accessibility Prediction and Optimization with Deep Learning-based Pin Pattern Recognition.pdf:/home/linusboyle/Zotero/storage/QICEDR5I/YuT et al-Pin Accessibility Prediction and Optimization with Deep Learning-based Pin Pattern Recognition.pdf:application/pdf},
	isbn = {978-1-4503-6725-7},
	keywords = {EDA与AI},
	language = {en},
	pages = {1--6},
	publisher = {ACM Press},
	title = {Pin {Accessibility} {Prediction} and {Optimization} with {Deep} {Learning}-based {Pin} {Pattern} {Recognition}},
	url = {http://dl.acm.org/citation.cfm?doid=3316781.3317882},
	urldate = {2020-02-21},
	year = {2019}
}

@inproceedings{lin_routability-driven_2019,
	abstract = {The mixed-size placement becomes a great challenge in the modern VLSI design. To handle this problem, the three-stage mixed-size placement methodology is considered as the most suitable approach for a commercial design flow, where the placement prototyping is the most important stage. Since standard cells and macros have to be considered simultaneously in this stage, it is more complicated than the other two stages. To reduce complexity and improve design quality, this paper applies the multilevel framework with a design hierarchy-guided clustering scheme for getting a better coarsening result in order to improve outcome in the following stages. We propose an efficient and effective clustering scheme to group standard cells and macros based on the tree built from their design hierarchies. More importantly, our clustering algorithm considers indirect connectivity between macros which is ignored by previous works. Moreover, we propose a new overlapping bounding box constraint to avoid clustering improper macros which have connections to fixed pins. The experimental results show that wirelength and routability are improved by our methodology.},
	address = {Las Vegas, NV, USA},
	author = {Lin, Jai-Ming and Li, Szu-Ting and Wang, Yi-Ting},
	booktitle = {Proceedings of the 56th {Annual} {Design} {Automation} {Conference} 2019 on - {DAC} '19},
	doi = {10.1145/3316781.3317901},
	file = {LinJ et al-Routability-driven Mixed-size Placement Prototyping Approach Considering Design Hierarchy and.pdf:/home/linusboyle/Zotero/storage/55GVDSZR/LinJ et al-Routability-driven Mixed-size Placement Prototyping Approach Considering Design Hierarchy and.pdf:application/pdf},
	isbn = {978-1-4503-6725-7},
	language = {en},
	pages = {1--6},
	publisher = {ACM Press},
	title = {Routability-driven {Mixed}-size {Placement} {Prototyping} {Approach} {Considering} {Design} {Hierarchy} and {Indirect} {Connectivity} {Between} {Macros}},
	url = {http://dl.acm.org/citation.cfm?doid=3316781.3317901},
	urldate = {2020-02-21},
	year = {2019}
}

@inproceedings{breuer_class_1988,
	address = {Not Known},
	author = {Breuer, M. A.},
	booktitle = {Papers on {Twenty}-five years of electronic design automation - 25 years of {DAC}},
	doi = {10.1145/62882.62896},
	file = {BreuerM-A class of min-cut placement algorithms.pdf:/home/linusboyle/Zotero/storage/FHQZGYE8/BreuerM-A class of min-cut placement algorithms.pdf:application/pdf},
	isbn = {978-0-89791-267-9},
	language = {en},
	pages = {142--148},
	publisher = {ACM Press},
	title = {A class of min-cut placement algorithms},
	url = {http://portal.acm.org/citation.cfm?doid=62882.62896},
	urldate = {2020-02-22},
	year = {1988}
}

@inproceedings{patyal_analog_2018,
	abstract = {Modern analog placement techniques require consideration of current path and symmetry constraints. The symmetry pairs can be efficiently packed using the symmetry island configurations, but not all these configurations result in minimum gate interconnection, which can impact the overall circuit routing and performance. This paper proposes the first work that reformulates this problem considering all of them together in the form of Parallel Current Path (PCP) constraints. Then a placement algorithm satisfying these constraints is formulated to reduce a vast search space via efficient sequence pair manipulation. Experimental results show that this formulation and algorithm can satisfy all the constraints in a more tightly packed configuration, resulting in lesser routing length, reduced parasitics and thus better post-layout performance.},
	address = {San Francisco, California},
	author = {Patyal, Abhishek and Pan, Po-Cheng and A, Asha K and Chen, Hung-Ming and Chi, Hao-Yu and Liu, Chien-Nan},
	booktitle = {Proceedings of the 55th {Annual} {Design} {Automation} {Conference} on - {DAC} '18},
	doi = {10.1145/3195970.3195990},
	file = {PatyalA et al-Analog placement with current flow and symmetry constraints using PCP-SP.pdf:/home/linusboyle/Zotero/storage/EHGYCK7E/PatyalA et al-Analog placement with current flow and symmetry constraints using PCP-SP.pdf:application/pdf},
	isbn = {978-1-4503-5700-5},
	language = {en},
	pages = {1--6},
	publisher = {ACM Press},
	title = {Analog placement with current flow and symmetry constraints using {PCP}-{SP}},
	url = {http://dl.acm.org/citation.cfm?doid=3195970.3195990},
	urldate = {2020-03-08},
	year = {2018}
}

@inproceedings{yu_dsa-friendly_2018,
	address = {San Francisco, California},
	author = {Yu, Hai-Juan and Chang, Yao-Wen},
	booktitle = {Proceedings of the 55th {Annual} {Design} {Automation} {Conference} on - {DAC} '18},
	doi = {10.1145/3195970.3196030},
	file = {YuH&ChangY-DSA-friendly detailed routing considering double patterning and DSA template assignments.pdf:/home/linusboyle/Zotero/storage/KRLFLU2T/YuH&ChangY-DSA-friendly detailed routing considering double patterning and DSA template assignments.pdf:application/pdf},
	isbn = {978-1-4503-5700-5},
	language = {en},
	pages = {1--6},
	publisher = {ACM Press},
	title = {{DSA}-friendly detailed routing considering double patterning and {DSA} template assignments},
	url = {http://dl.acm.org/citation.cfm?doid=3195970.3196030},
	urldate = {2020-03-08},
	year = {2018}
}

@inproceedings{zhu_generalized_2018,
	abstract = {Global placement dominates the circuit placement process in its solution quality and e ciency. With increasing design complexity and various design constraints, it is desirable to develop an e cient, high-quality global placement algorithm for modern large-scale circuit designs. In this paper, we rst analyze the properties of four nonlinear optimization methods (the quadratic penalty method, the Lagrange multiplier method, and two augmented Lagrangian methods) for global placement, and then develop a generalized augmented Lagrangian method to solve this problem. Our proposed method preserves the advantages of the quadratic penalty method and the augmented Lagrangian method, and provides a smooth progress from the quadratic penalty method to the augmented Lagrangian method. We prove that the proposed generalized augmented Lagrangian method is globally convergent for the original global placement problem, even with di erent constraints. Compared with the other four popular optimization methods, experimental results show that our method achieves the best quality and is robust for handling di erent objectives. In particular, our generalized augmented Lagrangian formulation is theoretically sound and can solve generic large-scale constrained nonlinear optimization problems, which are widely used in many elds.},
	address = {San Francisco, CA},
	author = {Zhu, Ziran and Chen, Jianli and Peng, Zheng and Zhu, Wenxing and Chang, Yao-Wen},
	booktitle = {2018 55th {ACM}/{ESDA}/{IEEE} {Design} {Automation} {Conference} ({DAC})},
	doi = {10.1109/DAC.2018.8465922},
	file = {ZhuZ et al-Generalized Augmented Lagrangian and Its Applications to VLSI Global Placement sup-sup.pdf:/home/linusboyle/Zotero/storage/9UJZTSXZ/ZhuZ et al-Generalized Augmented Lagrangian and Its Applications to VLSI Global Placement sup-sup.pdf:application/pdf},
	isbn = {978-1-5386-4114-9},
	language = {en},
	month = jun,
	pages = {1--6},
	publisher = {IEEE},
	title = {Generalized {Augmented} {Lagrangian} and {Its} {Applications} to {VLSI} {Global} {Placement} $^{\textrm{*}}$},
	url = {https://ieeexplore.ieee.org/document/8465922/},
	urldate = {2020-03-08},
	year = {2018}
}

@inproceedings{tabrizi_machine_2018,
	abstract = {Detecting and preventing routing violations has become a critical issue in physical design, especially in the early stages. Lack of correlation between global and detailed routing congestion estimations and the long runtime required to frequently consult a global router adds to the problem. In this paper, we propose a machine learning framework to predict detailed routing short violations from a placed netlist. Factors contributing to routing violations are determined and a supervised neural network model is implemented to detect these violations. Experimental results show that the proposed method is able to predict on average 90\% of the shorts with only 7\% false alarms and considerably reduced computational time.},
	address = {San Francisco, California},
	author = {Tabrizi, Aysa Fakheri and Darav, Nima Karimpour and Xu, Shuchang and Rakai, Logan and Bustany, Ismail and Kennings, Andrew and Behjat, Laleh},
	booktitle = {Proceedings of the 55th {Annual} {Design} {Automation} {Conference} on - {DAC} '18},
	doi = {10.1145/3195970.3195975},
	file = {TabriziA et al-A machine learning framework to identify detailed routing short violations from a placed netlist.pdf:/home/linusboyle/Zotero/storage/JTR8KMYA/TabriziA et al-A machine learning framework to identify detailed routing short violations from a placed netlist.pdf:application/pdf},
	isbn = {978-1-4503-5700-5},
	keywords = {EDA与AI},
	language = {en},
	pages = {1--6},
	publisher = {ACM Press},
	title = {A machine learning framework to identify detailed routing short violations from a placed netlist},
	url = {http://dl.acm.org/citation.cfm?doid=3195970.3195975},
	urldate = {2020-03-08},
	year = {2018}
}

@inproceedings{chuang_planaronoc_2018,
	abstract = {Optical networks-on-chips (ONoCs) have become a promising solution for the on-chip communication of multi-and many-core systems to provide superior communication bandwidths, efficiency in power consumption, and latency performance compared to electronic NoCs. Serving as the critical part of ONoCs, an optical router composed of waveguides and photonic switching elements (PSEs) routes signals between two hubs or between a hub and a memory controller. Many studies focus on developing efficient architectures of optical routers, while their physical implementation that can seriously deteriorate the quality of the architectures is rarely addressed. The existing automatic place-and-route tools suffer from considerable insertion loss due to many waveguide crossings outside of PSEs, which leads to huge power consumption of laser sources. By observing that the logic schemes of most optical routers are actually planar, we develop a concurrent PSE placement and waveguide routing flow, called PlanarONoC, that guarantees optimal solutions in terms of crossings for planar logic schemes. Experimental results show that the proposed flow reduces the maximum insertion loss by 37\% on average, guarantees no waveguide crossing outside of PSEs, and performs much more efficient compared to the state-of-the-art work.},
	address = {San Francisco, CA},
	author = {Chuang, Yu-Kai and Chen, Kuan-Jung and Lin, Kun-Lin and Fang, Shao-Yun and Li, Bing and Schlichtmann, Ulf},
	booktitle = {2018 55th {ACM}/{ESDA}/{IEEE} {Design} {Automation} {Conference} ({DAC})},
	doi = {10.1109/DAC.2018.8465775},
	file = {ChuangY et al-PlanarONoC - Concurrent Placement and Routing Considering Crossing Minimization for Optical.pdf:/home/linusboyle/Zotero/storage/YXCJQ4ER/ChuangY et al-PlanarONoC - Concurrent Placement and Routing Considering Crossing Minimization for Optical.pdf:application/pdf},
	isbn = {978-1-5386-4114-9},
	language = {en},
	month = jun,
	pages = {1--6},
	publisher = {IEEE},
	shorttitle = {{PlanarONoC}},
	title = {{PlanarONoC}: {Concurrent} {Placement} and {Routing} {Considering} {Crossing} {Minimization} for {Optical} {Networks}-on-{Chip} $^{\textrm{*}}$},
	url = {https://ieeexplore.ieee.org/document/8465775/},
	urldate = {2020-03-08},
	year = {2018}
}

@inproceedings{li_routability-driven_2018,
	abstract = {Placement is one of the most critical stages in the physical synthesis flow. Circuits with increasing numbers of cells of multi-row height have brought challenges to traditional placers on efficiency and effectiveness. Furthermore, constraints on fence region and routability (e.g., edge spacing, pin access/short) should be considered, besides providing an overlap-free solution close to the global placement (GP) solution and fulfilling the power and ground (P/G) alignments. In this paper, we propose a legalization method for mixed-cell-height circuits by a window-based cell insertion technique and two post-processing network-flow-based optimizations. Compared with the champion of the IC/CAD 2017 Contest, our algorithm achieves 18\% and 12\% less average and maximum displacement respectively as well as significantly fewer routability violations. Comparing our algorithm with the state-of-the-art algorithms on this problem, there is a 9\% improvement in total displacement with 20\% less running time.},
	address = {San Francisco, CA},
	author = {Li, Haocheng and Chow, Wing-Kai and Chen, Gengjie and Young, Evangeline F. Y. and Yu, Bei},
	booktitle = {2018 55th {ACM}/{ESDA}/{IEEE} {Design} {Automation} {Conference} ({DAC})},
	doi = {10.1109/DAC.2018.8465819},
	file = {LiH et al-Routability-Driven and Fence-Aware Legalization for Mixed-Cell-Height Circuits.pdf:/home/linusboyle/Zotero/storage/589CHZ59/LiH et al-Routability-Driven and Fence-Aware Legalization for Mixed-Cell-Height Circuits.pdf:application/pdf},
	isbn = {978-1-5386-4114-9},
	language = {en},
	month = jun,
	pages = {1--6},
	publisher = {IEEE},
	title = {Routability-{Driven} and {Fence}-{Aware} {Legalization} for {Mixed}-{Cell}-{Height} {Circuits}},
	url = {https://ieeexplore.ieee.org/document/8465819/},
	urldate = {2020-03-08},
	year = {2018}
}

@inproceedings{fonseca_empirical_2017,
	abstract = {Recent advances in formal verification techniques enabled the implementation of distributed systems with machinechecked proofs. While results are encouraging, the importance of distributed systems warrants a large scale evaluation of the results and verification practices.},
	address = {Belgrade, Serbia},
	author = {Fonseca, Pedro and Zhang, Kaiyuan and Wang, Xi and Krishnamurthy, Arvind},
	booktitle = {Proceedings of the {Twelfth} {European} {Conference} on {Computer} {Systems} - {EuroSys} '17},
	doi = {10.1145/3064176.3064183},
	file = {FonsecaP et al-An Empirical Study on the Correctness of Formally Verified Distributed Systems.pdf:/home/linusboyle/Zotero/storage/5Z4RNWEN/FonsecaP et al-An Empirical Study on the Correctness of Formally Verified Distributed Systems.pdf:application/pdf},
	isbn = {978-1-4503-4938-3},
	keywords = {push button verification},
	language = {en},
	pages = {328--343},
	publisher = {ACM Press},
	title = {An {Empirical} {Study} on the {Correctness} of {Formally} {Verified} {Distributed} {Systems}},
	url = {http://dl.acm.org/citation.cfm?doid=3064176.3064183},
	urldate = {2020-03-18},
	year = {2017}
}

@inproceedings{chen_using_2015,
	abstract = {FSCQ is the first file system with a machine-checkable proof (using the Coq proof assistant) that its implementation meets its specification and whose specification includes crashes. FSCQ provably avoids bugs that have plagued previous file systems, such as performing disk writes without sufficient barriers or forgetting to zero out directory blocks. If a crash happens at an inopportune time, these bugs can lead to data loss. FSCQ{\rq}s theorems prove that, under any sequence of crashes followed by reboots, FSCQ will recover the file system correctly without losing data. To state FSCQ{\rq}s theorems, this paper introduces the Crash Hoare logic (CHL), which extends traditional Hoare logic with a crash condition, a recovery procedure, and logical address spaces for specifying disk states at different abstraction levels. CHL also reduces the proof effort for developers through proof automation. Using CHL, we developed, specified, and proved the correctness of the FSCQ file system. Although FSCQ{\rq}s design is relatively simple, experiments with FSCQ running as a user-level file system show that it is sufficient to run Unix applications with usable performance. FSCQ{\rq}s specifications and proofs required significantly more work than the implementation, but the work was manageable even for a small team of a few researchers.},
	address = {Monterey, California},
	author = {Chen, Haogang and Ziegler, Daniel and Chajed, Tej and Chlipala, Adam and Kaashoek, M. Frans and Zeldovich, Nickolai},
	booktitle = {Proceedings of the 25th {Symposium} on {Operating} {Systems} {Principles} - {SOSP} '15},
	doi = {10.1145/2815400.2815402},
	file = {ChenH et al-Using Crash Hoare logic for certifying the FSCQ file system.pdf:/home/linusboyle/Zotero/storage/2AVX8KNK/ChenH et al-Using Crash Hoare logic for certifying the FSCQ file system.pdf:application/pdf},
	isbn = {978-1-4503-3834-9},
	language = {en},
	pages = {18--37},
	publisher = {ACM Press},
	title = {Using {Crash} {Hoare} logic for certifying the {FSCQ} file system},
	url = {http://dl.acm.org/citation.cfm?doid=2815400.2815402},
	urldate = {2020-03-18},
	year = {2015}
}

@article{chen_specifying_nodate,
	author = {Chen, Haogang and Ziegler, Daniel and Chlipala, Adam and Kaashoek, M Frans and Kohler, Eddie and Zeldovich, Nickolai},
	file = {ChenH et al-Specifying Crash Safety for Storage Systems.pdf:/home/linusboyle/Zotero/storage/G3ZMB67N/ChenH et al-Specifying Crash Safety for Storage Systems.pdf:application/pdf},
	language = {en},
	pages = {7},
	title = {Specifying {Crash} {Safety} for {Storage} {Systems}}
}

@inproceedings{chen_verifying_2017,
	abstract = {DFSCQ is the first file system that (1) provides a precise specification for fsync and fdatasync, which allow applications to achieve high performance and crash safety, and (2) provides a machine-checked proof that its implementation meets this specification. DFSCQ{\rq}s specification captures the behavior of sophisticated optimizations, including logbypass writes, and DFSCQ{\rq}s proof rules out some of the common bugs in file-system implementations despite the complex optimizations.},
	address = {Shanghai, China},
	author = {Chen, Haogang and Chajed, Tej and Konradi, Alex and Wang, Stephanie and {\.I}leri, Atalay and Chlipala, Adam and Kaashoek, M. Frans and Zeldovich, Nickolai},
	booktitle = {Proceedings of the 26th {Symposium} on {Operating} {Systems} {Principles} - {SOSP} '17},
	doi = {10.1145/3132747.3132776},
	file = {ChenH et al-Verifying a high-performance crash-safe file system using a tree specification.pdf:/home/linusboyle/Zotero/storage/8BEZAHC6/ChenH et al-Verifying a high-performance crash-safe file system using a tree specification.pdf:application/pdf},
	isbn = {978-1-4503-5085-3},
	language = {en},
	pages = {270--286},
	publisher = {ACM Press},
	title = {Verifying a high-performance crash-safe file system using a tree specification},
	url = {http://dl.acm.org/citation.cfm?doid=3132747.3132776},
	urldate = {2020-03-18},
	year = {2017}
}

@article{wang_jitk_nodate,
	abstract = {Modern operating systems run multiple interpreters in the kernel, which enable user-space applications to add new functionality or specialize system policies. The correctness of such interpreters is critical to the overall system security: bugs in interpreters could allow adversaries to compromise user-space applications and even the kernel. Jitk is a new infrastructure for building in-kernel interpreters that guarantee functional correctness as they compile user-space policies down to native instructions for execution in the kernel. To demonstrate Jitk, we implement two interpreters in the Linux kernel, BPF and INET-DIAG, which are used for network and system call filtering and socket monitoring, respectively. To help application developers write correct filters, we introduce a high-level rule language, along with a proof that Jitk correctly translates high-level rules all the way to native machine code, and demonstrate that this language can be integrated into OpenSSH with tens of lines of code. We built a prototype of Jitk on top of the CompCert verified compiler and integrated it into the Linux kernel. Experimental results show that Jitk is practical, fast, and trustworthy.},
	author = {Wang, Xi and Lazar, David and Zeldovich, Nickolai and Chlipala, Adam and Tatlock, Zachary},
	file = {WangX et al-Jitk - A Trustworthy In-Kernel Interpreter Infrastructure.pdf:/home/linusboyle/Zotero/storage/6JIFDBKT/WangX et al-Jitk - A Trustworthy In-Kernel Interpreter Infrastructure.pdf:application/pdf},
	keywords = {push button verification},
	language = {en},
	pages = {15},
	title = {Jitk: {A} {Trustworthy} {In}-{Kernel} {Interpreter} {Infrastructure}}
}

@inproceedings{nelson_hyperkernel_2017,
	address = {Shanghai, China},
	author = {Nelson, Luke and Sigurbjarnarson, Helgi and Zhang, Kaiyuan and Johnson, Dylan and Bornholt, James and Torlak, Emina and Wang, Xi},
	booktitle = {Proceedings of the 26th {Symposium} on {Operating} {Systems} {Principles} - {SOSP} '17},
	doi = {10.1145/3132747.3132748},
	file = {NelsonL et al-Hyperkernel - Push-Button Verification of an OS Kernel.pdf:/home/linusboyle/Zotero/storage/7G32B5MI/NelsonL et al-Hyperkernel - Push-Button Verification of an OS Kernel.pdf:application/pdf},
	isbn = {978-1-4503-5085-3},
	keywords = {push button verification},
	language = {en},
	pages = {252--269},
	publisher = {ACM Press},
	shorttitle = {Hyperkernel},
	title = {Hyperkernel: {Push}-{Button} {Verification} of an {OS} {Kernel}},
	url = {http://dl.acm.org/citation.cfm?doid=3132747.3132748},
	urldate = {2020-03-19},
	year = {2017}
}

@inproceedings{nelson_scaling_2019,
	abstract = {This paper presents Serval, a framework for developing automated verifiers for systems software. Serval provides an extensible infrastructure for creating verifiers by lifting interpreters under symbolic evaluation, and a systematic approach to identifying and repairing verification performance bottlenecks using symbolic profiling and optimizations. Using Serval, we build automated verifiers for the RISC-V, x86-32, LLVM, and BPF instruction sets. We report our experience of retrofitting CertiKOS and Komodo, two systems previously verified using Coq and Dafny, respectively, for automated verification using Serval, and discuss trade-offs of different verification methodologies. In addition, we apply Serval to the Keystone security monitor and the BPF compilers in the Linux kernel, and uncover 18 new bugs through verification, all confirmed and fixed by developers.},
	address = {Huntsville, Ontario, Canada},
	author = {Nelson, Luke and Bornholt, James and Gu, Ronghui and Baumann, Andrew and Torlak, Emina and Wang, Xi},
	booktitle = {Proceedings of the 27th {ACM} {Symposium} on {Operating} {Systems} {Principles} - {SOSP} '19},
	doi = {10.1145/3341301.3359641},
	file = {NelsonL et al-Scaling symbolic evaluation for automated verification of systems code with Serval.pdf:/home/linusboyle/Zotero/storage/6VQ9H8WB/NelsonL et al-Scaling symbolic evaluation for automated verification of systems code with Serval.pdf:application/pdf},
	isbn = {978-1-4503-6873-5},
	keywords = {push button verification, Read},
	language = {en},
	pages = {225--242},
	publisher = {ACM Press},
	title = {Scaling symbolic evaluation for automated verification of systems code with {Serval}},
	url = {http://dl.acm.org/citation.cfm?doid=3341301.3359641},
	urldate = {2020-03-21},
	year = {2019}
}

@inproceedings{shi_formally_2017,
	address = {Buenos Aires, Argentina},
	author = {Shi, Gang and Gan, Yuanke and Shang, Shu and Wang, Shengyuan and Dong, Yuan and Yew, Pen-Chung},
	booktitle = {2017 {IEEE}/{ACM} 39th {International} {Conference} on {Software} {Engineering} {Companion} ({ICSE}-{C})},
	doi = {10.1109/ICSE-C.2017.83},
	file = {ShiG et al-A Formally Verified Sequentializer for Lustre-Like Concurrent Synchronous Data-Flow Programs.pdf:/home/linusboyle/Zotero/storage/WDYSTJ7L/ShiG et al-A Formally Verified Sequentializer for Lustre-Like Concurrent Synchronous Data-Flow Programs.pdf:application/pdf},
	isbn = {978-1-5386-1589-8},
	keywords = {Lustre, L2C},
	month = may,
	pages = {109--111},
	publisher = {IEEE},
	title = {A {Formally} {Verified} {Sequentializer} for {Lustre}-{Like} {Concurrent} {Synchronous} {Data}-{Flow} {Programs}},
	url = {http://ieeexplore.ieee.org/document/7965271/},
	urldate = {2020-03-21},
	year = {2017}
}

@article{shi_formally_2019,
	author = {Shi, Gang and Zhang, Yucheng and Shang, Shu and Wang, Shengyuan and Dong, Yuan and Yew, Pen-Chung},
	doi = {10.1007/s11432-016-9270-0},
	issn = {1674-733X, 1869-1919},
	journal = {Science China Information Sciences},
	keywords = {Lustre, \#nosource, L2C},
	language = {en},
	month = jan,
	number = {1},
	pages = {12801},
	title = {A formally verified transformation to unify multiple nested clocks for a {Lustre}-like language},
	url = {http://link.springer.com/10.1007/s11432-016-9270-0},
	urldate = {2020-03-21},
	volume = {62},
	year = {2019}
}

@article{moggi_notions_1991,
	abstract = {The \ensuremath{\lamda}-calculus is considered an useful mathematical tool in the study of programming languages, since programs can be identified with \ensuremath{\lamda}-terms. However, if one goes further and uses \ensuremath{\beta}\ensuremath{\eta}-conversion to prove equivalence of programs, then a gross simplification is introduced (programs are identified with total functions from values to values), that may jeopardise the applicability of theoretical results. In this paper we introduce calculi based on a categorical semantics for computations, that provide a correct basis for proving equivalence of programs, for a wide range of notions of computation.},
	author = {Moggi, Eugenio},
	doi = {10.1016/0890-5401(91)90052-4},
	file = {MoggiE-Notions of computation and monads-Information and Computation.pdf:/home/linusboyle/Zotero/storage/IPB4UEZN/MoggiE-Notions of computation and monads-Information and Computation.pdf:application/pdf},
	issn = {08905401},
	journal = {Information and Computation},
	language = {en},
	month = jul,
	number = {1},
	pages = {55--92},
	title = {Notions of computation and monads},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0890540191900524},
	urldate = {2020-03-23},
	volume = {93},
	year = {1991}
}

@inproceedings{debacker_vertical_2017,
	abstract = {Aggressive pitch scaling in sub-10nm nodes has introduced complex design rules which make routing extremely challenging. Cell architectures have also been changed to meet the design rules. For example, metal layers below M1 are used to gain additional routing resources. New cell architectures wherein inter-row M1 routing is allowed force consideration of vertical alignment of cells. In this work, we propose a mixedinteger linear programming (MILP)-based, detailed placement optimization to maximize direct vertical M1 routing utilization for congestion and wirelength reduction.},
	address = {Austin, TX, USA},
	author = {Debacker, Peter and Han, Kwangsoo and Kahng, Andrew B. and Lee, Hyein and Raghavan, Praveen and Wang, Lutong},
	booktitle = {Proceedings of the 54th {Annual} {Design} {Automation} {Conference} 2017 on - {DAC} '17},
	file = {DebackerP et al-Vertical M1 Routing-Aware Detailed Placement for Congestion and Wirelength Reduction in Sub-10nm.pdf:/home/linusboyle/Zotero/storage/PPP55FZH/DebackerP et al-Vertical M1 Routing-Aware Detailed Placement for Congestion and Wirelength Reduction in Sub-10nm.pdf:application/pdf},
	isbn = {978-1-4503-4927-7},
	language = {en},
	pages = {1--6},
	publisher = {ACM Press},
	title = {Vertical {M1} {Routing}-{Aware} {Detailed} {Placement} for {Congestion} and {Wirelength} {Reduction} in {Sub}-10nm {Nodes}},
	url = {http://dl.acm.org/citation.cfm?doid=3061639.3062338},
	urldate = {2020-03-23},
	year = {2017}
}

@inproceedings{shi_trapl_2017,
	abstract = {We propose a framework to quickly analyze track congestion inside each g-cell at the global routing stage. Adistinguishing feature of our framework compared to prior work is estimating the locations of vias and partial track ut ilization by a g lobal segment inside each g-cell for a given global routing solution. We integrate this model with a proposed track assignment algorithm which we show can more effectively reduce track overlaps compared to prior work. A strength of this work is to evaluate the accuracy with respect to an accurat e congestion map generat ed by a commercial detailed router as reference. This work is a step towards bridging the gap between global and detailed routing which is an important obstacle facing modern Ie design.},
	address = {Austin, TX, USA},
	author = {Shi, Daohang and Davoodi, Azadeh},
	booktitle = {Proceedings of the 54th {Annual} {Design} {Automation} {Conference} 2017 on - {DAC} '17},
	file = {ShiD&DavoodiA-TraPL - Track Planning of Local Congestion for Global Routing.pdf:/home/linusboyle/Zotero/storage/KU8S487W/ShiD&DavoodiA-TraPL - Track Planning of Local Congestion for Global Routing.pdf:application/pdf},
	isbn = {978-1-4503-4927-7},
	language = {en},
	pages = {1--6},
	publisher = {ACM Press},
	shorttitle = {{TraPL}},
	title = {{TraPL}: {Track} {Planning} of {Local} {Congestion} for {Global} {Routing}},
	url = {http://dl.acm.org/citation.cfm?doid=3061639.3062335},
	urldate = {2020-03-23},
	year = {2017}
}

@inproceedings{kim_fast_2017,
	abstract = {Incremental timing-driven placement (TDP) is one of the most crucial steps for timing closure in a physical design. The need for highperformance incremental TDP continues to grow, but prior studies have focused on optimizing only setup timing slacks, which can be easily stuck in local optima. In this paper, we present a useful skew methodology based on a maximum mean weight cycle (MMWC) approach in the incremental TDP. The proposed useful skew methodology finds an optimal clock latency for each flip-flop, and the clock latency is implemented by moving the flip-flops and/ or reassigning them to local clock buffers. With the proposed TDP method, we effectively reduce the early slack of ICCAD 2015 contest benchmarks, and achieve 124(\%) and 78(\%) of total quality score improvement compared to the 2015 contest winner, and early slack histogram compression (EHC) method, respectively. Moreover, with fewer iterations in the optimization, the runtime of our predictive useful skew method is an average of 7.4 times faster than an EHC method.},
	address = {Austin, TX, USA},
	author = {Kim, Seungwon and Do, SangGi and Kang, Seokhyeong},
	booktitle = {Proceedings of the 54th {Annual} {Design} {Automation} {Conference} 2017 on - {DAC} '17},
	file = {KimS et al-Fast Predictive Useful Skew Methodology for Timing-Driven Placement Optimization.pdf:/home/linusboyle/Zotero/storage/F5KTA5PJ/KimS et al-Fast Predictive Useful Skew Methodology for Timing-Driven Placement Optimization.pdf:application/pdf},
	isbn = {978-1-4503-4927-7},
	language = {en},
	pages = {1--6},
	publisher = {ACM Press},
	title = {Fast {Predictive} {Useful} {Skew} {Methodology} for {Timing}-{Driven} {Placement} {Optimization}},
	url = {http://dl.acm.org/citation.cfm?doid=3061639.3062247},
	urldate = {2020-03-23},
	year = {2017}
}

@inproceedings{seitanidis_timing_2017,
	abstract = {To reduce clock power, we present a novel timing-driven incremental multi-bit register (MBR) composition methodology for designs that may be rich in MBRs after logic synthesis. It identifies nearby compatible registers that can be merged without degrading timing, and without reducing the ``useful clock skew'' potential. These registers are merged providing the MBR placement can be legalized according to the proposed simplified physical constraints. A new integer linear programming (ILP) formulation minimizes the total number of registers in the design. It significantly reduces register count and clock capacitance, without adding any timing/routing/placement violations and without increasing the total wire-length of the designs, as shown by experimental results on industrial benchmarks.},
	address = {Austin, TX, USA},
	author = {Seitanidis, Ioannis and Dimitrakopoulos, Giorgos and Matheakis, Pavlos and Masse-Navete, Laurent and Chinnery, David},
	booktitle = {Proceedings of the 54th {Annual} {Design} {Automation} {Conference} 2017 on - {DAC} '17},
	file = {SeitanidisI et al-Timing Driven Incremental Multi-Bit Register Composition Using a Placement-Aware ILP formulation.pdf:/home/linusboyle/Zotero/storage/3RNUKCRU/SeitanidisI et al-Timing Driven Incremental Multi-Bit Register Composition Using a Placement-Aware ILP formulation.pdf:application/pdf},
	isbn = {978-1-4503-4927-7},
	language = {en},
	pages = {1--6},
	publisher = {ACM Press},
	title = {Timing {Driven} {Incremental} {Multi}-{Bit} {Register} {Composition} {Using} a {Placement}-{Aware} {ILP} formulation},
	url = {http://dl.acm.org/citation.cfm?doid=3061639.3062327},
	urldate = {2020-03-23},
	year = {2017}
}

@inproceedings{seo_pin_2017,
	abstract = {The layout of standard cells is very dense these days, so some pins are hard to get access to. This is in particular true in complex cells with many pins (e.g. AOI) and in the layout where many of those cells are densely packed without much whitespace. We redesign those complex cells, so a library now contains both original cell and its new version with easier pin access; a systematic method is proposed to pick candidate cells for redesign and to dictate how redesign should be performed. We also introduce a measure of inaccessibility of pins in a cell, named IOC. Placement optimization is performed, which uses IOC to determine which cells should be replaced by its redesigned version and how whitespace should be redistributed. Experiments with 12 test circuits indicate that the number of routing errors (after the initial placement) is reduced by 82\% on average, and the subsequent detailed routing takes 72\% less runtime.},
	address = {Austin, TX, USA},
	author = {Seo, Jaewoo and Jung, Jinwook and Kim, Sangmin and Shin, Youngsoo},
	booktitle = {Proceedings of the 54th {Annual} {Design} {Automation} {Conference} 2017 on - {DAC} '17},
	file = {SeoJ et al-Pin Accessibility-Driven Cell Layout Redesign and Placement Optimization.pdf:/home/linusboyle/Zotero/storage/CW8UIAXU/SeoJ et al-Pin Accessibility-Driven Cell Layout Redesign and Placement Optimization.pdf:application/pdf},
	isbn = {978-1-4503-4927-7},
	language = {en},
	pages = {1--6},
	publisher = {ACM Press},
	title = {Pin {Accessibility}-{Driven} {Cell} {Layout} {Redesign} and {Placement} {Optimization}},
	url = {http://dl.acm.org/citation.cfm?doid=3061639.3062302},
	urldate = {2020-03-23},
	year = {2017}
}

@inproceedings{yang_layout_2017,
	abstract = {Detecting layout hotspots is one of the key problems in physical verification flow. Although machine learning solutions show benefits over lithography simulation and pattern matching based methods, it is still hard to select a proper model for large scale problems and it is inevitable that performance degradation will occur. To overcome these issues, in this paper we develop a deep learning framework for high performance and large scale hotspot detection. First, feature tensor generation is proposed to extract representative layout features that fit well with convolutional neural networks while keeping the spatial relationship of the original layout pattern with minimal information loss. Second, we propose a biased learning algorithm to train the convolutional neural network to further improve detection accuracy with small false alarm penalties. Experimental results show that our framework outperforms previous machine learning-based hotspot detectors in both the ICCAD 2012 Contest benchmarks and large scale industrial benchmarks.},
	address = {Austin, TX, USA},
	author = {Yang, Haoyu and Su, Jing and Zou, Yi and Yu, Bei and Young, Evangeline F. Y.},
	booktitle = {Proceedings of the 54th {Annual} {Design} {Automation} {Conference} 2017 on - {DAC} '17},
	file = {YangH et al-Layout Hotspot Detection with Feature Tensor Generation and Deep Biased Learning.pdf:/home/linusboyle/Zotero/storage/HFHQAKXN/YangH et al-Layout Hotspot Detection with Feature Tensor Generation and Deep Biased Learning.pdf:application/pdf},
	isbn = {978-1-4503-4927-7},
	keywords = {EDA与AI},
	language = {en},
	pages = {1--6},
	publisher = {ACM Press},
	title = {Layout {Hotspot} {Detection} with {Feature} {Tensor} {Generation} and {Deep} {Biased} {Learning}},
	url = {http://dl.acm.org/citation.cfm?doid=3061639.3062270},
	urldate = {2020-03-23},
	year = {2017}
}

@inproceedings{lin_detailed_2017,
	abstract = {Two-dimensional directed self-assembly (2D-DSA), a promising nanotechnology, manipulates the orientation of double posts to guide block copolymers to fabricate 2D patterns in nanoscale. In this paper, we present the first detailed placement algorithm for 2D-DSA. We first propose an orientationnumber model for nets to estimate post orientations. Then, a cost model based on the orientation numbers is defined to estimate the 2D-DSA routability efficiently during placement, and a novel detailed placement framework is proposed to consider the orientations to optimize wirelength. In this framework, we develop a dynamic-programming-based algorithm for single-row detailed placement with a specialized pruning technique based on the defined orientation cost. Experimental results show that our algorithm can effectively generate a 2D-DSA friendly placement solution.},
	address = {Austin, TX, USA},
	author = {Lin, Zhi-Wen and Chang, Yao-Wen},
	booktitle = {Proceedings of the 54th {Annual} {Design} {Automation} {Conference} 2017 on - {DAC} '17},
	file = {LinZ&ChangY-Detailed Placement for Two-Dimensional Directed Self-Assembly Technology.pdf:/home/linusboyle/Zotero/storage/A3QV7J27/LinZ&ChangY-Detailed Placement for Two-Dimensional Directed Self-Assembly Technology.pdf:application/pdf},
	isbn = {978-1-4503-4927-7},
	language = {en},
	pages = {1--6},
	publisher = {ACM Press},
	title = {Detailed {Placement} for {Two}-{Dimensional} {Directed} {Self}-{Assembly} {Technology}},
	url = {http://dl.acm.org/citation.cfm?doid=3061639.3062229},
	urldate = {2020-03-23},
	year = {2017}
}

@inproceedings{xu_concurrent_2017,
	abstract = {In advanced technology nodes, standard cell pin access is becoming challenging due to a small number of routing tracks and complex design-for-manufacturing constraints. Pin access interference is further exacerbated by unidirectional routing, which is highly preferred to enable high-density metal patterns and comply with self-aligned multiple patterning solutions. Previous manufacturing-aware routing studies simply depend on the router or sequential planning schemes to resolve pin access interference, which introduces significant overhead on solution qualities. Therefore, we propose concurrent pin access optimization techniques to achieve fast and high-quality routing solutions. The concurrent pin access optimization is modeled as a weighted interval assignment problem, which is solved by an optimal integer linear programming formulation and a scalable Lagrangian relaxation algorithm. A concurrent pin access router is implemented while accommodating advanced manufacturing constraints, which outperforms state-of-the-art manufacturing-aware routers with better routability, fewer vias and faster runtime.},
	address = {Austin, TX, USA},
	author = {Xu, Xiaoqing and Lin, Yibo and Livramento, Vinicius and Pan, David Z.},
	booktitle = {Proceedings of the 54th {Annual} {Design} {Automation} {Conference} 2017 on - {DAC} '17},
	file = {XuX et al-Concurrent Pin Access Optimization for Unidirectional Routing.pdf:/home/linusboyle/Zotero/storage/R58F4PQE/XuX et al-Concurrent Pin Access Optimization for Unidirectional Routing.pdf:application/pdf},
	isbn = {978-1-4503-4927-7},
	language = {en},
	pages = {1--6},
	publisher = {ACM Press},
	title = {Concurrent {Pin} {Access} {Optimization} for {Unidirectional} {Routing}},
	url = {http://dl.acm.org/citation.cfm?doid=3061639.3062214},
	urldate = {2020-03-23},
	year = {2017}
}

@inproceedings{park_road_2019,
	abstract = {Routability diagnosis has increasingly become the bottleneck in detailed routing for sub-10nm technology due to the limited tracks, high density, and complex design rules. The conventional ways to examine the routability of detailed routing are ILP- and SAT-based techniques. However, once we identify the routability, the diagnosis remains an open problem for physical designers. In this paper, we propose a novel framework, called ROAD, which diagnoses explicit reasons for routing failures. The proposed ROAD framework utilizes a diagnosis-friendly SAT formulation to represent design{\rq}s layout and diagnoses the routability with SAT solving techniques. Based on the diagnosis, ROAD provides human-interpretable explanations for conflicted routing conditions. To show the practical value of our framework, we also generate comprehensive test-sets that enable exhaustive exploration of layouts based on Rent{\rq}s rule. We demonstrate that ROAD successfully examines conflict causes for diverse pin layouts. Throughout extensive diagnosis, we also present several key findings for design failure. ROAD performs routability diagnosis within 2 minutes on average for 90 grids testsets, while diagnosing the exact causes of routing failures in terms of congestion and conditional design rules.},
	address = {San Francisco, CA, USA},
	author = {Park, Dongwon and Kang, Ilgweon and Kim, Yeseong and Gao, Sicun and Lin, Bill and Cheng, Chung-Kuan},
	booktitle = {Proceedings of the 2019 {International} {Symposium} on {Physical} {Design} - {ISPD} '19},
	file = {ParkD et al-ROAD - Routability Analysis and Diagnosis Framework Based on SAT Techniques.pdf:/home/linusboyle/Zotero/storage/53NMLWRB/ParkD et al-ROAD - Routability Analysis and Diagnosis Framework Based on SAT Techniques.pdf:application/pdf},
	isbn = {978-1-4503-6253-5},
	language = {en},
	pages = {65--72},
	publisher = {ACM Press},
	shorttitle = {{ROAD}},
	title = {{ROAD}: {Routability} {Analysis} and {Diagnosis} {Framework} {Based} on {SAT} {Techniques}},
	url = {http://dl.acm.org/citation.cfm?doid=3299902.3309752},
	urldate = {2020-03-23},
	year = {2019}
}

@inproceedings{truppel_psion_2019,
	address = {San Francisco, CA, USA},
	author = {Truppel, Alexandre and Tseng, Tsun-Ming and Bertozzi, Davide and Alves, Jos{\'e} Carlos and Schlichtmann, Ulf},
	booktitle = {Proceedings of the 2019 {International} {Symposium} on {Physical} {Design} - {ISPD} '19},
	file = {TruppelA et al-PSION - Combining Logical Topology and Physical Layout Optimization for Wavelength-Routed ONoCs.pdf:/home/linusboyle/Zotero/storage/H3WPDVA2/TruppelA et al-PSION - Combining Logical Topology and Physical Layout Optimization for Wavelength-Routed ONoCs.pdf:application/pdf},
	isbn = {978-1-4503-6253-5},
	language = {en},
	pages = {49--56},
	publisher = {ACM Press},
	shorttitle = {{PSION}},
	title = {{PSION}: {Combining} {Logical} {Topology} and {Physical} {Layout} {Optimization} for {Wavelength}-{Routed} {ONoCs}},
	url = {http://dl.acm.org/citation.cfm?doid=3299902.3309747},
	urldate = {2020-03-23},
	year = {2019}
}

@inproceedings{ryzhenko_pin_2019,
	abstract = {In this paper, we propose a routing flow for nets within a standard cell that generates layout of standard cells without any design rule violations. Design rules, density rules for metal fill, and pin-access requirements are modeled via Boolean formulas for discrete layout objects on grids. Formulas are translated into a single Boolean satisfiability problem (SAT). Having constraints on net connectivity and candidate routes, the SAT solver produces legal and complete routing concurrently for all nets satisfying mandatory pin-access and density requirements. A SAT-based optimization engine minimizes undesired layout patterns such as DFM (design-for-manufacturing) hot spots.},
	address = {San Francisco, CA, USA},
	author = {Ryzhenko, Nikolay and Burns, Steven and Sorokin, Anton and Talalay, Mikhail},
	booktitle = {Proceedings of the 2019 {International} {Symposium} on {Physical} {Design} - {ISPD} '19},
	file = {RyzhenkoN et al-Pin Access-Driven Design Rule Clean and DFM Optimized Routing of Standard Cells under Boolean.pdf:/home/linusboyle/Zotero/storage/Y2SX6EZW/RyzhenkoN et al-Pin Access-Driven Design Rule Clean and DFM Optimized Routing of Standard Cells under Boolean.pdf:application/pdf},
	isbn = {978-1-4503-6253-5},
	language = {en},
	pages = {41--47},
	publisher = {ACM Press},
	title = {Pin {Access}-{Driven} {Design} {Rule} {Clean} and {DFM} {Optimized} {Routing} of {Standard} {Cells} under {Boolean} {Constraints}},
	url = {http://dl.acm.org/citation.cfm?doid=3299902.3309744},
	urldate = {2020-03-23},
	year = {2019}
}

@inproceedings{li_analytical_2019,
	abstract = {Modern circuit designs often contain standard cells of different row heights to meet various design requirements. Due to the higher interference among heterogeneous cell structures, the legalization problem for mixed-cell-height standard cells becomes more challenging. In this paper, we present an analytical legalization algorithm for mixed-cell-height standard cells to simultaneously minimize the average and the maximum cell movements. We formulate it as a mixed integer quadratic programming problem (MIQP), which allows cell spreading concurrently in both the horizontal and vertical directions. By relaxing its discrete constraints to linear ones, we convert the MIQP into a quadratic programming problem (QP). To solve the QP efficiently, we further reformulate it as a linear complementarity problem (LCP), and solve the LCP by a modulus-based matrix splitting iteration method (MMSIM). To guarantee the convergence of the MMSIM and the equivalence between the QP and the LCP, we use a series of operations to ensure that its induced objective matrix is symmetric positive definite and its constraint matrix is of full row rank. Experimental results demonstrate the effectiveness of our algorithm in reducing both the average and the maximum cell movements for mixed-cell-height legalization.},
	address = {San Francisco, CA, USA},
	author = {Li, Xingquan and Chen, Jianli and Zhu, Wenxing and Chang, Yao-Wen},
	booktitle = {Proceedings of the 2019 {International} {Symposium} on {Physical} {Design} - {ISPD} '19},
	file = {LiX et al-Analytical Mixed-Cell-Height Legalization Considering Average and Maximum Movement Minimization.pdf:/home/linusboyle/Zotero/storage/KS7Q92D3/LiX et al-Analytical Mixed-Cell-Height Legalization Considering Average and Maximum Movement Minimization.pdf:application/pdf},
	isbn = {978-1-4503-6253-5},
	language = {en},
	pages = {27--34},
	publisher = {ACM Press},
	title = {Analytical {Mixed}-{Cell}-{Height} {Legalization} {Considering} {Average} and {Maximum} {Movement} {Minimization}},
	url = {http://dl.acm.org/citation.cfm?doid=3299902.3309750},
	urldate = {2020-03-23},
	year = {2019}
}

@inproceedings{liu_ispd_2019,
	abstract = {Detailed routing becomes the most complicated and runtime consuming stage in the physical design flow as technology nodes advance. Due to the inaccessibility of advanced routing rules and industrial designs, it is hard to conduct detailed routing academic researches using the modern real-world designs. ISPD18 hosts the first detailed routing contest [1] and releases a set of benchmarks synthesized by industrial tools with practical routing rules. ISPD18 contest spurs detailed routing researches and provides students the opportunity to become familiar with the industrial designs and rules. On top of ISPD18 detailed routing contest, we host another detailed routing contest in ISPD19 [2] to consider several advanced routing rules and make the contest problem one step closer to the real-world routing challenges in advanced technology nodes. ISPD19 detailed routing contest encourages participants to use double-cut vias to improve yield and result quality. In addition, in order to drive the development of efficient routing frameworks, the deterministic multithreading feature is encouraged but optional in this contest.},
	address = {San Francisco, CA, USA},
	author = {Liu, Wen-Hao and Mantik, Stefanus and Chow, Wing-Kai and Ding, Yixiao and Farshidi, Amin and Posser, Gracieli},
	booktitle = {Proceedings of the 2019 {International} {Symposium} on {Physical} {Design} - {ISPD} '19},
	file = {LiuW et al-ISPD 2019 Initial Detailed Routing Contest and Benchmark with Advanced Routing Rules.pdf:/home/linusboyle/Zotero/storage/L4KR9S8U/LiuW et al-ISPD 2019 Initial Detailed Routing Contest and Benchmark with Advanced Routing Rules.pdf:application/pdf},
	isbn = {978-1-4503-6253-5},
	language = {en},
	pages = {147--151},
	publisher = {ACM Press},
	title = {{ISPD} 2019 {Initial} {Detailed} {Routing} {Contest} and {Benchmark} with {Advanced} {Routing} {Rules}},
	url = {http://dl.acm.org/citation.cfm?doid=3299902.3311067},
	urldate = {2020-03-23},
	year = {2019}
}

@inproceedings{xu_device_2019,
	abstract = {The layouts of analog/mixed-signal (AMS) integrated circuits (ICs) are dramatically different from their digital counterparts. AMS circuit layouts usually include a variety of devices, including transistors, capacitors, resistors, and inductors. A complicated AMS IC system with hierarchical structure may also consist of pre-laid out subcircuits. Different types of devices can occupy different manufacturing layers. Therefore, during the layout stage, the devices require co-optimization to achieve high circuit performance. Leveraging the fact that some devices can be built by mutually exclusive layers, they can be carefully designed to overlap each other to effectively reduce the total area and wirelength without degrading the circuit performance. In this paper, we propose an analytical framework to tackle the device layer-aware analog placement problem. Experimental results show that on average the proposed techniques can reduce the total area and half-perimeter wirelength by 9\% and 23\%, respectively. To verify the routability of the placement results, we also develop an analog global router, which demonstrates that the device layer-aware placement can achieve 18\% shorter wirelength during global routing.},
	address = {San Francisco, CA, USA},
	author = {Xu, Biying and Li, Shaolan and Pui, Chak-Wa and Liu, Derong and Shen, Linxiao and Lin, Yibo and Sun, Nan and Pan, David Z.},
	booktitle = {Proceedings of the 2019 {International} {Symposium} on {Physical} {Design} - {ISPD} '19},
	file = {XuB et al-Device Layer-Aware Analytical Placement for Analog Circuits.pdf:/home/linusboyle/Zotero/storage/ISG93Q27/XuB et al-Device Layer-Aware Analytical Placement for Analog Circuits.pdf:application/pdf},
	isbn = {978-1-4503-6253-5},
	language = {en},
	pages = {19--26},
	publisher = {ACM Press},
	title = {Device {Layer}-{Aware} {Analytical} {Placement} for {Analog} {Circuits}},
	url = {http://dl.acm.org/citation.cfm?doid=3299902.3309751},
	urldate = {2020-03-23},
	year = {2019}
}

@inproceedings{netto_how_2019,
	abstract = {Machine learning has been used to improve the predictability of different physical design problems, such as timing, clock tree synthesis and routing, but not for legalization. Predicting the outcome of legalization can be helpful to guide incremental placement and circuit partitioning, speeding up those algorithms. In this work we extract histograms of features and snapshots of the circuit from several regions in a way that the model can be trained independently from region size. Then, we evaluate how traditional and convolutional deep learning models use this set of features to predict the quality of a legalization algorithm without having to executing it. When evaluating the models with holdout cross validation, the best model achieves an accuracy of 80\% and an F-score of at least 0.7. Finally, we used the best model to prune partitions with large displacement in a circuit partitioning strategy. Experimental results in circuits (with up to millions of cells) showed that the pruning strategy improved the maximum displacement of the legalized solution by 5\% to 94\%. In addition, using the machine learning model avoided from 22\% to 99\% of the calls to the legalization algorithm, which speeds up the pruning process by up to 3{\texttimes}.},
	address = {San Francisco, CA, USA},
	author = {Netto, Renan and Fabre, Sheiny and Fontana, Tiago Augusto and Livramento, Vinicius and Pilla, La{\'e}rcio and G{\"u}ntzel, Jos{\'e} Lu{\'\i}s},
	booktitle = {Proceedings of the 2019 {International} {Symposium} on {Physical} {Design} - {ISPD} '19},
	file = {NettoR et al-How Deep Learning Can Drive Physical Synthesis Towards More Predictable Legalization.pdf:/home/linusboyle/Zotero/storage/Q5US6IMW/NettoR et al-How Deep Learning Can Drive Physical Synthesis Towards More Predictable Legalization.pdf:application/pdf},
	isbn = {978-1-4503-6253-5},
	language = {en},
	pages = {3--10},
	publisher = {ACM Press},
	title = {How {Deep} {Learning} {Can} {Drive} {Physical} {Synthesis} {Towards} {More} {Predictable} {Legalization}},
	url = {http://dl.acm.org/citation.cfm?doid=3299902.3309754},
	urldate = {2020-03-23},
	year = {2019}
}

@inproceedings{lin_construction_2019,
	abstract = {Monolithic three-dimensional (3D) integration enables stacking multiple ultra-thin silicon tiers in a single package, thereby providing smaller footprint area, shorter wirelength, higher performance, and lower power consumption than conventional planar fabrication technologies. Physical design of monolithic 3D integrated circuits (ICs) requires several design steps such as 3D placement, 3D clock-tree synthesis, 3D routing, and 3D optimization. Among the steps, 3D routing is very time-consuming due to numerous routing blockages. Thus, 3D routing is typically performed in two sub-steps, monolithic inter-layer via (MIV) insertion and tier-by-tier routing. In this paper, we propose an algorithm to build a routing topology database that can be used to construct all multilayer monolithic rectilinear Steiner minimum trees on the 3D Hanan grid. The database will help 3D routers reduce the runtime of the MIV insertion step and improve the quality of the 3D routing.},
	address = {San Francisco, CA, USA},
	author = {Lin, Sheng-En David and Kim, Dae Hyun},
	booktitle = {Proceedings of the 2019 {International} {Symposium} on {Physical} {Design} - {ISPD} '19},
	file = {LinS&KimD-Construction of All Multilayer Monolithic Rectilinear Steiner Minimum Trees on the 3D Hanan Grid.pdf:/home/linusboyle/Zotero/storage/IFZAQ3DT/LinS&KimD-Construction of All Multilayer Monolithic Rectilinear Steiner Minimum Trees on the 3D Hanan Grid.pdf:application/pdf},
	isbn = {978-1-4503-6253-5},
	language = {en},
	pages = {57--64},
	publisher = {ACM Press},
	title = {Construction of {All} {Multilayer} {Monolithic} {Rectilinear} {Steiner} {Minimum} {Trees} on the {3D} {Hanan} {Grid} for {Monolithic} {3D} {IC} {Routing}},
	url = {http://dl.acm.org/citation.cfm?doid=3299902.3309749},
	urldate = {2020-03-23},
	year = {2019}
}

@inproceedings{chang_graceful_2019,
	address = {San Francisco, CA, USA},
	author = {Chang, Ya-Chu and Lin, Tung-Wei and Jiang, Iris Hui-Ru and Nam, Gi-Joon},
	booktitle = {Proceedings of the 2019 {International} {Symposium} on {Physical} {Design} - {ISPD} '19},
	file = {ChangY et al-Graceful Register Clustering by Effective Mean Shift Algorithm for Power and Timing Balancing.pdf:/home/linusboyle/Zotero/storage/XX5Z4FY4/ChangY et al-Graceful Register Clustering by Effective Mean Shift Algorithm for Power and Timing Balancing.pdf:application/pdf},
	isbn = {978-1-4503-6253-5},
	language = {en},
	pages = {11--18},
	publisher = {ACM Press},
	title = {Graceful {Register} {Clustering} by {Effective} {Mean} {Shift} {Algorithm} for {Power} and {Timing} {Balancing}},
	url = {http://dl.acm.org/citation.cfm?doid=3299902.3309753},
	urldate = {2020-03-23},
	year = {2019}
}

@article{moschovakis_lecture_nodate,
	author = {Moschovakis, Yiannis N},
	file = {MoschovakisY-LECTURE NOTES IN LOGIC.pdf:/home/linusboyle/Zotero/storage/GSJY7ZW8/MoschovakisY-LECTURE NOTES IN LOGIC.pdf:application/pdf},
	keywords = {⛔ No DOI found},
	language = {en},
	pages = {319},
	title = {{LECTURE} {NOTES} {IN} {LOGIC}}
}

@inproceedings{chen_digital_2019,
	abstract = {Mixed-signal time-domain computing (TC) has recently drawn significant attention due to its high efficiency in applications such as machine learning accelerators. However, due to the nature of analog and mixed-signal design, there is a lack of a systematic flow of synthesis and place \& route for time-domain circuits. This paper proposed a comprehensive design flow for TC. In the front-end, a variation-aware digital compatible synthesis flow is proposed. In the back-end, a placement technique using graph-based optimization engine is proposed to deal with the especially stringent matching requirement in TC. Simulation results show significant improvement over the prior analog placement methods. A 55nm test chip is used to demonstrate that the proposed design flow can meet the stringent timing matching target for TC with significant performance boost over conventional digital design.},
	author = {Chen, Zhengyu and Zhou, Hai and Gu, Jie},
	booktitle = {2019 56th {ACM}/{IEEE} {Design} {Automation} {Conference} ({DAC})},
	file = {ChenZ et al-Digital Compatible Synthesis, Placement and Implementation of Mixed-Signal Time-Domain Computing.html:/home/linusboyle/Zotero/storage/ZF8S5PHB/ChenZ et al-Digital Compatible Synthesis, Placement and Implementation of Mixed-Signal Time-Domain Computing.html:text/html; ChenZ et al-Digital Compatible Synthesis, Placement and Implementation of Mixed-Signal Time-Domain Computing.pdf:/home/linusboyle/Zotero/storage/TLSMU52A/ChenZ et al-Digital Compatible Synthesis, Placement and Implementation of Mixed-Signal Time-Domain Computing.pdf:application/pdf},
	keywords = {analog placement methods, analogue integrated circuits, circuit optimisation, comprehensive design flow, conventional digital design, graph theory, graph-based optimization engine, integrated circuit layout, integrated circuit testing, mixed analogue-digital integrated circuits, mixed-signal design, mixed-signal time-domain computing, Neural networks, Optimization, pattern matching, Sensitivity, size 55.0 nm, stringent timing matching target, systematic flow, test chip, time-domain analysis, Time-domain analysis, time-domain circuits, variation-aware digital compatible synthesis flow},
	month = jun,
	note = {ISSN: 0738-100X},
	pages = {1--6},
	title = {Digital {Compatible} {Synthesis}, {Placement} and {Implementation} of {Mixed}-{Signal} {Time}-{Domain} {Computing}},
	year = {2019}
}

@inproceedings{martin_flat_2019,
	abstract = {In this paper, we propose a novel, flat analytic timing-driven placer without explicit packing for Xilinx UltraScale FPGA devices. Our work uses novel methods to simultaneously optimize for timing, wirelength and congestion throughout the global and detailed placement stages. We evaluate the effectiveness of the flat placer on the ISPD 2016 benchmark suite for the xcvu095 UltraScale device, as well as on industrial benchmarks. Experimental results show that on average, FTPlace achieves an 8\% increase in maximum clock rate, an 18\% decrease in routed wirelength, and produces placements that require 80\% less time to route when compared to Xilinx Vivado 2018.1.},
	author = {Martin, Timothy and Maarouf, Dani and Abuowaimer, Ziad and Alhyari, Abeer and Grewal, Gary and Areibi, Shawki},
	booktitle = {2019 56th {ACM}/{IEEE} {Design} {Automation} {Conference} ({DAC})},
	file = {MartinT et al-A Flat Timing-Driven Placement Flow for Modern FPGAs.html:/home/linusboyle/Zotero/storage/RS5ATAJA/MartinT et al-A Flat Timing-Driven Placement Flow for Modern FPGAs.html:text/html; MartinT et al-A Flat Timing-Driven Placement Flow for Modern FPGAs.pdf:/home/linusboyle/Zotero/storage/6FUGJ5PP/MartinT et al-A Flat Timing-Driven Placement Flow for Modern FPGAs.pdf:application/pdf},
	keywords = {circuit optimisation, field programmable gate arrays, flat analytic timing-driven placer, flat timing-driven placement flow, FPGA, FTPlace, global placement stages, industrial benchmarks, ISPD 2016 benchmark suite, maximum clock rate, modern FPGAs, Placement, routed wirelength, Timing Driven, UltraScale Architecture, xcvu095 UltraScale device, Xilinx UltraScale FPGA devices, Xilinx Vivado 2018.1},
	month = jun,
	note = {ISSN: 0738-100X},
	pages = {1--6},
	title = {A {Flat} {Timing}-{Driven} {Placement} {Flow} for {Modern} {FPGAs}},
	year = {2019}
}

@article{dean_deep_nodate,
	abstract = {The past decade has seen a remarkable series of advances in machine learning, and in particular deep learning approaches based on artificial neural networks, to improve our abilities to build more accurate systems across a broad range of areas, including computer vision, speech recognition, language translation, and natural language understanding tasks. This paper is a companion paper to a keynote talk at the 2020 International Solid-State Circuits Conference (ISSCC) discussing some of the advances in machine learning, and their implications on the kinds of computational devices we need to build, especially in the post-Moore{\rq}s Law-era. It also discusses some of the ways that machine learning may also be able to help with some aspects of the circuit design process. Finally, it provides a sketch of at least one interesting direction towards much larger-scale multi-task models that are sparsely activated and employ much more dynamic, example- and task-based routing than the machine learning models of today.},
	author = {Dean, Jeffrey},
	file = {DeanJ-The Deep Learning Revolution and Its Implications for Computer Architecture and Chip Design.pdf:/home/linusboyle/Zotero/storage/49SNAGEV/DeanJ-The Deep Learning Revolution and Its Implications for Computer Architecture and Chip Design.pdf:application/pdf},
	keywords = {⛔ No DOI found, Read, 数字系统设计课, EDA与AI},
	language = {en},
	pages = {17},
	title = {The {Deep} {Learning} {Revolution} and {Its} {Implications} for {Computer} {Architecture} and {Chip} {Design}}
}

@inproceedings{pui_clock-aware_2017,
	abstract = {As the complexity and scale of circuits keep growing, clocking architectures of FPGAs have become more complex to meet the timing requirement. In this paper, to optimize wirelength and meanwhile meet emerging clocking architectural constraints, we propose several detailed placement techniques, i.e., two-step clock constraint legalization and chain move. After integrating these techniques into our FPGA placement framework, experimental results on ISPD 2017 benchmarks show that our proposed approach yields 2.3\% shorter routed wirelength and the running time is 2{\texttimes} faster compared to the first place winner in the ISPD 2017 contest. Moreover, we explore the possibilities to use machine learning-based methods to predict routing congestion in UltraScale FPGAs. Experimental results on both ISPD 2016 and ISPD 2017 benchmarks show that our proposed congestion estimation model is a good approximation to the one obtained from Vivado and can lead to good placement results compared to the previous methods.},
	address = {Irvine, CA},
	author = {Pui, Chak-Wa and Chen, Gengjie and Ma, Yuzhe and Young, Evangeline F. Y. and Yu, Bei},
	booktitle = {2017 {IEEE}/{ACM} {International} {Conference} on {Computer}-{Aided} {Design} ({ICCAD})},
	file = {PuiC et al-Clock-aware ultrascale FPGA placement with machine learning routability prediction - (Invited paper).pdf:/home/linusboyle/Zotero/storage/7VGALVZD/PuiC et al-Clock-aware ultrascale FPGA placement with machine learning routability prediction - (Invited paper).pdf:application/pdf},
	isbn = {978-1-5386-3093-8},
	keywords = {软件前沿报告, Read, EDA与AI},
	language = {en},
	month = nov,
	pages = {929--936},
	publisher = {IEEE},
	shorttitle = {Clock-aware ultrascale {FPGA} placement with machine learning routability prediction},
	title = {Clock-aware ultrascale {FPGA} placement with machine learning routability prediction: ({Invited} paper)},
	url = {http://ieeexplore.ieee.org/document/8203880/},
	urldate = {2020-03-25},
	year = {2017}
}

@inproceedings{zhao_machine_2019,
	abstract = {High-level synthesis (HLS) shortens the development time of hardware designs and enables faster design space exploration at a higher abstraction level. Optimization of complex applications in HLS is challenging due to the effects of implementation issues such as routing congestion. Routing congestion estimation is absent or inaccurate in existing HLS design methods and tools. Early and accurate congestion estimation is of great benefit to guide the optimization in HLS and improve the efficiency of implementation. However, routability, a serious concern in FPGA designs, has been difficult to evaluate in HLS without analyzing post-implementation details after Place and Route. To this end, we propose a novel method to predict routing congestion in HLS using machine learning and map the expected congested regions in the design to the relevant high-level source code. This is greatly beneficial in early identification of routability oriented bottlenecks in the high-level source code without running timeconsuming register-transfer level (RTL) implementation flow. Experiments demonstrate that our approach accurately estimates vertical and horizontal routing congestion with errors of 6.71\% and 10.05\% respectively. By presenting Face Detection application as a case study, we show that by discovering the bottlenecks in high-level source code, routing congestion can be easily and quickly resolved compared to the efforts involved in RTL level implementation and design feedback.},
	address = {Florence, Italy},
	author = {Zhao, Jieru and Liang, Tingyuan and Sinha, Sharad and Zhang, Wei},
	booktitle = {2019 {Design}, {Automation} \& {Test} in {Europe} {Conference} \& {Exhibition} ({DATE})},
	file = {ZhaoJ et al-Machine Learning Based Routing Congestion Prediction in FPGA High-Level Synthesis.pdf:/home/linusboyle/Zotero/storage/NN963FHA/ZhaoJ et al-Machine Learning Based Routing Congestion Prediction in FPGA High-Level Synthesis.pdf:application/pdf},
	isbn = {978-3-9819263-2-3},
	language = {en},
	month = mar,
	pages = {1130--1135},
	publisher = {IEEE},
	title = {Machine {Learning} {Based} {Routing} {Congestion} {Prediction} in {FPGA} {High}-{Level} {Synthesis}},
	url = {https://ieeexplore.ieee.org/document/8714724/},
	urldate = {2020-03-25},
	year = {2019}
}

@inproceedings{hyun_accurate_2019,
	abstract = {Placement-aware synthesis, which combines logic synthesis with virtual placement and routing (P\&R) to better take account of wiring, has been popular for timing closure. The wirelength after virtual placement is correlated to actual wirelength, but correlation is not strong enough for some chosen paths. An algorithm to predict the actual wirelength from placement-aware synthesis is presented. It extracts a number of parameters from a given virtual path. A handful of synthetic parameters are compiled through linear discriminant analysis (LDA), and they are submitted to a few machine learning models. The final prediction of actual wirelength is given by the weighted sum of prediction from such machine learning models, in which weight is determined by the population of neighbors in parameter space. Experiments indicate that the predicted wirelength is 93\% accurate compared to actual wirelength; this can be compared to conventional virtual placement, in which wirelength is predicted with only 79\% accuracy.},
	address = {Florence, Italy},
	author = {Hyun, Daijoon and Fan, Yuepeng and Shin, Youngsoo},
	booktitle = {2019 {Design}, {Automation} \& {Test} in {Europe} {Conference} \& {Exhibition} ({DATE})},
	file = {HyunD et al-Accurate Wirelength Prediction for Placement-Aware Synthesis through Machine Learning.pdf:/home/linusboyle/Zotero/storage/TG8QWAFB/HyunD et al-Accurate Wirelength Prediction for Placement-Aware Synthesis through Machine Learning.pdf:application/pdf},
	isbn = {978-3-9819263-2-3},
	keywords = {Read, 数字系统设计课, EDA与AI},
	language = {en},
	month = mar,
	pages = {324--327},
	publisher = {IEEE},
	title = {Accurate {Wirelength} {Prediction} for {Placement}-{Aware} {Synthesis} through {Machine} {Learning}},
	url = {https://ieeexplore.ieee.org/document/8715016/},
	urldate = {2020-03-25},
	year = {2019}
}

@article{rossignol_scade_2013,
	abstract = {Radar displays have evolved over time from being pure mechanically-driven devices (oscilloscopes) able of displaying raw video showing the operators the detected and amplified target return signal (and the receiver noise) to more evolved, computer-based, interfaces where digital processing allows cleaning up the display by eliminating noise and clutter, and creating precise symbols for each target. These radar interfaces are now becoming more and more interactive, giving radar display operators the ability to not only observe the images, but also interact with them and send commands.},
	author = {Rossignol, Vincent},
	file = {RossignolV-SCADE Display{\textregistered} for the Design of Airborne and Ground-Based Radar Human-Machine Interfaces (HMIs).pdf:/home/linusboyle/Zotero/storage/DYKAQT5M/RossignolV-SCADE Display{\textregistered} for the Design of Airborne and Ground-Based Radar Human-Machine Interfaces (HMIs).pdf:application/pdf},
	keywords = {⛔ No DOI found, SCADE, SCADE Display},
	language = {en},
	pages = {5},
	title = {{SCADE} {Display}{\textregistered} for the {Design} of {Airborne} and {Ground}-{Based} {Radar} {Human}-{Machine} {Interfaces} ({HMIs})},
	year = {2013}
}

@article{berry_synchronous_2008,
	author = {Berry, G{\'e}rard},
	file = {BerryG-Synchronous Techniques for Software and Hardware Embedded Systems.pdf:/home/linusboyle/Zotero/storage/BYWWMJLW/BerryG-Synchronous Techniques for Software and Hardware Embedded Systems.pdf:application/pdf},
	keywords = {⛔ No DOI found},
	language = {en},
	pages = {163},
	title = {Synchronous {Techniques} for {Software} and {Hardware} {Embedded} {Systems}},
	year = {2008}
}

@article{colaco_scade_nodate,
	author = {Cola{\c c}o, Jean-Louis},
	file = {Cola{\c c}oJ-Scade 6, a Formal Language for Embedded Software Development.pdf:/home/linusboyle/Zotero/storage/CC5EH6TA/Cola{\c c}oJ-Scade 6, a Formal Language for Embedded Software Development.pdf:application/pdf},
	keywords = {⛔ No DOI found, SCADE},
	language = {en},
	pages = {78},
	title = {Scade 6, a {Formal} {Language} for {Embedded} {Software} {Development}}
}

@article{jung_safe_nodate,
	author = {Jung, Ralf and Jourdan, Jacques-Henri and Krebbers, Robbert and Dreyer, Derek},
	file = {JungR et al-Safe Systems Programming in Rust - The Promise and the Challenge.pdf:/home/linusboyle/Zotero/storage/MNVHM5C6/JungR et al-Safe Systems Programming in Rust - The Promise and the Challenge.pdf:application/pdf},
	keywords = {⛔ No DOI found},
	language = {en},
	pages = {8},
	title = {Safe {Systems} {Programming} in {Rust}: {The} {Promise} and the {Challenge}}
}

@article{chen_decision_2019,
	author = {Chen, Taolue and Hague, Matthew and Lin, Anthony W. and R{\"u}mmer, Philipp and Wu, Zhilin},
	file = {ChenT et al-Decision procedures for path feasibility of string-manipulating programs with complex operations.pdf:/home/linusboyle/Zotero/storage/6TC56QHB/ChenT et al-Decision procedures for path feasibility of string-manipulating programs with complex operations.pdf:application/pdf},
	journal = {PACMPL},
	keywords = {Read},
	language = {en},
	number = {POPL},
	pages = {49:1--49:30},
	title = {Decision procedures for path feasibility of string-manipulating programs with complex operations},
	volume = {3},
	year = {2019}
}

@article{chen_what_2018,
	author = {Chen, Taolue and Chen, Yan and Hague, Matthew and Lin, Anthony W. and Wu, Zhilin},
	file = {ChenT et al-What is decidable about string constraints with the ReplaceAll function.pdf:/home/linusboyle/Zotero/storage/XL6Z6XHG/ChenT et al-What is decidable about string constraints with the ReplaceAll function.pdf:application/pdf},
	journal = {PACMPL},
	keywords = {Read},
	number = {POPL},
	pages = {3:1--3:29},
	title = {What is decidable about string constraints with the {ReplaceAll} function},
	volume = {2},
	year = {2018}
}

@article{j_ellis_mctaggart_unreality_1908,
	author = {{J. Ellis Mctaggart}},
	file = {J. Ellis Mctaggart-The Unreality of Time.pdf:/home/linusboyle/Zotero/storage/SELM6D78/J. Ellis Mctaggart-The Unreality of Time.pdf:application/pdf},
	issn = {0026-4423},
	journal = {Mind},
	keywords = {⛔ No DOI found, Course Material, Data analysis, Data products, Datasets, Philosophy, Eastern philosophy, Philosophy, Etiology, Information science, Information analysis, Information science, Logic, Metaphilosophy, Metaphysics, Ontology, Philosophy, Paradoxes, Philosophy, Philosophy of mind, Philosophy, Western philosophy, Philosophy, tense logic},
	language = {eng},
	note = {Publisher: MacMillan and Company, Limited},
	number = {68},
	pages = {457--474},
	title = {The {Unreality} of {Time}},
	volume = {17},
	year = {1908}
}

@inproceedings{necula_translation_2000,
	abstract = {We describe a translation validation infrastructure for the GNU C compiler. During the compilation the infrastructure compares the intermediate form of the program before and after each compiler pass and verifies the preservation of semantics. We discuss a general framework that the optimizer can use to communicate to the validator what transformations were performed. Our implementation however does not rely on help from the optimizer and it is quite successful by using instead a few heuristics to detect the transformations that take place. The main message of this paper is that a practical translation validation infrastructure, able to check the correctness of many of the transformations performed by a realistic compiler, can be implemented with about the effort typically required to implement one compiler pass. We demonstrate this in the context of the GNU C compiler for a number of its optimizations while compiling realistic programs such as the compiler itself or the Linux kernel. We believe that the price of such an infrastructure is small considering the qualitative increase in the ability to isolate compilation errors during compiler testing and maintenance.},
	address = {Vancouver, British Columbia, Canada},
	author = {Necula, George C.},
	booktitle = {Proceedings of the {ACM} {SIGPLAN} 2000 conference on {Programming} language design and implementation},
	file = {NeculaG-Translation validation for an optimizing compiler.pdf:/home/linusboyle/Zotero/storage/MU85AVSR/NeculaG-Translation validation for an optimizing compiler.pdf:application/pdf},
	isbn = {978-1-58113-199-4},
	month = may,
	pages = {83--94},
	publisher = {Association for Computing Machinery},
	series = {{PLDI} '00},
	title = {Translation validation for an optimizing compiler},
	url = {https://doi.org/10.1145/349299.349314},
	urldate = {2020-04-16},
	year = {2000}
}

@inproceedings{tristan_formal_2008,
	abstract = {Translation validation consists of transforming a program and a posteriori validating it in order to detect a modification of itssemantics. This approach can be used in a verified compiler, provided that validation is formally proved to be correct. We present two such validators and their Coq proofs of correctness. The validators are designed for two instruction scheduling optimizations: list scheduling and trace scheduling.},
	address = {San Francisco, California, USA},
	author = {Tristan, Jean-Baptiste and Leroy, Xavier},
	booktitle = {Proceedings of the 35th annual {ACM} {SIGPLAN}-{SIGACT} symposium on {Principles} of programming languages},
	file = {TristanJ&LeroyX-Formal verification of translation validators - a case study on instruction scheduling optimizations.pdf:/home/linusboyle/Zotero/storage/L76QT7WY/TristanJ&LeroyX-Formal verification of translation validators - a case study on instruction scheduling optimizations.pdf:application/pdf},
	isbn = {978-1-59593-689-9},
	keywords = {scheduling optimizations, the coq proof assistant, translation validation, verified compilers},
	month = jan,
	pages = {17--27},
	publisher = {Association for Computing Machinery},
	series = {{POPL} '08},
	shorttitle = {Formal verification of translation validators},
	title = {Formal verification of translation validators: a case study on instruction scheduling optimizations},
	url = {https://doi.org/10.1145/1328438.1328444},
	urldate = {2020-04-16},
	year = {2008}
}

@inproceedings{loring_sound_2019,
	abstract = {Support for regular expressions in symbolic execution-based tools for test generation and bug finding is insufficient. Common aspects of mainstream regular expression engines, such as backreferences or greedy matching, are ignored or imprecisely approximated, leading to poor test coverage or missed bugs. In this paper, we present a model for the complete regular expression language of ECMAScript 2015 (ES6), which is sound for dynamic symbolic execution of the test and exec functions. We model regular expression operations using string constraints and classical regular expressions and use a refinement scheme to address the problem of matching precedence and greediness. We implemented our model in ExpoSE, a dynamic symbolic execution engine for JavaScript, and evaluated it on over 1,000 Node.js packages containing regular expressions, demonstrating that the strategy is effective and can significantly increase the number of successful regular expression queries and therefore boost coverage.},
	address = {Phoenix, AZ, USA},
	author = {Loring, Blake and Mitchell, Duncan and Kinder, Johannes},
	booktitle = {Proceedings of the 40th {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation} - {PLDI} 2019},
	file = {LoringB et al-Sound regular expression semantics for dynamic symbolic execution of JavaScript.pdf:/home/linusboyle/Zotero/storage/MMF7V3DR/LoringB et al-Sound regular expression semantics for dynamic symbolic execution of JavaScript.pdf:application/pdf},
	isbn = {978-1-4503-6712-7},
	keywords = {Read},
	language = {en},
	pages = {425--438},
	publisher = {ACM Press},
	title = {Sound regular expression semantics for dynamic symbolic execution of {JavaScript}},
	url = {http://dl.acm.org/citation.cfm?doid=3314221.3314645},
	urldate = {2020-04-17},
	year = {2019}
}

@inproceedings{bjorner_path_2009,
	abstract = {We discuss the problem of path feasibility for programs manipulating strings using a collection of standard string library functions. We prove results on the complexity of this problem, including its undecidability in the general case and decidability of some special cases. In the context of test-case generation, we are interested in an efficient finite model finding method for string constraints. To this end we develop a two-tier finite model finding procedure. First, an integer abstraction of string constraints are passed to an SMT (Satisfiability Modulo Theories) solver. The abstraction is either unsatisfiable, or the solver produces a model that fixes lengths of enough strings to reduce the entire problem to be finite domain. The resulting fixed-length string constraints are then solved in a second phase. We implemented the procedure in a symbolic execution framework, report on the encouraging results and discuss directions for improving the method further.},
	address = {York, UK},
	author = {Bj{\o}rner, Nikolaj and Tillmann, Nikolai and Voronkov, Andrei},
	booktitle = {Proceedings of the 15th {International} {Conference} on {Tools} and {Algorithms} for the {Construction} and {Analysis} of {Systems}: {Held} as {Part} of the {Joint} {European} {Conferences} on {Theory} and {Practice} of {Software}, {ETAPS} 2009,},
	file = {Bj{\o}rnerN et al-Path Feasibility Analysis for String-Manipulating Programs.pdf:/home/linusboyle/Zotero/storage/VHNMMHDR/Bj{\o}rnerN et al-Path Feasibility Analysis for String-Manipulating Programs.pdf:application/pdf},
	isbn = {978-3-642-00767-5},
	month = mar,
	pages = {307--321},
	publisher = {Springer-Verlag},
	series = {{TACAS} '09},
	title = {Path {Feasibility} {Analysis} for {String}-{Manipulating} {Programs}},
	url = {https://doi.org/10.1007/978-3-642-00768-2_27},
	urldate = {2020-04-20},
	year = {2009}
}

@inproceedings{muller-olm_model-checking_1999,
	abstract = {In the past two decades, model-checking has emerged as a promising and,powerful approach,to fully automatic verication of hardware systems. But model checking technology can be usefully applied to other application areas, and this article provides fundamentals that a practitioner can use to translate verication problems into modelchecking questions. A taxonomy of the notions of {\textbackslash}model," {\textbackslash}property," and {\textbackslash}model checking" are presented, and three standard model-checking approaches are described and applied to examples.},
	author = {Muller-olm, Markus and Schmidt, David and Steffen, Bernhard},
	file = {Muller-olmM et al-Model-Checking - A Tutorial Introduction.pdf:/home/linusboyle/Zotero/storage/67A2K5LF/Muller-olmM et al-Model-Checking - A Tutorial Introduction.pdf:application/pdf},
	keywords = {Read},
	month = jan,
	pages = {330--354},
	shorttitle = {Model-{Checking}},
	title = {Model-{Checking}: {A} {Tutorial} {Introduction}},
	volume = {1694},
	year = {1999}
}

@article{waterman_volume_nodate-1,
	author = {Waterman, Andrew and Asanovic, Krste and Division, CS},
	file = {WatermanA et al-Volume I - Unprivileged ISA.pdf:/home/linusboyle/Zotero/storage/VTP5PIFU/WatermanA et al-Volume I - Unprivileged ISA.pdf:application/pdf},
	keywords = {⛔ No DOI found, RISCV},
	language = {en},
	pages = {238},
	title = {Volume {I}: {Unprivileged} {ISA}}
}

@article{campeanu_formal_2003,
	abstract = {Regular expressions are used in many practical applications. Practical regular expressions are commonly called ``regex''. It is known that regex are different from regular expressions. In this paper, we give regex a formal treatment. We make a distinction between regex and extended regex; while regex represent regular languages, extended regex represent a family of languages larger than regular languages. We prove a pumping lemma for the languages expressed by extended regex. We show that the languages represented by extended regex are incomparable with context-free languages and a proper subset of context-sensitive languages. Other properties of the languages represented by extended regex are also studied.},
	author = {C{\^a}mpeanu, Cezar and Salomaa, Kai and Yu, Sheng},
	file = {C{\^a}mpeanuC et al-A Formal Study of Practical Regular Expressions-Int. J. Found. Comput. Sci..pdf:/home/linusboyle/Zotero/storage/LWPCSLRI/C{\^a}mpeanuC et al-A Formal Study of Practical Regular Expressions-Int. J. Found. Comput. Sci..pdf:application/pdf},
	issn = {0129-0541, 1793-6373},
	journal = {International Journal of Foundations of Computer Science},
	keywords = {Read},
	language = {en},
	month = dec,
	number = {06},
	pages = {1007--1018},
	title = {A {Formal} {Study} of {Practical} {Regular} {Expressions}},
	url = {https://www.worldscientific.com/doi/abs/10.1142/S012905410300214X},
	urldate = {2020-04-30},
	volume = {14},
	year = {2003}
}

@article{schmid_characterising_2016,
	abstract = {A (factor-)reference in a word is a special symbol that refers to another factor in the same word; a reference is dereferenced by substituting it with the referenced factor. We introduce and investigate the class ref-REG of all languages that can be obtained by taking a regular language R and then dereferencing all possible references in the words of R. We show that ref-REG coincides with the class of languages defined by regular expressions as they exist in modern programming languages like Perl, Python, Java, etc. (often called REGEX languages).},
	author = {Schmid, Markus L.},
	file = {SchmidM-Characterising REGEX languages by regular languages equipped with factor-referencing-Information and Computation.pdf:/home/linusboyle/Zotero/storage/FADL4P2U/SchmidM-Characterising REGEX languages by regular languages equipped with factor-referencing-Information and Computation.pdf:application/pdf},
	issn = {08905401},
	journal = {Information and Computation},
	keywords = {Read},
	language = {en},
	month = aug,
	pages = {1--17},
	title = {Characterising {REGEX} languages by regular languages equipped with factor-referencing},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0890540116000109},
	urldate = {2020-04-30},
	volume = {249},
	year = {2016}
}

@article{freydenberger_deterministic_2019,
	abstract = {Most modern libraries for regular expression matching allow back-references (i. e., repetition operators) that substantially increase expressive power, but also lead to intractability. In order to find a better balance between expressiveness and tractability, we combine these with the notion of determinism for regular expressions used in XML DTDs and XML Schema. This includes the definition of a suitable automaton model, and a generalization of the Glushkov construction.},
	author = {Freydenberger, Dominik D. and Schmid, Markus L.},
	file = {FreydenbergerD&SchmidM-Deterministic regular expressions with back-references-Journal of Computer and System Sciences.pdf:/home/linusboyle/Zotero/storage/DLAQVDNR/FreydenbergerD&SchmidM-Deterministic regular expressions with back-references-Journal of Computer and System Sciences.pdf:application/pdf},
	issn = {00220000},
	journal = {Journal of Computer and System Sciences},
	keywords = {Read},
	language = {en},
	month = nov,
	pages = {1--39},
	title = {Deterministic regular expressions with back-references},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0022000018301818},
	urldate = {2020-04-30},
	volume = {105},
	year = {2019}
}

@article{berglund_semantics_2017,
	abstract = {We introduce prioritized transducers to formalize capturing groups in regular expression matching in a way that permits straightforward modeling of capturing in Java{\rq}s 1 regular expression library. The broader questions of parsing semantics and performance are also considered. In addition, the complexity of deciding equivalence of regular expressions with capturing groups is investigated.},
	author = {Berglund, Martin and van der Merwe, Brink},
	file = {BerglundM&van der MerweB-On the semantics of regular expression parsing in the wild-Theoretical Computer Science.pdf:/home/linusboyle/Zotero/storage/FQQIGHKL/BerglundM&van der MerweB-On the semantics of regular expression parsing in the wild-Theoretical Computer Science.pdf:application/pdf},
	issn = {03043975},
	journal = {Theoretical Computer Science},
	keywords = {Read},
	language = {en},
	month = may,
	pages = {69--82},
	title = {On the semantics of regular expression parsing in the wild},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304397516304790},
	urldate = {2020-04-30},
	volume = {679},
	year = {2017}
}

@article{chen_decision_2018,
	abstract = {The design and implementation of decision procedures for checking path
feasibility in string-manipulating programs is an important problem, whose
applications include symbolic execution and automated detection of cross-site
scripting (XSS) vulnerabilities. A (symbolic) path is a finite sequence of
assignments and assertions (i.e. without loops), and checking its feasibility
amounts to determining the existence of inputs that yield a successful
execution.
  We give two general semantic conditions which together ensure the
decidability of path feasibility: (1) each assertion admits regular monadic
decomposition, and (2) each assignment uses a (possibly nondeterministic)
function whose inverse relation preserves regularity. We show these conditions
are expressive since they are satisfied by a multitude of string operations.
They also strictly subsume existing decidable string theories, and most
existing benchmarks (e.g. most of Kaluza's, and all of SLOG's, Stranger's, and
SLOTH's). We give a simple decision procedure and an extensible architecture of
a string solver in that a user may easily incorporate his/her own string
functions. We show the general fragment has a tight, but high complexity. To
address this, we propose to allow only partial string functions (i.e., prohibit
nondeterminism) in condition (2). When nondeterministic functions are needed,
we also provide a syntactic fragment that provides a support of
nondeterministic functions but can be reduced to an existing solver SLOTH.
  We provide an efficient implementation of our decision procedure for
deterministic partial string functions in a new string solver OSTRICH. It
provides built-in support for concatenation, reverse, functional transducers,
and replaceall and provides a framework for extensibility to support further
string functions. We demonstrate the efficacy of our new solver against other
competitive solvers.},
	author = {Chen, Taolue and Hague, Matthew and Lin, Anthony W. and R{\"u}mmer, Philipp and Wu, Zhilin},
	file = {ChenT et al-Decision Procedures for Path Feasibility of String-Manipulating Programs with Complex.html:/home/linusboyle/Zotero/storage/663E7RCE/ChenT et al-Decision Procedures for Path Feasibility of String-Manipulating Programs with Complex.html:text/html; ChenT et al-Decision Procedures for Path Feasibility of String-Manipulating Programs with Complex.pdf:/home/linusboyle/Zotero/storage/XSU2AHMU/ChenT et al-Decision Procedures for Path Feasibility of String-Manipulating Programs with Complex.pdf:application/pdf},
	language = {en},
	month = nov,
	title = {Decision {Procedures} for {Path} {Feasibility} of {String}-{Manipulating} {Programs} with {Complex} {Operations}({Full} {Version})},
	url = {https://arxiv.org/abs/1811.03167v1},
	urldate = {2020-04-30},
	year = {2018}
}

@article{freydenberger_extended_2013,
	author = {Freydenberger, Dominik D.},
	file = {FreydenbergerD-Extended Regular Expressions - Succinctness and Decidability-Theory Comput Syst.pdf:/home/linusboyle/Zotero/storage/92KLBILG/FreydenbergerD-Extended Regular Expressions - Succinctness and Decidability-Theory Comput Syst.pdf:application/pdf},
	issn = {1432-4350, 1433-0490},
	journal = {Theory of Computing Systems},
	language = {en},
	month = aug,
	number = {2},
	pages = {159--193},
	shorttitle = {Extended {Regular} {Expressions}},
	title = {Extended {Regular} {Expressions}: {Succinctness} and {Decidability}},
	url = {http://link.springer.com/10.1007/s00224-012-9389-0},
	urldate = {2020-05-13},
	volume = {53},
	year = {2013}
}

@article{freydenberger_logic_2019,
	abstract = {Document spanners are a formal framework for information extraction that was introduced by Fagin, Kimelfeld, Reiss, and Vansummeren (PODS 2013, JACM 2015). One of the central models in this framework are core spanners, which are based on regular expressions with variables that are then extended with an algebra. As shown by Freydenberger and Holldack (ICDT 2016), there is a connection between core spanners and ECreg, the existential theory of concatenation with regular constraints. The present paper further develops this connection by defining SpLog, a fragment of ECreg that has the same expressive power as core spanners. This equivalence extends beyond equivalence of expressive power, as we show the existence of polynomial time conversions between this fragment and core spanners. This even holds for variants of core spanners that are based on automata instead of regular expressions. Applications of this approach include an alternative way of defining relations for spanners, insights into the relative succinctness of various classes of spanner representations, and a pumping lemma for core spanners.},
	author = {Freydenberger, Dominik D.},
	file = {FreydenbergerD-A Logic for Document Spanners-Theory Comput Syst.pdf:/home/linusboyle/Zotero/storage/M9ZJBDNR/FreydenbergerD-A Logic for Document Spanners-Theory Comput Syst.pdf:application/pdf},
	issn = {1432-4350, 1433-0490},
	journal = {Theory of Computing Systems},
	language = {en},
	month = oct,
	number = {7},
	pages = {1679--1754},
	title = {A {Logic} for {Document} {Spanners}},
	url = {http://link.springer.com/10.1007/s00224-018-9874-1},
	urldate = {2020-05-13},
	volume = {63},
	year = {2019}
}

@inproceedings{torlak_lightweight_2013,
	abstract = {Solver-aided domain-specific languages (SDSLs) are an emerging class of computer-aided programming systems. They ease the construction of programs by using satisfiability solvers to automate tasks such as verification, debugging, synthesis, and non-deterministic execution. But reducing programming tasks to satisfiability problems involves translating programs to logical constraints, which is an engineering challenge even for domain-specific languages. We have previously shown that translation to constraints can be avoided if SDSLs are implemented by (traditional) embedding into a host language that is itself solver-aided. This paper describes how to implement a symbolic virtual machine (SVM) for such a host language. Our symbolic virtual machine is lightweight because it compiles to constraints only a small subset of the host{\rq}s constructs, while allowing SDSL designers to use the entire language, including constructs for DSL embedding. This lightweight compilation employs a novel symbolic execution technique with two key properties: it produces compact encodings, and it enables concrete evaluation to strip away host constructs that are outside the subset compilable to constraints. Our symbolic virtual machine architecture is at the heart of ROSETTE, a solver-aided language that is host to several new SDSLs.},
	address = {Edinburgh, United Kingdom},
	author = {Torlak, Emina and Bodik, Rastislav},
	booktitle = {Proceedings of the 35th {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation} - {PLDI} '14},
	file = {TorlakE&BodikR-A lightweight symbolic virtual machine for solver-aided host languages.pdf:/home/linusboyle/Zotero/storage/IEFCT5VA/TorlakE&BodikR-A lightweight symbolic virtual machine for solver-aided host languages.pdf:application/pdf},
	isbn = {978-1-4503-2784-8},
	keywords = {Rosette},
	language = {en},
	pages = {530--541},
	publisher = {ACM Press},
	title = {A lightweight symbolic virtual machine for solver-aided host languages},
	url = {http://dl.acm.org/citation.cfm?doid=2594291.2594340},
	urldate = {2020-05-16},
	year = {2013}
}

@article{baltag_simple_nodate,
	abstract = {This paper presents a simple decidable logic of functional dependence LFD, based on an extension of classical propositional logic with dependence atoms and dependence quantifiers, modeled within the setting of generalized assignment semantics for FOL. The logic{\rq}s expressive strength, complete proof calculus and meta-properties are explored. Various extensions are presented, as well as boundaries with undecidable logics for independence. Finally, more concrete settings for dependence are discussed: continuous dependence in topological models, linear dependence in vector spaces, and temporal dependence in dynamical systems and games.},
	author = {Baltag, Alexandru and van Benthem, Johan},
	file = {BaltagA&van BenthemJ-A Simple Logic of Functional Dependence.pdf:/home/linusboyle/Zotero/storage/N4BR9STT/BaltagA&van BenthemJ-A Simple Logic of Functional Dependence.pdf:application/pdf},
	keywords = {⛔ No DOI found},
	language = {en},
	pages = {42},
	title = {A {Simple} {Logic} of {Functional} {Dependence}}
}

@article{samanta_provably_2015,
	abstract = {Routing is a very important step in VLSI physical design. A set of nets are routed under delay and resource constraints in multi-net global routing. In this paper a delay-driven congestion-aware global routing algorithm is developed, which is a heuristic based method to solve a multi-objective NP-hard optimization problem. The proposed delay-driven Steiner tree construction method is of O{\dh}n2 log n{\TH} complexity, where n is the number of terminal points and it provides n-approximation solution of the critical time minimization problem for a certain class of grid graphs. The existing timing-driven method (Hu and Sapatnekar, 2002) has a complexity O{\dh}n4{\TH} and is implemented on nets with small number of sinks. Next we propose a FPTAS Gradient algorithm for minimizing the total overflow. This is a concurrent approach considering all the nets simultaneously contrary to the existing approaches of sequential rip-up and reroute. The algorithms are implemented on ISPD98 derived benchmarks and the drastic reduction of overflow is observed.},
	author = {Samanta, Radhamanjari and Erzin, Adil I. and Raha, Soumyendu and Shamardin, Yuriy V. and Takhonov, Ivan I. and Zalyubovskiy, Vyacheslav V.},
	file = {SamantaR et al-A provably tight delay-driven concurrently congestion mitigating global routing algorithm-Applied Mathematics and Computation.pdf:/home/linusboyle/Zotero/storage/BQSFAIQI/SamantaR et al-A provably tight delay-driven concurrently congestion mitigating global routing algorithm-Applied Mathematics and Computation.pdf:application/pdf},
	issn = {00963003},
	journal = {Applied Mathematics and Computation},
	keywords = {Read, 数字系统设计课},
	language = {en},
	month = mar,
	pages = {92--104},
	title = {A provably tight delay-driven concurrently congestion mitigating global routing algorithm},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S009630031401594X},
	urldate = {2020-05-18},
	volume = {255},
	year = {2015}
}

@article{campeanu_intersection_2009,
	abstract = {In this paper we revisit the semantics of extended regular expressions (regex), defined succinctly in the 90s [A.V. Aho, Algorithms for finding patterns in strings, in: Jan van Leeuwen (Ed.), Handbook of Theoretical Computer Science, in: Algorithms and Complexity, vol. A, Elsevier and MIT Press, 1990, pp. 255--300] and rigorously in 2003 by C{\^a}mpeanu, Salomaa and Yu [C. C{\^a}mpeanu, K. Salomaa, S. Yu, A formal study of practical regular expressions, IJFCS 14 (6) (2003) 1007--1018], when the authors reported an open problem, namely whether regex languages are closed under the intersection with regular languages. We give a positive answer; and for doing so, we propose a new class of machines --- regex automata systems (RAS) --- which are equivalent to regex. Among others, these machines provide a consistent and convenient method of implementing regex in practice. We also prove, as a consequence of this closure property, that several languages, such as the mirror language, the language of palindromes, and the language of balanced words are not regex languages.},
	author = {Campeanu, Cezar and Santean, Nicolae},
	file = {CampeanuC&SanteanN-On the intersection of regex languages with regular languages-Theor. Comput. Sci..pdf:/home/linusboyle/Zotero/storage/PVIRWMNL/CampeanuC&SanteanN-On the intersection of regex languages with regular languages-Theor. Comput. Sci..pdf:application/pdf},
	journal = {Theor. Comput. Sci.},
	keywords = {Read},
	month = may,
	pages = {2336--2344},
	title = {On the intersection of regex languages with regular languages},
	volume = {410},
	year = {2009}
}

@article{salomaa_alternating_2000,
	abstract = {For a given extended regular expression e we construct an equational representation of an alternating finite automaton accepting the language denoted by e. For star-free extended regular expressions the construction yields a loop-free alternating finite automaton. Also the inclusion in the opposite direction holds and, thus, we obtain a new characterization for the class of star-free languages.},
	author = {Salomaa, Kai and Yu, Sheng},
	doi = {10.1016/S0304-3975(98)00047-4},
	file = {SalomaaK&YuS-Alternating finite automata and star-free languages-Theoretical Computer Science.pdf:/home/linusboyle/Zotero/storage/HHPVHWZJ/SalomaaK&YuS-Alternating finite automata and star-free languages-Theoretical Computer Science.pdf:application/pdf},
	issn = {0304-3975},
	journal = {Theoretical Computer Science},
	keywords = {Alternation, Finite automata, Language equations, Star-free languages, AFA},
	language = {en},
	month = mar,
	number = {1},
	pages = {167--176},
	title = {Alternating finite automata and star-free languages},
	url = {http://www.sciencedirect.com/science/article/pii/S0304397598000474},
	urldate = {2020-05-19},
	volume = {234},
	year = {2000}
}

@article{hengel_alternating_nodate,
	author = {Hengel, Matthias},
	file = {HengelM-Alternating Finite Automata.pdf:/home/linusboyle/Zotero/storage/NMMG3L7W/HengelM-Alternating Finite Automata.pdf:application/pdf},
	keywords = {⛔ No DOI found, Slides, AFA},
	language = {en},
	pages = {94},
	title = {Alternating {Finite} {Automata}}
}

@misc{russ_cox_implementing_2007,
	author = {{Russ Cox}},
	file = {Russ Cox-Implementing Regular Expressions.html:/home/linusboyle/Zotero/storage/FQVX3CZT/Russ Cox-Implementing Regular Expressions.html:text/html},
	title = {Implementing {Regular} {Expressions}},
	url = {https://swtch.com/~rsc/regexp/},
	urldate = {2020-05-19},
	year = {2007}
}

@article{saarinen_logic_1983,
	author = {Saarinen, Esa},
	file = {SaarinenE-On the logic of perception sentences-Synthese.pdf:/home/linusboyle/Zotero/storage/R2T9QYDH/SaarinenE-On the logic of perception sentences-Synthese.pdf:application/pdf},
	journal = {Synthese},
	keywords = {perception},
	month = jan,
	pages = {115--128},
	title = {On the logic of perception sentences},
	volume = {54},
	year = {1983}
}

@article{shoham_logic_1991,
	abstract = {We present a modal logic for reasoning about perception and belief, captured respectively by the operators P and B. The B operator is the standard belief operator used in recent years, and the P operator is similarly defined. The contribution of the paper is twofold. First, in terms of P we provide a definition of perceptual indistinguishability, such as arises out of limited visual acuity. The definition is concise, intuitive (we find), and avoids traditional paradoxes. Second, we explore the bimodal B - P system. We argue that the relationship between the two modalities varies among settings: The agent may or may not have confidence in its perception, may or may not be accurate in it, and so on. We therefore define a number of agent types corresponding to these various assumptions, and for each such agent type we provide a sound and complete axiomatization of the B - P system.},
	author = {Shoham, Yoav and de, Alvaro},
	file = {ShohamY&deA-A Logic for Perception and Belief.pdf:/home/linusboyle/Zotero/storage/4X3MR4TA/ShohamY&deA-A Logic for Perception and Belief.pdf:application/pdf},
	keywords = {⛔ No DOI found, Read, perception},
	language = {en},
	pages = {25},
	title = {A {Logic} for {Perception} and {Belief}},
	year = {1991}
}

@article{alur_expressiveness_2010,
	abstract = {Streaming string transducers [1] define (partial) functions from input strings to output strings. A streaming string transducer makes a single pass through the input string and uses a finite set of variables that range over strings from the output alphabet. At every step, the transducer processes an input symbol, and updates all the variables in parallel using assignments whose right-hand-sides are concatenations of output symbols and variables with the restriction that a variable can be used at most once in a right-hand-side expression. It has been shown that streaming string transducers operating on strings over infinite data domains are of interest in algorithmic verification of list-processing programs, as they lead to Pspace decision procedures for checking pre/post conditions and for checking semantic equivalence, for a well-defined class of heap-manipulating programs. In order to understand the theoretical expressiveness of streaming transducers, we focus on streaming transducers processing strings over finite alphabets, given the existence of a robust and well-studied class of ``regular'' transductions for this case. Such regular transductions can be defined either by two-way deterministic finite-state transducers, or using a logical MSO-based characterization. Our main result is that the expressiveness of streaming string transducers coincides exactly with this class of regular transductions.},
	author = {Alur, Rajeev and {\v C}ern{\'y}, Pavol},
	collaborator = {Herbstritt, Marc},
	file = {AlurR&{\v C}ern{\'y}P-Expressiveness of streaming string transducers.pdf:/home/linusboyle/Zotero/storage/MD8V4I2A/AlurR&{\v C}ern{\'y}P-Expressiveness of streaming string transducers.pdf:application/pdf},
	keywords = {⛔ No DOI found, Read},
	language = {en},
	note = {Artwork Size: 12 pages Medium: application/pdf Publisher: Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik GmbH, Wadern/Saarbruecken, Germany},
	pages = {12},
	title = {Expressiveness of streaming string transducers},
	url = {http://drops.dagstuhl.de/opus/volltexte/2010/2853/},
	urldate = {2020-06-15},
	year = {2010}
}

@inproceedings{alhyari_deep_2019,
	abstract = {The ability to accurately and efficiently estimate the routability of a circuit based on its placement is one of the most challenging and difficult tasks in the Field Programmable Gate Array (FPGA) flow. In this paper, we present a novel, deep-learning framework based on a Convolutional Neural Network model for predicting the routability of a placement. We also incorporate the deep-learning model into a state-of-the-art placement tool, and show how the model can be used to (1) avoid costly, but futile, place-and-route iterations, and (2) improve the placer{\rq}s ability to produce routable placements for hard-toroute circuits using feedback based on routability estimates generated by the proposed model. The model is trained and evaluated using over 26K placement images derived from 372 benchmarks supplied by Xilinx Inc. Experimental results show that the proposed framework achieves a routability prediction accuracy of 97\%, while exhibiting runtimes of only a few milliseconds.},
	address = {Barcelona, Spain},
	author = {Alhyari, Abeer and Shamli, Ahmed and Abuwaimer, Ziad and Areibi, Shawki and Grewal, Gary},
	booktitle = {2019 29th {International} {Conference} on {Field} {Programmable} {Logic} and {Applications} ({FPL})},
	file = {AlhyariA et al-A Deep Learning Framework to Predict Routability for FPGA Circuit Placement.pdf:/home/linusboyle/Zotero/storage/G4TV7TXW/AlhyariA et al-A Deep Learning Framework to Predict Routability for FPGA Circuit Placement.pdf:application/pdf},
	isbn = {978-1-72814-884-7},
	keywords = {EDA与AI},
	language = {en},
	month = sep,
	pages = {334--341},
	publisher = {IEEE},
	title = {A {Deep} {Learning} {Framework} to {Predict} {Routability} for {FPGA} {Circuit} {Placement}},
	url = {https://ieeexplore.ieee.org/document/8892143/},
	urldate = {2020-05-29},
	year = {2019}
}

@article{kwon_learning-based_nodate,
	abstract = {Logic synthesis and physical design (LSPD) tools automate complex design tasks previously performed by human designers. One timeconsuming task that remains manual is configuring the LSPD flow parameters, which significantly impacts design results. To reduce the parameter-tuning effort, we propose an LSPD parameter recommender system that involves learning a collaborative prediction model through tensor decomposition and regression. Using a model trained with archived data from multiple state-of-the-art 14nm processors, we reduce the exploration cost while achieving comparable design quality. Furthermore, we demonstrate the transfer-learning properties of our approach by showing that this model can be successfully applied for 7nm designs.},
	author = {Kwon, Jihye and Ziegler, Matthew M and Carloni, Luca P},
	file = {KwonJ et al-A Learning-Based Recommender System for Autotuning Design Flows of Industrial High-Performance.pdf:/home/linusboyle/Zotero/storage/PFQNUNSV/KwonJ et al-A Learning-Based Recommender System for Autotuning Design Flows of Industrial High-Performance.pdf:application/pdf},
	keywords = {Read, 数字系统设计课, EDA与AI},
	language = {en},
	pages = {6},
	title = {A {Learning}-{Based} {Recommender} {System} for {Autotuning} {Design} {Flows} of {Industrial} {High}-{Performance} {Processors}}
}

@article{sadasivam_invited_nodate,
	abstract = {The exponential growth in PVT corners due to Moore{\rq}s law scaling, and the increasing demand for consumer applications and longer battery life in mobile devices, has ushered in significant cost and power-related challenges for designing and productizing mobile chips within a predictable schedule. Two main reasons for this are the reliance on human decision-making to achieve the desired performance within the target area and power budget, and significant increases in complexity of the human decision-making space. The problem is that to-date human design experience has not been replaced by design automation tools, and tasks requiring experience of past designs are still being performed manually. In this paper we investigate how machine learning may be applied to develop tools that learn from experience just like human designers, thus automating tasks that still require human intervention. The potential advantage of the machine learning approach is the ability to scale with increasing complexity and therefore hold the design-time constant with same manpower.},
	author = {Sadasivam, Shankar and Lee, Jinwon and Chen, Zhuo and Jain, Rajeev},
	file = {SadasivamS et al-Invited - Efficient Reinforcement Learning for Automating Human Decision-Making in SoC Design.pdf:/home/linusboyle/Zotero/storage/M2WC7Y25/SadasivamS et al-Invited - Efficient Reinforcement Learning for Automating Human Decision-Making in SoC Design.pdf:application/pdf},
	keywords = {EDA与AI},
	language = {en},
	pages = {6},
	title = {Invited: {Efficient} {Reinforcement} {Learning} for {Automating} {Human} {Decision}-{Making} in {SoC} {Design}}
}

@inproceedings{sun_system-level_2019,
	abstract = {Disk and memory faults are the leading causes of server breakdown. A proactive solution is to predict such hardware failure at the runtime and then isolate the hardware at risk and backup the data. However, the current model-based predictors are incapable of using the discrete time-series data, such as the values of device attributes, which conveys high-level information of the device behavior. In this paper, we propose a novel deep-learning based prediction scheme for system-level hardware failure prediction. We normalize the distribution of samples{\rq} attributes from different vendors to make use of diverse training sets. We propose a temporal Convolution Neural Network based model that is insensitive to the noise in the time dimension. Finally, we design a loss function to train the model with extremely imbalanced samples effectively. Experimental results from an open S.M.A.R.T data set and an industrial data set show the effectiveness of the proposed scheme.},
	address = {Las Vegas NV USA},
	author = {Sun, Xiaoyi and Chakrabarty, Krishnendu and Huang, Ruirui and Chen, Yiquan and Zhao, Bing and Cao, Hai and Han, Yinhe and Liang, Xiaoyao and Jiang, Li},
	booktitle = {Proceedings of the 56th {Annual} {Design} {Automation} {Conference} 2019},
	file = {SunX et al-System-level hardware failure prediction using deep learning.pdf:/home/linusboyle/Zotero/storage/8YEIBZ9B/SunX et al-System-level hardware failure prediction using deep learning.pdf:application/pdf},
	isbn = {978-1-4503-6725-7},
	keywords = {EDA与AI},
	language = {en},
	month = jun,
	pages = {1--6},
	publisher = {ACM},
	title = {System-level hardware failure prediction using deep learning},
	url = {https://dl.acm.org/doi/10.1145/3316781.3317918},
	urldate = {2020-05-29},
	year = {2019}
}

@misc{lin_unit_2020,
	author = {Lin, Hanti},
	file = {LinH-Unit 0 Probability Theory - the Very Essentials.pdf:/home/linusboyle/Zotero/storage/Y2A4KI3E/LinH-Unit 0 Probability Theory - the Very Essentials.pdf:application/pdf},
	keywords = {⛔ No DOI found, Read, Lecture, Learning Theory},
	language = {en},
	title = {Unit 0 {Probability} {Theory} - the {Very} {Essentials}},
	year = {2020}
}

@article{chen_what_2018-1,
	author = {Chen, Taolue and Chen, Yan and Hague, Matthew and Lin, Anthony W. and Wu, Zhilin},
	file = {ChenT et al-What Is Decidable about String Constraints with the ReplaceAll Function (Technical Report).pdf:/home/linusboyle/Zotero/storage/VBU3NRGF/ChenT et al-What Is Decidable about String Constraints with the ReplaceAll Function (Technical Report).pdf:application/pdf},
	journal = {PACMPL},
	keywords = {Read},
	number = {POPL},
	pages = {3:1--3:29},
	title = {What {Is} {Decidable} about {String} {Constraints} with the {ReplaceAll} {Function} ({Technical} {Report})},
	volume = {2},
	year = {2018}
}

@misc{bartlomiej_piotrowski_test_2020,
	author = {{Bart{\l}omiej Piotrowski}},
	file = {Bart{\l}omiej Piotrowski-Test driving Flathub mirror for users in China.html:/home/linusboyle/Zotero/storage/KVQMY3TT/Bart{\l}omiej Piotrowski-Test driving Flathub mirror for users in China.html:text/html},
	journal = {barthalion's blog},
	title = {Test driving {Flathub} mirror for users in {China}},
	url = {https://barthalion.blog/test-driving-flathub-mirror-for-users-in-china/},
	urldate = {2020-06-03},
	year = {2020}
}

@article{veanes_symbolic_2012,
	abstract = {Finite automata and finite transducers are used in a wide range of applications in software engineering, from regular expressions to specification languages. We extend these classic objects with symbolic alphabets represented as parametric theories. Admitting potentially infinite alphabets makes this representation strictly more general and succinct than classical finite transducers and automata over strings. Despite this, the main operations, including composition, checking that a transducer is single-valued, and equivalence checking for single-valued symbolic finite transducers are effective given a decision procedure for the background theory. We provide novel algorithms for these operations and extend composition to symbolic transducers augmented with registers. Our base algorithms are unusual in that they are nonconstructive, therefore, we also supply a separate model generation algorithm that can quickly find counterexamples in the case two symbolic finite transducers are not equivalent. The algorithms give rise to a complete decidable algebra of symbolic transducers. Unlike previous work, we do not need any syntactic restriction of the formulas on the transitions, only a decision procedure. In practice we leverage recent advances in satisfiability modulo theory (SMT) solvers. We demonstrate our techniques on four case studies, covering a wide range of applications. Our techniques can synthesize string pre-images in excess of 8,000 bytes in roughly a minute, and we find that our new encodings significantly outperform previous techniques in succinctness and speed of analysis.},
	author = {Veanes, Margus and Hooimeijer, Pieter and Livshits, Benjamin and Molnar, David and Bjorner, Nikolaj},
	file = {VeanesM et al-Symbolic finite state transducers - algorithms and applications.pdf:/home/linusboyle/Zotero/storage/6G6VRPFQ/VeanesM et al-Symbolic finite state transducers - algorithms and applications.pdf:application/pdf},
	issn = {0362-1340},
	journal = {ACM SIGPLAN Notices},
	keywords = {Smt, Automata, Composition, Equivalence, Computer Science},
	language = {eng},
	number = {1},
	pages = {137--150},
	shorttitle = {Symbolic finite state transducers},
	title = {Symbolic finite state transducers: algorithms and applications},
	volume = {47},
	year = {2012}
}

@article{shepherdson_reduction_1959,
	abstract = {Rabin has proved1,2 that two-way finite automata, which are allowed to move in both directions along their input tape, are equivalent to one-way automata as far as the classification of input tapes is concerned. Rabin's proof is rather complicated and consists in giving a method for the successive elimination of loops in the motion of the machine. The purposeo f this note is to give a short, direct proof of the result.},
	author = {Shepherdson, J. C.},
	file = {ShepherdsonJ-The Reduction of Two-Way Automata to One-Way Automata.pdf:/home/linusboyle/Zotero/storage/4QN2SSTJ/ShepherdsonJ-The Reduction of Two-Way Automata to One-Way Automata.pdf:application/pdf},
	issn = {0018-8646},
	journal = {IBM Journal of Research and Development},
	month = apr,
	note = {Conference Name: IBM Journal of Research and Development},
	number = {2},
	pages = {198--200},
	title = {The {Reduction} of {Two}-{Way} {Automata} to {One}-{Way} {Automata}},
	volume = {3},
	year = {1959}
}

@inproceedings{alur_streaming_2011,
	abstract = {We introduce streaming data string transducers that map input data strings to output data strings in a single left-to-right pass in linear time. Data strings are (unbounded) sequences of data values, tagged with symbols from a finite set, over a potentially infinite data domain that supports only the operations of equality and ordering. The transducer uses a finite set of states, a finite set of variables ranging over the data domain, and a finite set of variables ranging over data strings. At every step, it can make decisions based on the next input symbol, updating its state, remembering the input data value in its data variables, and updating data-string variables by concatenating data-string variables and new symbols formed from data variables, while avoiding duplication. We establish PSPACE bounds for the problems of checking functional equivalence of two streaming transducers, and of checking whether a streaming transducer satisfies pre/post verification conditions specified by streaming acceptors over input/output data-strings. We identify a class of imperative and a class of functional programs, manipulating lists of data items, which can be effectively translated to streaming data-string transducers. The imperative programs dynamically modify a singly-linked heap by changing next-pointers of heap-nodes and by adding new nodes. The main restriction specifies how the next-pointers can be used for traversal. We also identify an expressively equivalent fragment of functional programs that traverse a list using syntactically restricted recursive calls. Our results lead to algorithms for assertion checking and for checking functional equivalence of two programs, written possibly in different programming styles, for commonly used routines such as insert, delete, and reverse.},
	address = {Austin, Texas, USA},
	author = {Alur, Rajeev and {\v C}ern{\'y}, Pavol},
	booktitle = {Proceedings of the 38th annual {ACM} {SIGPLAN}-{SIGACT} symposium on {Principles} of programming languages},
	file = {AlurR&{\v C}ern{\'y}P-Streaming transducers for algorithmic verification of single-pass list-processing programs.pdf:/home/linusboyle/Zotero/storage/BTUETVQD/AlurR&{\v C}ern{\'y}P-Streaming transducers for algorithmic verification of single-pass list-processing programs.pdf:application/pdf},
	isbn = {978-1-4503-0490-0},
	keywords = {algorithmic software verification, lists, transducers},
	month = jan,
	pages = {599--610},
	publisher = {Association for Computing Machinery},
	series = {{POPL} '11},
	title = {Streaming transducers for algorithmic verification of single-pass list-processing programs},
	url = {https://doi.org/10.1145/1926385.1926454},
	urldate = {2020-06-14},
	year = {2011}
}

@article{yang_aarf_2018,
	author = {Yang, Kailin and Yao, Hailong and Ho, Tsung-Yi and Xin, Kunze and Cai, Yici},
	file = {YangK et al-AARF - Any-Angle Routing for Flow-Based Microfluidic Biochips-IEEE Trans. Comput.-Aided Des. Integr. Circuits Syst..pdf:/home/linusboyle/Zotero/storage/HL3ZEW6Y/YangK et al-AARF - Any-Angle Routing for Flow-Based Microfluidic Biochips-IEEE Trans. Comput.-Aided Des. Integr. Circuits Syst..pdf:application/pdf},
	issn = {0278-0070, 1937-4151},
	journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	month = dec,
	number = {12},
	pages = {3042--3055},
	shorttitle = {{AARF}},
	title = {{AARF}: {Any}-{Angle} {Routing} for {Flow}-{Based} {Microfluidic} {Biochips}},
	url = {https://ieeexplore.ieee.org/document/8252921/},
	urldate = {2020-06-16},
	volume = {37},
	year = {2018}
}

@inproceedings{dave_regular_2018,
	abstract = {Functional MSO transductions, deterministic two-way transducers, as well as streaming string transducers are all equivalent models for regular functions. In this paper, we show that every regular function, either on finite words or on infinite words, captured by a deterministic two-way transducer, can be described with a regular transducer expression (RTE). For infinite words, the transducer uses Muller acceptance and \ensuremath{\omega}-regular look-ahead. RTEs are constructed from constant functions using the combinators if-then-else (deterministic choice), Hadamard product, and unambiguous versions of the Cauchy product, the 2-chained Kleene-iteration and the 2-chained omega-iteration. Our proof works for transformations of both finite and infinite words, extending the result on finite words of Alur et al. in LICS'14. In order to construct an RTE associated with a deterministic two-way Muller transducer with look-ahead, we introduce the notion of transition monoid for such two-way transducers where the look-ahead is captured by some backward deterministic B{\"u}chi automaton. Then, we use an unambiguous version of Imre Simon's famous forest factorization theorem in order to derive a "good" (\ensuremath{\omega}-)regular expression for the domain of the two-way transducer. "Good" expressions are unambiguous and Kleene-plus as well as \ensuremath{\omega}-iterations are only used on subexpressions corresponding to idempotent elements of the transition monoid. The combinator expressions are finally constructed by structural induction on the "good" (\ensuremath{\omega}-)regular expression describing the domain of the transducer.},
	address = {Oxford, United Kingdom},
	author = {Dave, Vrunda and Gastin, Paul and Krishna, Shankara Narayanan},
	booktitle = {Proceedings of the 33rd {Annual} {ACM}/{IEEE} {Symposium} on {Logic} in {Computer} {Science}},
	file = {DaveV et al-Regular Transducer Expressions for Regular Transformations.pdf:/home/linusboyle/Zotero/storage/DRJVLK7F/DaveV et al-Regular Transducer Expressions for Regular Transformations.pdf:application/pdf},
	isbn = {978-1-4503-5583-4},
	keywords = {Transducers, Transition monoid, Unambiguity},
	month = jul,
	pages = {315--324},
	publisher = {Association for Computing Machinery},
	series = {{LICS} '18},
	title = {Regular {Transducer} {Expressions} for {Regular} {Transformations}},
	url = {https://doi.org/10.1145/3209108.3209182},
	urldate = {2020-06-24},
	year = {2018}
}

@inproceedings{thome_search-driven_2017,
	abstract = {Constraint solving is an essential technique for detecting vulnerabilities in programs, since it can reason about input sanitization and validation operations performed on user inputs. However, real-world programs typically contain complex string operations that challenge vulnerability detection. State-ofthe-art string constraint solvers support only a limited set of string operations and fail when they encounter an unsupported one; this leads to limited effectiveness in finding vulnerabilities. In this paper we propose a search-driven constraint solving technique that complements the support for complex string operations provided by any existing string constraint solver. Our technique uses a hybrid constraint solving procedure based on the Ant Colony Optimization meta-heuristic. The idea is to execute it as a fallback mechanism, only when a solver encounters a constraint containing an operation that it does not support.},
	address = {Buenos Aires},
	author = {Thome, Julian and Shar, Lwin Khin and Bianculli, Domenico and Briand, Lionel},
	booktitle = {2017 {IEEE}/{ACM} 39th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	doi = {10.1109/ICSE.2017.26},
	file = {ThomeJ et al-Search-Driven String Constraint Solving for Vulnerability Detection.pdf:/home/linusboyle/Zotero/storage/A9QGUWKR/ThomeJ et al-Search-Driven String Constraint Solving for Vulnerability Detection.pdf:application/pdf},
	isbn = {978-1-5386-3868-2},
	keywords = {Read},
	language = {en},
	month = may,
	pages = {198--208},
	publisher = {IEEE},
	title = {Search-{Driven} {String} {Constraint} {Solving} for {Vulnerability} {Detection}},
	url = {http://ieeexplore.ieee.org/document/7985662/},
	urldate = {2020-06-24},
	year = {2017}
}

@misc{lin_unit_2020-1,
	author = {Lin, Hanti},
	file = {LinH-Unit 2 Learning-Theoretic Epistemology.pdf:/home/linusboyle/Zotero/storage/2AXEJT8S/LinH-Unit 2 Learning-Theoretic Epistemology.pdf:application/pdf},
	keywords = {⛔ No DOI found, Read, Lecture, Learning Theory},
	language = {en},
	title = {Unit 2 {Learning}-{Theoretic} {Epistemology}},
	year = {2020}
}

@misc{lin_unit_2020-2,
	author = {Lin, Hanti},
	file = {LinH-Unit 3 Application - Binary Classification.pdf:/home/linusboyle/Zotero/storage/Y5ZYDP62/LinH-Unit 3 Application - Binary Classification.pdf:application/pdf},
	keywords = {Lecture, Learning Theory},
	language = {en},
	title = {Unit 3 {Application} - {Binary} {Classification}},
	year = {2020}
}

@misc{lin_unit_2020-3,
	author = {Lin, Hanti},
	file = {LinH-Unit 3 Supplement - Proofs of Some Results.pdf:/home/linusboyle/Zotero/storage/Z32MPNKV/LinH-Unit 3 Supplement - Proofs of Some Results.pdf:application/pdf},
	keywords = {Lecture, Learning Theory},
	language = {en},
	title = {Unit 3 {Supplement}: {Proofs} of {Some} {Results}},
	year = {2020}
}

@misc{lin_unit_2020-4,
	author = {Lin, Hanti},
	file = {LinH-Unit 4 Application - Curve Fitting (aka Regression).pdf:/home/linusboyle/Zotero/storage/3DTQCKGD/LinH-Unit 4 Application - Curve Fitting (aka Regression).pdf:application/pdf},
	keywords = {Lecture, Learning Theory},
	language = {en},
	title = {Unit 4 {Application} - {Curve} {Fitting} (aka {Regression})},
	year = {2020}
}

@misc{lin_unit_2020-5,
	author = {Lin, Hanti},
	file = {LinH-Unit 4 Supplement - Q&A.pdf:/home/linusboyle/Zotero/storage/8UV7A2PM/LinH-Unit 4 Supplement - Q&A.pdf:application/pdf},
	keywords = {Lecture, Learning Theory},
	language = {en},
	title = {Unit 4 {Supplement}: {Q}\&{A}},
	year = {2020}
}

@misc{lin_unit_2020-6,
	author = {Lin, Hanti},
	file = {LinH-Unit 4 Application - Curve Fitting with Proofs.pdf:/home/linusboyle/Zotero/storage/HSRHMJC2/LinH-Unit 4 Application - Curve Fitting with Proofs.pdf:application/pdf},
	keywords = {Lecture, Learning Theory},
	language = {en},
	title = {Unit 4 {Application} - {Curve} {Fitting} with {Proofs}},
	year = {2020}
}

@misc{filiot_transducer_nodate,
	author = {Filiot, Emmanuel},
	file = {FiliotE-Transducer Theory and Streaming Transformations.pdf:/home/linusboyle/Zotero/storage/KY29XY5X/FiliotE-Transducer Theory and Streaming Transformations.pdf:application/pdf},
	keywords = {⛔ No DOI found},
	language = {en},
	title = {Transducer {Theory} and {Streaming} {Transformations}}
}

@misc{leroy_formally_2016,
	author = {Leroy, Xavier},
	file = {LeroyX-Formally verifying a compiler - what does it mean, exactly.pdf:/home/linusboyle/Zotero/storage/35X7Q3HA/LeroyX-Formally verifying a compiler - what does it mean, exactly.pdf:application/pdf},
	keywords = {CompCert, ⛔ No DOI found},
	language = {en},
	title = {Formally verifying a compiler: what does it mean, exactly?},
	year = {2016}
}

@misc{the_ieee_and_the_open_group_regular_2018,
	author = {{The IEEE and The Open Group}},
	file = {The IEEE and The Open Group-Regular Expressions.html:/home/linusboyle/Zotero/storage/N6KLLYU7/The IEEE and The Open Group-Regular Expressions.html:text/html},
	journal = {The Open Group Base Specifications Issue 7, 2018 edition},
	language = {en},
	shorttitle = {Regular {Expressions}},
	title = {Regular {Expressions}},
	url = {https://pubs.opengroup.org/onlinepubs/9699919799/basedefs/V1_chap09.html},
	urldate = {2020-06-27},
	year = {2018}
}

@article{glushkov_abstract_1961,
	author = {Glushkov, V. M.},
	file = {GlushkovV-The abstract theory of automata-Russ. Math. Surv..pdf:/home/linusboyle/Zotero/storage/LNQ9UKGH/GlushkovV-The abstract theory of automata-Russ. Math. Surv..pdf:application/pdf},
	issn = {0036-0279},
	journal = {Russian Mathematical Surveys},
	language = {en},
	note = {Publisher: IOP Publishing},
	number = {5},
	pages = {1},
	title = {The abstract theory of automata},
	url = {https://iopscience.iop.org/article/10.1070/RM1961v016n05ABEH004112/meta},
	urldate = {2020-06-28},
	volume = {16},
	year = {1961}
}

@misc{alvaro_munoz_ghsl-2020-099_nodate,
	abstract = {The GitHub Security Lab team has found a potential mXSS vulnerabulity in AngularJS.},
	author = {{Alvaro Mu{\~n}oz}},
	file = {Alvaro Mu{\~n}oz-GHSL-2020-099 - mXSS vulnerability in AngularJS - GitHub Security Lab.html:/home/linusboyle/Zotero/storage/LJMD3YPK/Alvaro Mu{\~n}oz-GHSL-2020-099 - mXSS vulnerability in AngularJS - GitHub Security Lab.html:text/html},
	note = {Library Catalog: securitylab.github.com},
	shorttitle = {{GHSL}-2020-099},
	title = {{GHSL}-2020-099: {mXSS} vulnerability in {AngularJS} - {GitHub} {Security} {Lab}},
	url = {https://securitylab.github.com/advisories/GHSL-2020-099-mxss-angular},
	urldate = {2020-06-29}
}

@misc{simon_e_spero_introduction_nodate,
	author = {{Simon E Spero}},
	file = {Simon E Spero-Introduction to ASN.1 and the Packed Encoding Rules.html:/home/linusboyle/Zotero/storage/RNE36K9W/Simon E Spero-Introduction to ASN.1 and the Packed Encoding Rules.html:text/html},
	title = {Introduction to {ASN}.1 and the {Packed} {Encoding} {Rules}},
	url = {https://www.w3.org/Protocols/HTTP-NG/asn1.html},
	urldate = {2020-06-29}
}

@inproceedings{davis_testing_2019,
	abstract = {The regular expression (regex) practices of software engineers affect the maintainability, correctness, and security of their software applications. Empirical research has described characteristics like the distribution of regex feature usage, the structural complexity of regexes, and worst-case regex match behaviors. But researchers have not critically examined the methodology they follow to extract regexes, and findings to date are typically generalized from regexes written in only 1-2 programming languages. This is an incomplete foundation. Generalizing existing research depends on validating two hypotheses: (1) Various regex extraction methodologies yield similar results, and (2) Regex characteristics are similar across programming languages. To test these hypotheses, we defined eight regex metrics to capture the dimensions of regex representation, string language diversity, and worst-case match complexity. We report that the two competing regex extraction methodologies yield comparable corpuses, suggesting that simpler regex extraction techniques will still yield sound corpuses. But in comparing regexes across programming languages, we found significant differences in some characteristics by programming language. Our findings have bearing on future empirical methodology, as the programming language should be considered, and generalizability will not be assured. Our measurements on a corpus of 537,806 regexes can guide data-driven designs of a new generation of regex tools and regex engines. "There are more things in heaven and earth, Horatio, Than are dreamt of in your philosophy." -Hamlet},
	address = {San Diego, California},
	author = {Davis, James C. and Moyer, Daniel and Kazerouni, Ayaan M. and Lee, Dongyoon},
	booktitle = {Proceedings of the 34th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	file = {DavisJ et al-Testing regex generalizability and its implications - a large-scale many-language measurement study.pdf:/home/linusboyle/Zotero/storage/TZG8LMJQ/DavisJ et al-Testing regex generalizability and its implications - a large-scale many-language measurement study.pdf:application/pdf},
	isbn = {978-1-72812-508-4},
	month = nov,
	pages = {427--439},
	publisher = {IEEE Press},
	series = {{ASE} '19},
	shorttitle = {Testing regex generalizability and its implications},
	title = {Testing regex generalizability and its implications: a large-scale many-language measurement study},
	url = {https://doi.org/10.1109/ASE.2019.00048},
	urldate = {2020-06-30},
	year = {2019}
}

@inproceedings{frisch_greedy_2004,
	abstract = {This paper studies the problem of matching sequences against regular expressions in order to produce structured values.},
	author = {Frisch, Alain and Cardelli, Luca},
	booktitle = {Automata, {Languages} and {Programming}},
	file = {FrischA&CardelliL-Greedy Regular Expression Matching.pdf:/home/linusboyle/Zotero/storage/W4XQYNJ3/FrischA&CardelliL-Greedy Regular Expression Matching.pdf:application/pdf},
	language = {en},
	month = jul,
	pages = {618--629},
	publisher = {Springer, Berlin, Heidelberg},
	title = {Greedy {Regular} {Expression} {Matching}},
	url = {https://link.springer.com/chapter/10.1007/978-3-540-27836-8_53},
	urldate = {2020-07-12},
	year = {2004}
}

@inproceedings{weideman_analyzing_2016,
	abstract = {We apply results from ambiguity of non-deterministic finite automata to the problem of determining the asymptotic worst-case matching time, as a function of the length of the input strings, when...},
	author = {Weideman, Nicolaas and Merwe, Brink van der and Berglund, Martin and Watson, Bruce},
	booktitle = {Implementation and {Application} of {Automata}},
	file = {WeidemanN et al-Analyzing Matching Time Behavior of Backtracking Regular Expression Matchers by Using Ambiguity of.pdf:/home/linusboyle/Zotero/storage/CWY3YH3B/WeidemanN et al-Analyzing Matching Time Behavior of Backtracking Regular Expression Matchers by Using Ambiguity of.pdf:application/pdf},
	language = {en},
	month = jul,
	pages = {322--334},
	publisher = {Springer, Cham},
	title = {Analyzing {Matching} {Time} {Behavior} of {Backtracking} {Regular} {Expression} {Matchers} by {Using} {Ambiguity} of {NFA}},
	url = {https://link.springer.com/chapter/10.1007/978-3-319-40946-7_27},
	urldate = {2020-07-12},
	year = {2016}
}

@article{schiemer_logic_2013,
	abstract = {In historical discussions of twentieth-century logic, it is typically assumed that model theory emerged within the tradition that adopted first-order logic as the standard framework. Work within the type-theoretic tradition, in the style of Principia Mathematica, tends to be downplayed or ignored in this connection. Indeed, the shift from type theory to first-order logic is sometimes seen as involving a radical break that first made possible the rise of modern model theory. While comparing several early attempts to develop the semantics of axiomatic theories in the 1930s, by two proponents of the type-theoretic tradition (Carnap and Tarski) and two proponents of the first-order tradition (G{\"o}del and Hilbert), we argue that, instead, the move from type theory to first-order logic is better understood as a gradual transformation, and further, that the contributions to semantics made in the type-theoretic tradition should be seen as central to the evolution of model theory.},
	author = {Schiemer, Georg and Reck, Erich H.},
	issn = {1079-8986, 1943-5894},
	journal = {Bulletin of Symbolic Logic},
	language = {en},
	month = sep,
	note = {Publisher: Cambridge University Press},
	number = {4},
	pages = {433--472},
	shorttitle = {Logic in the 1930s},
	title = {Logic in the 1930s: {Type} {Theory} and {Model} {Theory}},
	url = {https://www.cambridge.org/core/journals/bulletin-of-symbolic-logic/article/logic-in-the-1930s-type-theory-and-model-theory/05C644923AF4F0415ECEFB5F535F1B2A},
	urldate = {2020-07-13},
	volume = {19},
	year = {2013}
}

@article{abel_who_2020,
	abstract = {We analyze the computational complexity of the many types of pencil-and-paper-style puzzles featured in the 2016 puzzle video game The Witness. In all puzzles, the goal is to draw a simple path in a rectangular grid graph from a start vertex to a destination vertex. The different puzzle types place different constraints on the path: preventing some edges from being visited (broken edges); forcing some edges or vertices to be visited (hexagons); forcing some cells to have certain numbers of incident path edges (triangles); or forcing the regions formed by the path to be partially monochromatic (squares), have exactly two special cells (stars), or be singly covered by given shapes (polyominoes) and/or negatively counting shapes (antipolyominoes). We show that any one of these clue types (except the first) is enough to make path finding NP-complete (``witnesses exist but are hard to find''), even for rectangular boards. Furthermore, we show that a final clue type (antibody), which necessarily ``cancels'' the effect of another clue in the same region, makes path finding \ensuremath{\Sigma}2-complete (``witnesses do not exist''), even with a single antibody (combined with many anti/polyominoes), and the problem gets no harder with many antibodies. On the positive side, we give a polynomial-time algorithm for monomino clues, by reducing to hexagon clues on the boundary of the puzzle, even in the presence of broken edges, and solving ``subset Hamiltonian path'' for terminals on the boundary of an embedded planar graph in polynomial time.},
	author = {Abel, Zachary and Bosboom, Jeffrey and Coulombe, Michael and Demaine, Erik D. and Hamilton, Linus and Hesterberg, Adam and Kopinsky, Justin and Lynch, Jayson and Rudoy, Mikhail and Thielen, Clemens},
	file = {AbelZ et al-Who witnesses The Witness - Finding witnesses in The Witness is hard and sometimes impossible-Theoretical Computer Science.pdf:/home/linusboyle/Zotero/storage/ISHIJG7L/AbelZ et al-Who witnesses The Witness - Finding witnesses in The Witness is hard and sometimes impossible-Theoretical Computer Science.pdf:application/pdf},
	issn = {0304-3975},
	journal = {Theoretical Computer Science},
	keywords = {Computational complexity, Algorithms, Hamiltonian path, Hardness, Path puzzles, Video games},
	language = {en},
	month = jun,
	shorttitle = {Who witnesses {The} {Witness}?},
	title = {Who witnesses {The} {Witness}? {Finding} witnesses in {The} {Witness} is hard and sometimes impossible},
	url = {http://www.sciencedirect.com/science/article/pii/S0304397520303133},
	urldate = {2020-07-22},
	year = {2020}
}

@article{savage_your_2020,
	author = {Savage, Neil},
	file = {SavageN-Your wish is my CMD-Commun. ACM.pdf:/home/linusboyle/Zotero/storage/PXQWJQJ7/SavageN-Your wish is my CMD-Commun. ACM.pdf:application/pdf},
	issn = {0001-0782, 1557-7317},
	journal = {Communications of the ACM},
	keywords = {Read},
	language = {en},
	month = jun,
	number = {7},
	pages = {15--16},
	title = {Your wish is my {CMD}},
	url = {https://dl.acm.org/doi/10.1145/3398392},
	urldate = {2020-07-22},
	volume = {63},
	year = {2020}
}

@article{kocher_spectre_2020,
	author = {Kocher, Paul and Horn, Jann and Fogh, Anders and Genkin, Daniel and Gruss, Daniel and Haas, Werner and Hamburg, Mike and Lipp, Moritz and Mangard, Stefan and Prescher, Thomas and Schwarz, Michael and Yarom, Yuval},
	file = {KocherP et al-Spectre attacks - exploiting speculative execution-Commun. ACM.pdf:/home/linusboyle/Zotero/storage/KWA85YJR/KocherP et al-Spectre attacks - exploiting speculative execution-Commun. ACM.pdf:application/pdf; KocherP et al-Spectre attacks - exploiting speculative execution-Commun. ACM.pdf:/home/linusboyle/Zotero/storage/ZD2DPHHR/KocherP et al-Spectre attacks - exploiting speculative execution-Commun. ACM.pdf:application/pdf},
	issn = {0001-0782, 1557-7317},
	journal = {Communications of the ACM},
	keywords = {Read},
	language = {en},
	month = jun,
	number = {7},
	pages = {93--101},
	shorttitle = {Spectre attacks},
	title = {Spectre attacks: exploiting speculative execution},
	url = {https://dl.acm.org/doi/10.1145/3399742},
	urldate = {2020-07-22},
	volume = {63},
	year = {2020}
}

@article{thome_integrated_2020,
	abstract = {Malicious users can attack Web applications by exploiting injection vulnerabilities in the source code. This work addresses the challenge of detecting injection vulnerabilities in the server-side code of Java Web applications in a scalable and effective way. We propose an integrated approach that seamlessly combines security slicing with hybrid constraint solving; the latter orchestrates automata-based solving with meta-heuristic search. We use static analysis to extract minimal program slices relevant to security from Web programs and to generate attack conditions. We then apply hybrid constraint solving to determine the satisfiability of attack conditions and thus detect vulnerabilities. The experimental results, using a benchmark comprising a set of diverse and representative Web applications/services as well as security benchmark applications, show that our approach (implemented in the JOACO tool) is significantly more effective at detecting injection vulnerabilities than state-of-the-art approaches, achieving 98 percent recall, without producing any false alarm. We also compared the constraint solving module of our approach with state-of-the-art constraint solvers, using six different benchmark suites; our approach correctly solved the highest number of constraints (665 out of 672), without producing any incorrect result, and was the one with the least number of time-out/failing cases. In both scenarios, the execution time was practically acceptable, given the offline nature of vulnerability detection.},
	author = {Thome, Julian and Shar, Lwin Khin and Bianculli, Domenico and Briand, Lionel},
	file = {ThomeJ et al-An Integrated Approach for Effective Injection Vulnerability Analysis of Web Applications Through-IIEEE Trans. Software Eng..pdf:/home/linusboyle/Zotero/storage/FQWGMIER/ThomeJ et al-An Integrated Approach for Effective Injection Vulnerability Analysis of Web Applications Through-IIEEE Trans. Software Eng..pdf:application/pdf},
	issn = {0098-5589, 1939-3520, 2326-3881},
	journal = {IEEE Transactions on Software Engineering},
	language = {en},
	month = feb,
	number = {2},
	pages = {163--195},
	title = {An {Integrated} {Approach} for {Effective} {Injection} {Vulnerability} {Analysis} of {Web} {Applications} {Through} {Security} {Slicing} and {Hybrid} {Constraint} {Solving}},
	url = {https://ieeexplore.ieee.org/document/8373739/},
	urldate = {2020-07-22},
	volume = {46},
	year = {2020}
}

@article{kugler_are_2020,
	author = {Kugler, Logan},
	file = {KuglerL-Are we addicted to technology-Commun. ACM.pdf:/home/linusboyle/Zotero/storage/UNPGAYIL/KuglerL-Are we addicted to technology-Commun. ACM.pdf:application/pdf},
	issn = {0001-0782, 1557-7317},
	journal = {Communications of the ACM},
	keywords = {Read},
	language = {en},
	month = jul,
	number = {8},
	pages = {15--16},
	title = {Are we addicted to technology?},
	url = {https://dl.acm.org/doi/10.1145/3403966},
	urldate = {2020-07-25},
	volume = {63},
	year = {2020}
}

@article{walder_rebellion_2014,
	abstract = {In the first five years after the onset of the Chinese Cultural Revolution, one of the largest political upheavals of the twentieth century paralyzed a highly centralized party state, leading to a harsh regime of military control. Despite a wave of post-Mao revelations in the 1980s, knowledge about the nationwide impact of this insurgency and its suppression remains selective and impressionistic, based primarily on a handful of local accounts. Employing a data set drawn from historical narratives published in 2,213 county and city annals, this article charts the temporal and geographic spread of a mass insurgency, its evolution through time, and the repression through which militarized state structures were rebuilt. Comparisons of published figures with internal investigation reports and statistical estimates from sample selection models yield estimates that range from 1.1 to 1.6 million deaths and 22 to 30 million direct victims of some form of political persecution. The vast majority of casualties were due to repression by authorities, not the actions of insurgents. Despite the large overall death toll, per capita death rates were considerably lower than a range of comparable cases, including the Soviet purges at the height of Stalinist terror in the late 1930s.},
	author = {Walder, Andrew G.},
	file = {WalderA-Rebellion and Repression in China, 1966--1971-Social Science History.html:/home/linusboyle/Zotero/storage/HDCTCGVJ/WalderA-Rebellion and Repression in China, 1966--1971-Social Science History.html:text/html},
	issn = {0145-5532, 1527-8034},
	journal = {Social Science History},
	language = {en},
	number = {3-4},
	pages = {513--539},
	title = {Rebellion and {Repression} in {China}, 1966--1971},
	url = {https://www.cambridge.org/core/product/identifier/S0145553215000231/type/journal_article},
	urldate = {2020-07-29},
	volume = {38},
	year = {2014}
}

@inproceedings{verma_complexity_2005,
	abstract = {Security protocols employing cryptographic primitives with algebraic properties are conveniently modeled using Horn clauses modulo equational theories. We consider clauses corresponding to the class {\textbackslash}({\textbackslash}mathcal\{H\}3{\textbackslash}) of Nielson, Nielson and Seidl. We show that modulo the theory ACU of an associative-commutative symbol with unit, as well as its variants like the theory XOR and the theory AG of Abelian groups, unsatisfiability is NP-complete. Also membership and intersection-non-emptiness problems for the closely related class of one-way as well as two-way tree automata modulo these equational theories are NP-complete. A key technical tool is a linear time construction of an existential Presburger formula corresponding to the Parikh image of a context-free language. Our algorithms require deterministic polynomial time using an oracle for existential Presburger formulas, suggesting efficient implementations are possible.},
	address = {Berlin, Heidelberg},
	author = {Verma, Kumar Neeraj and Seidl, Helmut and Schwentick, Thomas},
	booktitle = {Automated {Deduction} -- {CADE}-20},
	editor = {Nieuwenhuis, Robert},
	file = {VermaK et al-On the Complexity of Equational Horn Clauses.pdf:/home/linusboyle/Zotero/storage/2KC4SA78/VermaK et al-On the Complexity of Equational Horn Clauses.pdf:application/pdf},
	isbn = {978-3-540-31864-4},
	language = {en},
	pages = {337--352},
	publisher = {Springer},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {On the {Complexity} of {Equational} {Horn} {Clauses}},
	year = {2005}
}

@misc{cameron_graphs_nodate,
	author = {Cameron, Peter J},
	file = {CameronP-Graphs, groups and semigroups.pdf:/home/linusboyle/Zotero/storage/I6K2LDL3/CameronP-Graphs, groups and semigroups.pdf:application/pdf},
	keywords = {⛔ No DOI found},
	language = {en},
	title = {Graphs, groups and semigroups}
}

@inproceedings{hangal_tsotool_2004,
	abstract = {In this paper, we describe TSOtool, a program to check the behavior of the memory subsystem in a shared memory multiprocessor. TSOtool runs pseudo-randomly generated programs with data races on a system compliant with the total store order (TSO) memory consistency model; it then checks the results of the program against the formal TSO specification. Such analysis can expose subtle memory errors like data corruption, atomicity violation and illegal instruction ordering. While verifying TSO compliance completely is an NP-complete problem, we describe a new polynomial time algorithm which is incorporated in TSOtool. In spite of being incomplete, it has been successful in detecting several bugs in the design of commercial microprocessors and systems, during both pre-silicon and post-silicon phases of validation.},
	author = {Hangal, S. and Vahia, D. and Manovit, C. and Lu, J.-Y.J. and Narayanan, S.},
	booktitle = {Proceedings. 31st {Annual} {International} {Symposium} on {Computer} {Architecture}, 2004.},
	file = {HangalS et al-TSOtool - a program for verifying memory systems using the memory consistency model.pdf:/home/linusboyle/Zotero/storage/BG5MX9PB/HangalS et al-TSOtool - a program for verifying memory systems using the memory consistency model.pdf:application/pdf},
	keywords = {Read, atomicity violation, bug detection, computational complexity, Computer bugs, computer debugging, data corruption, data integrity, data races, formal specification, formal TSO specification, formal verification, Hardware, illegal instruction ordering, memory consistency model, memory errors, memory subsystem behavior checking, memory system verification, microprocessor chips, Microprocessors, multiprocessor verification, Multithreading, NP-complete problem, performance evaluation, Phase detection, polynomial time algorithm, Polynomials, Programming profession, Protocols, pseudorandomly generated programs, sequential consistency, shared memory multiprocessor, shared memory systems, Sun, Surface-mount technology, total store order, TSOtool program},
	month = jun,
	note = {ISSN: 1063-6897},
	pages = {114--123},
	shorttitle = {{TSOtool}},
	title = {{TSOtool}: a program for verifying memory systems using the memory consistency model},
	year = {2004}
}

@inproceedings{aharoni_using_2016,
	abstract = {The hardware address translation mechanism is an essential part of modern microprocessor memory management. The ever-growing demand for performance and low power of integrated circuits makes this mechanism exceptionally complex, and its verification requires sophisticated test generation tools. This paper presents a solution, based on constraint satisfaction, to generate stimuli for testing address translation.The address translation process passes through a sequence of steps and can therefore be naturally described as a directed acyclic graph. We developed a framework that we call graph-based constraint satisfaction problems (GCSP). These problems consist of a directed graph, combined with a CSP, where each variable and constraint of the CSP is linked to a particular node or edge of the graph. A solution to the problem is a path in the graph, such that all constraints defined along this path must be satisfied. We base our algorithm for solving GCSPs on conditional CSP. We successfully used this technology to verify the memory management units of several industrial microprocessors.},
	address = {Cham},
	author = {Aharoni, Merav and Ben-Haim, Yael and Doron, Shai and Koyfman, Anatoly and Tsanko, Elena and Veksler, Michael},
	booktitle = {Principles and {Practice} of {Constraint} {Programming}},
	editor = {Rueher, Michel},
	file = {AharoniM et al-Using Graph-Based CSP to Solve the Address Translation Problem.pdf:/home/linusboyle/Zotero/storage/HXVJUUNM/AharoniM et al-Using Graph-Based CSP to Solve the Address Translation Problem.pdf:application/pdf},
	isbn = {978-3-319-44953-1},
	keywords = {Read, Address translation, Conditional CSP, Constraint programming, Dynamic CSP, Graph, Graphplan, Memory management unit, Network, Path, Planning, Processor verification},
	language = {en},
	pages = {843--858},
	publisher = {Springer International Publishing},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Using {Graph}-{Based} {CSP} to {Solve} the {Address} {Translation} {Problem}},
	year = {2016}
}

@article{adve_shared_1996,
	abstract = {The memory consistency model of a system affects performance, programmability, and portability. We aim to describe memory consistency models in a way that most computer professionals would understand. This is important if the performance-enhancing features being incorporated by system designers are to be correctly and widely used by programmers. Our focus is consistency models proposed for hardware-based shared memory systems. Most of these models emphasize the system optimizations they support, and we retain this system-centric emphasis. We also describe an alternative, programmer-centric view of relaxed consistency models that describes them in terms of program behavior, not system optimizations.},
	author = {Adve, S.V. and Gharachorloo, K.},
	file = {AdveS&GharachorlooK-Shared memory consistency models - a tutorial.pdf:/home/linusboyle/Zotero/storage/U2ANNSJE/AdveS&GharachorlooK-Shared memory consistency models - a tutorial.pdf:application/pdf},
	issn = {1558-0814},
	journal = {Computer},
	keywords = {Read, data integrity, Hardware, Programming profession, shared memory systems, Computer architecture, computer professionals, hardware-based shared memory systems, High level languages, Magnetic heads, Message passing, Optimizing compilers, parallel programming, performance, performance-enhancing features, portability, program behavior, programmability, programmer-centric, Read-write memory, relaxed consistency models, shared memory consistency models, Software design, software performance evaluation, software portability, system designers, system optimizations, system-centric, Tutorial},
	month = dec,
	note = {Conference Name: Computer},
	number = {12},
	pages = {66--76},
	shorttitle = {Shared memory consistency models},
	title = {Shared memory consistency models: a tutorial},
	volume = {29},
	year = {1996}
}

@inproceedings{geller_assumption-based_2005,
	abstract = {A conditional constraint satisfaction problem (CCSP) is a variant of the standard constraint satisfaction problem (CSP). CCSPs model problems where some of the variables and constraints may be conditionally inactive such that they do not participate in a solution. Recently, algorithms were introduced that use MAC at their core to solve CCSP. We extend MAC with a simple assumption-based reasoning. The resulting algorithm, Activity MAC (AMAC), is able to achieve significantly better pruning than existing methods. AMAC is shown to be more than two orders of magnitude more efficient than CondMAC on certain problem classes. Our algorithm is most naturally expressed using a variant of the CCSP representation that we refer to as Activity CSP (ACSP). ACSP introduces activity variables which explicitly control the presence of other variables in the solution. Common aspects of CCSP, such as activity clustering and disjunction, are easily captured by ACSP and contribute to improved pruning by AMAC.},
	address = {Berlin, Heidelberg},
	author = {Geller, Felix and Veksler, Michael},
	booktitle = {Principles and {Practice} of {Constraint} {Programming} - {CP} 2005},
	editor = {van Beek, Peter},
	file = {GellerF&VekslerM-Assumption-Based Pruning in Conditional CSP.pdf:/home/linusboyle/Zotero/storage/7M7BH6AR/GellerF&VekslerM-Assumption-Based Pruning in Conditional CSP.pdf:application/pdf},
	isbn = {978-3-540-32050-0},
	keywords = {Activity Cluster, Activity Constraint, Activity Variable, Compatibility Constraint, Constraint Satisfaction Problem},
	language = {en},
	pages = {241--255},
	publisher = {Springer},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Assumption-{Based} {Pruning} in {Conditional} {CSP}},
	year = {2005}
}

@inproceedings{gehr_ai2_2018,
	abstract = {We present AI2, the first sound and scalable analyzer for deep neural networks. Based on overapproximation, AI2 can automatically prove safety properties (e.g., robustness) of realistic neural networks (e.g., convolutional neural networks). The key insight behind AI2 is to phrase reasoning about safety and robustness of neural networks in terms of classic abstract interpretation, enabling us to leverage decades of advances in that area. Concretely, we introduce abstract transformers that capture the behavior of fully connected and convolutional neural network layers with rectified linear unit activations (ReLU), as well as max pooling layers. This allows us to handle real-world neural networks, which are often built out of those types of layers. We present a complete implementation of AI2 together with an extensive evaluation on 20 neural networks. Our results demonstrate that: (i) AI2 is precise enough to prove useful specifications (e.g., robustness), (ii) AI2 can be used to certify the effectiveness of state-of-the-art defenses for neural networks, (iii) AI2 is significantly faster than existing analyzers based on symbolic analysis, which often take hours to verify simple fully connected networks, and (iv) AI2 can handle deep convolutional networks, which are beyond the reach of existing methods.},
	address = {San Francisco, CA},
	author = {Gehr, Timon and Mirman, Matthew and Drachsler-Cohen, Dana and Tsankov, Petar and Chaudhuri, Swarat and Vechev, Martin},
	booktitle = {2018 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	file = {GehrT et al-AI2 - Safety and Robustness Certification of Neural Networks with Abstract Interpretation.pdf:/home/linusboyle/Zotero/storage/MNXKZIC5/GehrT et al-AI2 - Safety and Robustness Certification of Neural Networks with Abstract Interpretation.pdf:application/pdf},
	isbn = {978-1-5386-4353-2},
	language = {en},
	month = may,
	pages = {3--18},
	publisher = {IEEE},
	shorttitle = {{AI2}},
	title = {{AI2}: {Safety} and {Robustness} {Certification} of {Neural} {Networks} with {Abstract} {Interpretation}},
	url = {https://ieeexplore.ieee.org/document/8418593/},
	urldate = {2020-08-17},
	year = {2018}
}

@article{narayanan_dark_2020,
	author = {Narayanan, Arvind and Mathur, Arunesh and Chetty, Marshini and Kshirsagar, Mihir},
	file = {NarayananA et al-Dark patterns - past, present, and future-Commun. ACM.pdf:/home/linusboyle/Zotero/storage/SCVWAZ83/NarayananA et al-Dark patterns - past, present, and future-Commun. ACM.pdf:application/pdf},
	issn = {0001-0782, 1557-7317},
	journal = {Communications of the ACM},
	language = {en},
	month = aug,
	number = {9},
	pages = {42--47},
	shorttitle = {Dark patterns},
	title = {Dark patterns: past, present, and future},
	url = {https://dl.acm.org/doi/10.1145/3397884},
	urldate = {2020-09-02},
	volume = {63},
	year = {2020}
}

@article{kelly_is_2020,
	author = {Kelly, Terence},
	file = {KellyT-Is persistent memory persistent-Commun. ACM.pdf:/home/linusboyle/Zotero/storage/X7ZR6EAI/KellyT-Is persistent memory persistent-Commun. ACM.pdf:application/pdf},
	issn = {0001-0782, 1557-7317},
	journal = {Communications of the ACM},
	language = {en},
	month = aug,
	number = {9},
	pages = {48--54},
	title = {Is persistent memory persistent?},
	url = {https://dl.acm.org/doi/10.1145/3397882},
	urldate = {2020-09-02},
	volume = {63},
	year = {2020}
}

@inproceedings{allauzen_unified_2006,
	abstract = {A number of different techniques have been introduced in the last few decades to create \ensuremath{\varepsilon}-free automata representing regular expressions such as the Glushkov automata, follow automata, or Antimirov...},
	author = {Allauzen, Cyril and Mohri, Mehryar},
	booktitle = {Mathematical {Foundations} of {Computer} {Science} 2006},
	file = {AllauzenC&MohriM-A Unified Construction of the Glushkov, Follow, and Antimirov Automata.pdf:/home/linusboyle/Zotero/storage/JWR9T789/AllauzenC&MohriM-A Unified Construction of the Glushkov, Follow, and Antimirov Automata.pdf:application/pdf},
	language = {en},
	month = aug,
	pages = {110--121},
	publisher = {Springer, Berlin, Heidelberg},
	title = {A {Unified} {Construction} of the {Glushkov}, {Follow}, and {Antimirov} {Automata}},
	url = {https://link.springer.com/chapter/10.1007/11821069_10},
	urldate = {2020-09-29},
	year = {2006}
}

@inproceedings{zhang_autotap_2019,
	abstract = {End-user programming, particularly trigger-action programming (TAP), is a popular method of letting users express their intent for how smart devices and cloud services interact. Unfortunately, sometimes it can be challenging for users to correctly express their desires through TAP. This paper presents AutoTap, a system that lets novice users easily specify desired properties for devices and services. AutoTap translates these properties to linear temporal logic (LTL) and both automatically synthesizes property-satisfying TAP rules from scratch and repairs existing TAP rules. We designed AutoTap based on a user study about properties users wish to express. Through a second user study, we show that novice users made significantly fewer mistakes when expressing desired behaviors using AutoTap than using TAP rules. Our experiments show that AutoTap is a simple and effective option for expressive end-user programming.},
	author = {Zhang, Lefan and He, Weijia and Martinez, Jesse and Brackenbury, Noah and Lu, Shan and Ur, Blase},
	booktitle = {2019 {IEEE}/{ACM} 41st {International} {Conference} on {Software} {Engineering} ({ICSE})},
	doi = {10.1109/icse.2019.00043},
	file = {ZhangL et al-AutoTap - Synthesizing and Repairing Trigger-Action Programs Using LTL Properties.pdf:/home/linusboyle/Zotero/storage/YKBAAPE7/ZhangL et al-AutoTap - Synthesizing and Repairing Trigger-Action Programs Using LTL Properties.pdf:application/pdf},
	keywords = {Programming, Read, Computer bugs, AutoTap, cloud computing, cloud services, end-user programming, End-user programming, linear temporal logic, LTL properties, Maintenance engineering, Microsoft Windows, program repair, Program repair, program synthesis, Program synthesis, property-satisfying TAP rules, Rain, Safety, smart devices, Smart devices, software maintenance, temporal logic, trigger-action programming, Trigger-action programming, trigger-action programs},
	month = may,
	note = {ISSN: 1558-1225},
	pages = {281--291},
	shorttitle = {{AutoTap}},
	title = {{AutoTap}: {Synthesizing} and {Repairing} {Trigger}-{Action} {Programs} {Using} {LTL} {Properties}},
	year = {2019}
}

@article{neugebauer_bibtool_nodate,
	author = {Neugebauer, Gerd},
	file = {NeugebauerG-BibTool Quick Reference Card.pdf:/home/linusboyle/Zotero/storage/FTJMRCCX/NeugebauerG-BibTool Quick Reference Card.pdf:application/pdf},
	keywords = {⛔ No DOI found},
	language = {en},
	pages = {2},
	title = {{BibTool} {Quick} {Reference} {Card}}
}

@article{chen_decision_2020,
	author = {Chen, Taolue and Hague, Matthew and He, Jinlong and Hu, Denghang and Lin, Anthony Widjaja and Rummer, Philipp and Wu, Zhilin},
	file = {ChenT et al-A decision procedure for path feasibility of string manipulating programs with integer data type.pdf:/home/linusboyle/Zotero/storage/D7B53CTZ/ChenT et al-A decision procedure for path feasibility of string manipulating programs with integer data type.pdf:application/pdf},
	note = {arXiv: 2007.06913 [cs.LO]},
	title = {A decision procedure for path feasibility of string manipulating programs with integer data type},
	year = {2020}
}

@article{gulwani_program_2017,
	author = {Gulwani, Sumit and Polozov, Oleksandr and Singh, Rishabh},
	file = {GulwaniS et al-Program synthesis.pdf:/home/linusboyle/Zotero/storage/Q8KSPXXK/GulwaniS et al-Program synthesis.pdf:application/pdf},
	journal = {Foundations and trends in programming languages},
	keywords = {Read},
	language = {eng},
	note = {ISBN: 9781680832921 Num Pages: 126 Place: Hanover, MA Delft Publisher: Now Publishers Series Number: 4.2017, 1-2},
	series = {Foundations and trends in programming languages},
	title = {Program synthesis},
	year = {2017}
}

@inproceedings{amadini_constraint_2019,
	author = {Amadini, Roberto and Andrlon, Mak and Gange, Graeme and Schachte, Peter and S{\o}ndergaard, Harald and Stuckey, Peter J.},
	booktitle = {Integration of {Constraint} {Programming}, {Artificial} {Intelligence}, and {Operations} {Research}: 16th {International} {Conference}, {CPAIOR} 2019 {Thessaloniki}, {Greece}, {June} 4--7, 2019 {Proceedings}},
	doi = {10.1007/978-3-030-19212-9_1},
	file = {AmadiniR et al-Constraint programming for dynamic symbolic execution of JavaScript.pdf:/home/linusboyle/Zotero/storage/3DLAZR3R/AmadiniR et al-Constraint programming for dynamic symbolic execution of JavaScript.pdf:application/pdf},
	language = {English},
	pages = {1--19},
	publisher = {Springer},
	title = {Constraint programming for dynamic symbolic execution of {JavaScript}},
	url = {https://research.monash.edu/en/publications/constraint-programming-for-dynamic-symbolic-execution-of-javascri},
	urldate = {2020-11-26},
	year = {2019}
}

@article{trinh_inter-theory_2020,
	abstract = {Solvers in the framework of Satisfiability Modulo Theories (SMT) have been widely successful in practice. Recently there has been an increasing interest in solvers for string constraints to address security issues in web programming, for example. To be practically useful, the solvers need to support an expressive constraint language over unbounded strings, and in particular, over string lengths. Satisfiability checking for these formulas, especially in the SMT context, is very hard; it is generally undecidable for a rich fragment. In this paper, we propose a form of dependency analysis for a rich fragment of string constraints including high-level operations such as length, contains to deal with their inter-theory interaction so as to solve them more efficiently. We implement our dependency analysis in the string theory of the Z3 solver to obtain a new one, called S3N. Finally, we demonstrate the superior performance of S3N over state-of-the-art string solvers such as Z3str3, CVC4, S3P, and Z3 on several large industrial-strength benchmarks.},
	author = {Trinh, Minh-Thai and Chu, Duc-Hiep and Jaffar, Joxan},
	doi = {10.1145/3428260},
	file = {TrinhM et al-Inter-theory dependency analysis for SMT string solvers-Proc. ACM Program. Lang..pdf:/home/linusboyle/Zotero/storage/M3XUES4A/TrinhM et al-Inter-theory dependency analysis for SMT string solvers-Proc. ACM Program. Lang..pdf:application/pdf},
	journal = {Proceedings of the ACM on Programming Languages},
	keywords = {Automated Reasoning, Dependency Analysis, Inter-theory, SMT, String Constraints, Web Security},
	month = nov,
	number = {OOPSLA},
	pages = {192:1--192:27},
	title = {Inter-theory dependency analysis for {SMT} string solvers},
	url = {https://doi.org/10.1145/3428260},
	urldate = {2020-11-27},
	volume = {4},
	year = {2020}
}

@inproceedings{godefroid_automated_2012,
	abstract = {Symbolic execution is a key component of precise binary program analysis tools. We discuss how to automatically boot-strap the construction of a symbolic execution engine for a processor instruction set such as x86, x64 or ARM. We show how to automatically synthesize symbolic representations of individual processor instructions from input/output examples and express them as bit-vector constraints. We present and compare various synthesis algorithms and instruction sampling strategies. We introduce a new synthesis algorithm based on smart sampling which we show is one to two orders of magnitude faster than previous synthesis algorithms in our context. With this new algorithm, we can automatically synthesize bit-vector circuits for over 500 x86 instructions (8/16/32-bits, outputs, EFLAGS) using only 6 synthesis templates and in less than two hours using the Z3 SMT solver on a regular machine. During this work, we also discovered several inconsistencies across x86 processors, errors in the x86 Intel spec, and several bugs in previous manually-written x86 instruction handlers.},
	address = {New York, NY, USA},
	author = {Godefroid, Patrice and Taly, Ankur},
	booktitle = {Proceedings of the 33rd {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	doi = {10.1145/2254064.2254116},
	file = {GodefroidP&TalyA-Automated synthesis of symbolic instruction encodings from I-O samples.pdf:/home/linusboyle/Zotero/storage/94ZH7JPG/GodefroidP&TalyA-Automated synthesis of symbolic instruction encodings from I-O samples.pdf:application/pdf},
	isbn = {978-1-4503-1205-9},
	keywords = {program synthesis, symbolic execution, x86},
	month = jun,
	pages = {441--452},
	publisher = {Association for Computing Machinery},
	series = {{PLDI} '12},
	title = {Automated synthesis of symbolic instruction encodings from {I}/{O} samples},
	url = {https://doi.org/10.1145/2254064.2254116},
	urldate = {2020-11-30},
	year = {2012}
}

@inproceedings{gulwani_automating_2011,
	abstract = {We describe the design of a string programming/expression language that supports restricted forms of regular expressions, conditionals and loops. The language is expressive enough to represent a wide variety of string manipulation tasks that end-users struggle with. We describe an algorithm based on several novel concepts for synthesizing a desired program in this language from input-output examples. The synthesis algorithm is very efficient taking a fraction of a second for various benchmark examples. The synthesis algorithm is interactive and has several desirable features: it can rank multiple solutions and has fast convergence, it can detect noise in the user input, and it supports an active interaction model wherein the user is prompted to provide outputs on inputs that may have multiple computational interpretations. The algorithm has been implemented as an interactive add-in for Microsoft Excel spreadsheet system. The prototype tool has met the golden test - it has synthesized part of itself, and has been used to solve problems beyond author's imagination.},
	address = {New York, NY, USA},
	author = {Gulwani, Sumit},
	booktitle = {Proceedings of the 38th annual {ACM} {SIGPLAN}-{SIGACT} symposium on {Principles} of programming languages},
	doi = {10.1145/1926385.1926423},
	file = {GulwaniS-Automating string processing in spreadsheets using input-output examples.pdf:/home/linusboyle/Zotero/storage/VXKHU5U4/GulwaniS-Automating string processing in spreadsheets using input-output examples.pdf:application/pdf},
	isbn = {978-1-4503-0490-0},
	keywords = {program synthesis, programming by example (pbe), spreadsheet programming, string manipulation, user intent, version space algebra},
	month = jan,
	pages = {317--330},
	publisher = {Association for Computing Machinery},
	series = {{POPL} '11},
	title = {Automating string processing in spreadsheets using input-output examples},
	url = {https://doi.org/10.1145/1926385.1926423},
	urldate = {2020-12-10},
	year = {2011}
}

@article{gulwani_spreadsheet_2012,
	abstract = {Millions of computer end users need to perform tasks over large spreadsheet data, yet lack the programming knowledge to do such tasks automatically. We present a programming by example methodology that allows end users to automate such repetitive tasks. Our methodology involves designing a domain-specific language and developing a synthesis algorithm that can learn programs in that language from user-provided examples. We present instantiations of this methodology for particular domains of tasks: (a) syntactic transformations of strings using restricted forms of regular expressions, conditionals, and loops, (b) semantic transformations of strings involving lookup in relational tables, and (c) layout transformations on spreadsheet tables. We have implemented this technology as an add-in for the Microsoft Excel Spreadsheet system and have evaluated it successfully over several benchmarks picked from various Excel help forums.},
	author = {Gulwani, Sumit and Harris, William R. and Singh, Rishabh},
	doi = {10.1145/2240236.2240260},
	file = {GulwaniS et al-Spreadsheet data manipulation using examples-Commun. ACM.pdf:/home/linusboyle/Zotero/storage/LEGNSKPP/GulwaniS et al-Spreadsheet data manipulation using examples-Commun. ACM.pdf:application/pdf},
	issn = {0001-0782},
	journal = {Communications of the ACM},
	month = aug,
	number = {8},
	pages = {97--105},
	title = {Spreadsheet data manipulation using examples},
	url = {https://doi.org/10.1145/2240236.2240260},
	urldate = {2020-12-10},
	volume = {55},
	year = {2012}
}

@inproceedings{heizmann_termination_2014,
	abstract = {We present a novel approach to termination analysis. In a first step, the analysis uses a program as a black-box which exhibits only a finite set of sample traces. Each sample trace is infinite but can be represented by a finite lasso. The analysis can ''learn'' a program from a termination proof for the lasso, a program that is terminating by construction. In a second step, the analysis checks that the set of sample traces is representative in a sense that we can make formal. An experimental evaluation indicates that the approach is a potentially useful addition to the portfolio of existing approaches to termination analysis.},
	address = {Cham},
	author = {Heizmann, Matthias and Hoenicke, Jochen and Podelski, Andreas},
	booktitle = {Computer {Aided} {Verification}},
	doi = {10.1007/978-3-319-08867-9_53},
	editor = {Biere, Armin and Bloem, Roderick},
	file = {HeizmannM et al-Termination Analysis by Learning Terminating Programs.pdf:/home/linusboyle/Zotero/storage/RHAV5NVK/HeizmannM et al-Termination Analysis by Learning Terminating Programs.pdf:application/pdf},
	isbn = {978-3-319-08867-9},
	keywords = {Fairness Constraint, Nondeterministic Choice, Ranking Function, Read, Termination Proof, Transition Invariant},
	language = {en},
	pages = {797--813},
	publisher = {Springer International Publishing},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Termination {Analysis} by {Learning} {Terminating} {Programs}},
	year = {2014}
}

@article{cook_proving_2011,
	author = {Cook, Byron and Podelski, Andreas and Rybalchenko, Andrey},
	doi = {10.1145/1941487.1941509},
	file = {CookB et al-Proving program termination-Commun. ACM.pdf:/home/linusboyle/Zotero/storage/QNWXZMG2/CookB et al-Proving program termination-Commun. ACM.pdf:application/pdf},
	issn = {0001-0782, 1557-7317},
	journal = {Communications of the ACM},
	keywords = {Read, Termination Proof},
	language = {en},
	month = may,
	number = {5},
	pages = {88--98},
	title = {Proving program termination},
	url = {https://dl.acm.org/doi/10.1145/1941487.1941509},
	urldate = {2020-12-14},
	volume = {54},
	year = {2011}
}

@inproceedings{cook_ramsey_2013-1,
	abstract = {Termination proving has traditionally been based on the search for (possibly lexicographic) ranking functions. In recent years, however, the discovery of termination proof techniques based on Ramsey{\rq}s theorem have led to new automation strategies, e.g. size-change, or iterative reductions from termination to safety. In this paper we revisit the decision to use Ramsey-based termination arguments in the iterative approach. We describe a new iterative termination proving procedure that instead searches for lexicographic termination arguments. Using experimental evidence we show that this new method leads to dramatic speedups.},
	address = {Berlin, Heidelberg},
	author = {Cook, Byron and See, Abigail and Zuleger, Florian},
	booktitle = {Tools and {Algorithms} for the {Construction} and {Analysis} of {Systems}},
	doi = {10.1007/978-3-642-36742-7_4},
	editor = {Piterman, Nir and Smolka, Scott A.},
	file = {CookB et al-Ramsey vs. Lexicographic Termination Proving.pdf:/home/linusboyle/Zotero/storage/4A2ALIAW/CookB et al-Ramsey vs. Lexicographic Termination Proving.pdf:application/pdf},
	isbn = {978-3-642-36742-7},
	keywords = {Ranking Function, Termination Proof, Model Check, Transitive Closure, Validity Check},
	language = {en},
	pages = {47--61},
	publisher = {Springer},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Ramsey vs. {Lexicographic} {Termination} {Proving}},
	year = {2013}
}

@inproceedings{chen_proving_2020,
	abstract = {Almost-sure termination is the most basic liveness property of probabilistic programs. We present a novel decomposition-based approach for proving almost-sure termination of probabilistic programs with complex control-flow structure and non-determinism. Our approach automatically decomposes the runs of the probabilistic program into a finite union of \ensuremath{\omega}-regular subsets and then proves almost-sure termination of each subset based on the notion of localized ranking supermartingales. Compared to the lexicographic methods and the compositional methods, our approach does not require a lexicographic order over the ranking supermartingales as well as the so-called unaffecting condition. Thus it has high generality. We present the algorithm of our approach and prove its soundness, as well as its relative completeness. We show that our approach can be applied to some hard cases and the evaluation on the benchmarks of previous works shows the significant efficiency of our approach.},
	address = {New York, NY, USA},
	author = {Chen, Jianhui and He, Fei},
	booktitle = {Proceedings of the 41st {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	doi = {10.1145/3385412.3386002},
	file = {ChenJ&HeF-Proving almost-sure termination by omega-regular decomposition.pdf:/home/linusboyle/Zotero/storage/2KSJP3LY/ChenJ&HeF-Proving almost-sure termination by omega-regular decomposition.pdf:application/pdf},
	isbn = {978-1-4503-7613-6},
	keywords = {Almost-Sure Termination, Omega-Regular Languages, Probabilistic Programs, Ranking Supermartingales},
	month = jun,
	pages = {869--882},
	publisher = {Association for Computing Machinery},
	series = {{PLDI} 2020},
	title = {Proving almost-sure termination by omega-regular decomposition},
	url = {https://doi.org/10.1145/3385412.3386002},
	urldate = {2020-12-16},
	year = {2020}
}

@article{he_termination_2020,
	abstract = {Research on program termination has a long tradition. However, most of the existing techniques target a single program only. We propose in this paper an incremental termination analysis approach by reusing certified modules across different program versions. A transformation-based procedure is further developed to increase the reusability of certified modules. The proposed approach has wide applicability, applicable to various program changes. The proposed technique, to the best of our knowledge, represents a novel attempt to the termination analysis of evolving programs. We implemented the approach on top of Ultimate Automizer. Experimental results show dramatic improvement of our approach over the state-of-the-art tool.},
	author = {He, Fei and Han, Jitao},
	doi = {10.1145/3428267},
	file = {HeF&HanJ-Termination analysis for evolving programs - an incremental approach by reusing certified modules-Proc. ACM Program. Lang..pdf:/home/linusboyle/Zotero/storage/SLC6C4ZI/HeF&HanJ-Termination analysis for evolving programs - an incremental approach by reusing certified modules-Proc. ACM Program. Lang..pdf:application/pdf},
	journal = {Proceedings of the ACM on Programming Languages},
	keywords = {Read, incremental analysis, ranking function, Termination analysis, B{\"u}chi automaton},
	month = nov,
	number = {OOPSLA},
	pages = {199:1--199:27},
	shorttitle = {Termination analysis for evolving programs},
	title = {Termination analysis for evolving programs: an incremental approach by reusing certified modules},
	url = {https://doi.org/10.1145/3428267},
	urldate = {2020-12-17},
	volume = {4},
	year = {2020}
}

@inproceedings{chen_advanced_2018,
	abstract = {In 2014, Heizmann et al. proposed a novel framework for program termination analysis. The analysis starts with a termination proof of a sample path. The path is generalized to a B{\"u}chi automaton (BA) whose language (by construction) represents a set of terminating paths. All these paths can be safely removed from the program. The removal of paths is done using automata difference, implemented via BA complementation and intersection. The analysis constructs in this way a set of BAs that jointly "cover" the behavior of the program, thus proving its termination. An implementation of the approach in Ultimate Automizer won the 1st place in the Termination category of SV-COMP 2017. In this paper, we exploit advanced automata-based algorithms and propose several non-trivial improvements of the framework. To alleviate the complementation computation for BAs---one of the most expensive operations in the framework---, we propose a multi-stage generalization construction. We start with generalizations producing subclasses of BAs (such as deterministic BAs) for which efficient complementation algorithms are known, and proceed to more general classes only if necessary. Particularly, we focus on the quite expressive subclass of semideterministic BAs and provide an improved complementation algorithm for this class. Our experimental evaluation shows that the proposed approach significantly improves the power of termination checking within the Ultimate Automizer framework.},
	address = {New York, NY, USA},
	author = {Chen, Yu-Fang and Heizmann, Matthias and Leng{\'a}l, Ond{\v r}ej and Li, Yong and Tsai, Ming-Hsien and Turrini, Andrea and Zhang, Lijun},
	booktitle = {Proceedings of the 39th {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	doi = {10.1145/3192366.3192405},
	file = {ChenY et al-Advanced automata-based algorithms for program termination checking.pdf:/home/linusboyle/Zotero/storage/FWZBT9YL/ChenY et al-Advanced automata-based algorithms for program termination checking.pdf:application/pdf},
	isbn = {978-1-4503-5698-5},
	keywords = {B{\"u}chi Automata Complementation and Language Difference, Program Termination},
	month = jun,
	pages = {135--150},
	publisher = {Association for Computing Machinery},
	series = {{PLDI} 2018},
	title = {Advanced automata-based algorithms for program termination checking},
	url = {https://doi.org/10.1145/3192366.3192405},
	urldate = {2020-12-16},
	year = {2018}
}

@inproceedings{blahoudek_complementing_2016,
	abstract = {We introduce an efficient complementation technique for semi-deterministic B{\"u}chi automata, which are B{\"u}chi automata that are deterministic in the limit: from every accepting state onward, their behaviour is deterministic. It is interesting to study semi-deterministic automata, because they play a role in practical applications of automata theory, such as the analysis of Markov decision processes. Our motivation to study their complementation comes from the termination analysis implemented in Ultimate B{\"u}chi Automizer, where these automata represent checked runs and have to be complemented to identify runs to be checked. We show that semi-determinism leads to a simpler complementation procedure: an extended breakpoint construction that allows for symbolic implementation. It also leads to significantly improved bounds as the complement of a semi-deterministic automaton with n states has less than 4n4n4{\textasciicircum}n states. Moreover, the resulting automaton is unambiguous, which again offers new applications, like the analysis of Markov chains. We have evaluated our construction against the semi-deterministic automata produced by the Ultimate B{\"u}chi Automizer. The evaluation confirms that our algorithm outperforms the known complementation techniques for general nondeterministic B{\"u}chi automata.},
	address = {Berlin, Heidelberg},
	author = {Blahoudek, Franti{\v s}ek and Heizmann, Matthias and Schewe, Sven and Strej{\v c}ek, Jan and Tsai, Ming-Hsien},
	booktitle = {Tools and {Algorithms} for the {Construction} and {Analysis} of {Systems}},
	doi = {10.1007/978-3-662-49674-9_49},
	editor = {Chechik, Marsha and Raskin, Jean-Fran{\c c}ois},
	file = {BlahoudekF et al-Complementing Semi-deterministic B{\"u}chi Automata.pdf:/home/linusboyle/Zotero/storage/LJJB9SKR/BlahoudekF et al-Complementing Semi-deterministic B{\"u}chi Automata.pdf:application/pdf},
	isbn = {978-3-662-49674-9},
	keywords = {Model Check, Complementation Construction, Infinite Word, Level Ranking, Markov Decision Process},
	language = {en},
	pages = {770--787},
	publisher = {Springer},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Complementing {Semi}-deterministic {B{\"u}chi} {Automata}},
	year = {2016}
}

@inproceedings{leike_ranking_2014,
	abstract = {We present a new method for the constraint-based synthesis of termination arguments for linear loop programs based on linear ranking templates. Linear ranking templates are parametrized, well-founded relations such that an assignment to the parameters gives rise to a ranking function. This approach generalizes existing methods and enables us to use templates for many different ranking functions with affine-linear components. We discuss templates for multiphase, piecewise, and lexicographic ranking functions. Because these ranking templates require both strict and non-strict inequalities, we use Motzkin{\rq}s Transposition Theorem instead of Farkas Lemma to transform the generated \ensuremath{\exists} \ensuremath{\forall}-constraint into an \ensuremath{\exists}-constraint.},
	address = {Berlin, Heidelberg},
	author = {Leike, Jan and Heizmann, Matthias},
	booktitle = {Tools and {Algorithms} for the {Construction} and {Analysis} of {Systems}},
	doi = {10.1007/978-3-642-54862-8_12},
	editor = {{\'A}brah{\'a}m, Erika and Havelund, Klaus},
	file = {LeikeJ&HeizmannM-Ranking Templates for Linear Loops.pdf:/home/linusboyle/Zotero/storage/Y9XWK752/LeikeJ&HeizmannM-Ranking Templates for Linear Loops.pdf:application/pdf},
	isbn = {978-3-642-54862-8},
	keywords = {Ranking Function, Boolean Combination, Function Symbol, Linear Arithmetic, Linear Loop},
	language = {en},
	pages = {172--186},
	publisher = {Springer},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Ranking {Templates} for {Linear} {Loops}},
	year = {2014}
}

@phdthesis{heizmann_traces_2015,
	address = {Freiburg im Breisgau, Germany},
	author = {Heizmann, Matthias},
	file = {HeizmannM-Traces, Interpolants, and Automata - A New Approach to Automatic Software Verification.pdf:/home/linusboyle/Zotero/storage/X88VIJXK/HeizmannM-Traces, Interpolants, and Automata - A New Approach to Automatic Software Verification.pdf:application/pdf},
	language = {en},
	school = {University of Freiburg},
	title = {Traces, {Interpolants}, and {Automata}: {A} {New} {Approach} to {Automatic} {Software} {Verification}},
	type = {Phd {Thesis}},
	url = {https://d-nb.info/1119452430/34},
	year = {2015}
}

@inproceedings{alur_adding_2006,
	abstract = {We propose nested words to capture models where there is both a natural linear sequencing of positions and a hierarchically nested matching of positions. Such dual structure exists for executions of structured programs where there is a natural well-nested correspondence among entries to and exits from program components such as functions and procedures, and for XML documents where each open-tag is matched with a closing tag in a well-nested manner.We define and study finite-state automata as acceptors of nested words. A nested-word automaton is similar to a classical finite-state word automaton, and reads the input from left to right according to the linear sequence. However, at a position with two predecessors, one due to linear sequencing and one due to a hierarchical nesting edge, the next state depends on states of the run at both these predecessors. The resulting class of regular languages of nested words has all the appealing theoretical properties that the class of classical regular word languages enjoys: deterministic nested word automata are as expressive as their nondeterministic counterparts; the class is closed under operations such as union, intersection, complementation, concatenation, and Kleene-*; decision problems such as membership, emptiness, language inclusion, and language equivalence are all decidable; definability in monadic second order logic of nested words corresponds exactly to finite-state recognizability; and finiteness of the congruence induced by a language of nested words is a necessary and sufficient condition for regularity.},
	address = {Berlin, Heidelberg},
	author = {Alur, Rajeev and Madhusudan, P.},
	booktitle = {Developments in {Language} {Theory}},
	doi = {10.1007/11779148_1},
	editor = {Ibarra, Oscar H. and Dang, Zhe},
	file = {AlurR&MadhusudanP-Adding Nesting Structure to Words.pdf:/home/linusboyle/Zotero/storage/VB2FUSJ3/AlurR&MadhusudanP-Adding Nesting Structure to Words.pdf:application/pdf},
	isbn = {978-3-540-35430-7},
	keywords = {Read},
	language = {en},
	pages = {1--13},
	publisher = {Springer},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Adding {Nesting} {Structure} to {Words}},
	year = {2006}
}

@misc{heizmann_trace_2010,
	author = {Heizmann, Matthias and Hoenicke, Jochen and Podelski, Andreas},
	file = {HeizmannM et al-Trace Abstraction.pdf:/home/linusboyle/Zotero/storage/82GFN8GN/HeizmannM et al-Trace Abstraction.pdf:application/pdf},
	keywords = {Read},
	language = {en},
	title = {Trace {Abstraction}},
	year = {2010}
}

@inproceedings{leike_geometric_2018,
	abstract = {We present a new kind of nontermination argument, called geometric nontermination argument. The geometric nontermination argument is a finite representation of an infinite execution that has the form of a sum of several geometric series. For so-called linear lasso programs we can decide the existence of a geometric nontermination argument using a nonlinear algebraic \ensuremath{\exists}\ensuremath{\exists}{\textbackslash}exists -constraint. We show that a deterministic conjunctive loop program with nonnegative eigenvalues is nonterminating if an only if there exists a geometric nontermination argument. Furthermore, we present an evaluation that demonstrates that our method is feasible in practice.},
	address = {Cham},
	author = {Leike, Jan and Heizmann, Matthias},
	booktitle = {Tools and {Algorithms} for the {Construction} and {Analysis} of {Systems}},
	doi = {10.1007/978-3-319-89963-3_16},
	editor = {Beyer, Dirk and Huisman, Marieke},
	file = {LeikeJ&HeizmannM-Geometric Nontermination Arguments.pdf:/home/linusboyle/Zotero/storage/SBPW2P7E/LeikeJ&HeizmannM-Geometric Nontermination Arguments.pdf:application/pdf},
	isbn = {978-3-319-89963-3},
	keywords = {Read},
	language = {en},
	pages = {266--283},
	publisher = {Springer International Publishing},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Geometric {Nontermination} {Arguments}},
	year = {2018}
}

@article{gupta_proving_2008,
	abstract = {The search for proof and the search for counterexamples (bugs) are complementary activities that need to be pursued concurrently in order to maximize the practical success rate of verification tools.While this is well-understood in safety verification, the current focus of liveness verification has been almost exclusively on the search for termination proofs. A counterexample to termination is an infinite programexecution. In this paper, we propose a method to search for such counterexamples. The search proceeds in two phases. We first dynamically enumerate lasso-shaped candidate paths for counterexamples, and then statically prove their feasibility. We illustrate the utility of our nontermination prover, called TNT, on several nontrivial examples, some of which require bit-level reasoning about integer representations.},
	author = {Gupta, Ashutosh and Henzinger, Thomas A. and Majumdar, Rupak and Rybalchenko, Andrey and Xu, Ru-Gang},
	doi = {10.1145/1328897.1328459},
	file = {GuptaA et al-Proving non-termination-SIGPLAN Not..pdf:/home/linusboyle/Zotero/storage/XNEPAKIX/GuptaA et al-Proving non-termination-SIGPLAN Not..pdf:application/pdf},
	issn = {0362-1340},
	journal = {ACM SIGPLAN Notices},
	keywords = {Read, model checking, non-termination, program verification, recurrent sets, testing},
	month = jan,
	number = {1},
	pages = {147--158},
	title = {Proving non-termination},
	url = {https://doi.org/10.1145/1328897.1328459},
	urldate = {2020-12-28},
	volume = {43},
	year = {2008}
}

@inproceedings{chen_proving_2014,
	abstract = {We show how the problem of nontermination proving can be reduced to a question of underapproximation search guided by a safety prover. This reduction leads to new nontermination proving implementation strategies based on existing tools for safety proving. Our preliminary implementation beats existing tools. Furthermore, our approach leads to easy support for programs with unbounded nondeterminism.},
	address = {Berlin, Heidelberg},
	author = {Chen, Hong-Yi and Cook, Byron and Fuhs, Carsten and Nimkar, Kaustubh and O{\rq}Hearn, Peter},
	booktitle = {Tools and {Algorithms} for the {Construction} and {Analysis} of {Systems}},
	doi = {10.1007/978-3-642-54862-8_11},
	editor = {{\'A}brah{\'a}m, Erika and Havelund, Klaus},
	file = {ChenH et al-Proving Nontermination via Safety.pdf:/home/linusboyle/Zotero/storage/VFDTQ8MS/ChenH et al-Proving Nontermination via Safety.pdf:application/pdf},
	isbn = {978-3-642-54862-8},
	keywords = {Read, Memory State, Nest Loop, Outgoing Edge, Reachable State, Transition Relation},
	language = {en},
	pages = {156--171},
	publisher = {Springer},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Proving {Nontermination} via {Safety}},
	year = {2014}
}

@inproceedings{velroyen_non-termination_2008,
	abstract = {While termination checking tailored to real-world library code or frameworks has received ever-increasing attention during the last years, the complementary question of disproving termination properties as a means of debugging has largely been ignored so far. We present an approach to automatic non-termination checking that relates to termination checking in the same way as symbolic testing does to program verification. Our method is based on the automated generation of invariants that show that terminating states of a program are unreachable from certain initial states. Such initial states are identified using constraint-solving techniques. The method is fully implemented on top of a program verification system and available for download. We give an empirical evaluation of the approach using a collection of non-terminating example programs.},
	address = {Berlin, Heidelberg},
	author = {Velroyen, Helga and R{\"u}mmer, Philipp},
	booktitle = {Tests and {Proofs}},
	doi = {10.1007/978-3-540-79124-9_11},
	editor = {Beckert, Bernhard and H{\"a}hnle, Reiner},
	file = {VelroyenH&R{\"u}mmerP-Non-termination Checking for Imperative Programs.pdf:/home/linusboyle/Zotero/storage/Q7T9MWR6/VelroyenH&R{\"u}mmerP-Non-termination Checking for Imperative Programs.pdf:application/pdf},
	isbn = {978-3-540-79124-9},
	keywords = {Read, Dynamic Logic, Fibonacci Number, Open Goal, Proof Tree, Sequent Calculus},
	language = {en},
	pages = {154--170},
	publisher = {Springer},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Non-termination {Checking} for {Imperative} {Programs}},
	year = {2008}
}

@inproceedings{li_proving_2020,
	abstract = {The search for a proof of correctness and the search for counterexamples (bugs) are complementary aspects of verification. In order to maximize the practical use of verification tools it is better to pursue them at the same time. While this is well-understood in the termination analysis of programs, this is not the case for the language inclusion analysis of B{\"u}chi automata, where research mainly focused on improving algorithms for proving language inclusion, with the search for counterexamples left to the expensive complementation operation.In this paper, we present IMC2IMC2{\textbackslash}mathsf \{IMC\}{\textasciicircum}\{2\}, a specific algorithm for proving B{\"u}chi automata non-inclusion L(A)\ensuremath{\nsubseteq}L(B)L(A)\ensuremath{\nsubseteq}L(B){\textbackslash}mathcal \{L\}({\textbackslash}mathcal \{A\}) {\textbackslash}not {\textbackslash}subseteq {\textbackslash}mathcal \{L\}({\textbackslash}mathcal \{B\}), based on Grosu and Smolka{\rq}s algorithm MC2MC2{\textbackslash}mathsf \{MC\}{\textasciicircum}\{2\} developed for Monte Carlo model checking against LTL formulas. The algorithm we propose takes M=⌈ln\ensuremath{\delta}/ln(1−\ensuremath{\varepsilon})⌉M=⌈ln\ensuremath{\delta}/ln(1−\ensuremath{\varepsilon})⌉M = {\textbackslash}lceil {\textbackslash}ln {\textbackslash}delta /{\textbackslash}ln (1-{\textbackslash}varepsilon ) {\textbackslash}rceil random lasso-shaped samples from AA{\textbackslash}mathcal \{A\} to decide whether to reject the hypothesis L(A)\ensuremath{\nsubseteq}L(B)L(A)\ensuremath{\nsubseteq}L(B){\textbackslash}mathcal \{L\}({\textbackslash}mathcal \{A\}) {\textbackslash}not {\textbackslash}subseteq {\textbackslash}mathcal \{L\}({\textbackslash}mathcal \{B\}), for given error probability \ensuremath{\varepsilon}\ensuremath{\varepsilon}{\textbackslash}varepsilon and confidence level 1−\ensuremath{\delta}1−\ensuremath{\delta}1 - {\textbackslash}delta . With such a number of samples, IMC2IMC2{\textbackslash}mathsf \{IMC\}{\textasciicircum}\{2\} ensures that the probability of witnessing L(A)\ensuremath{\nsubseteq}L(B)L(A)\ensuremath{\nsubseteq}L(B){\textbackslash}mathcal \{L\}({\textbackslash}mathcal \{A\}) {\textbackslash}not {\textbackslash}subseteq {\textbackslash}mathcal \{L\}({\textbackslash}mathcal \{B\}) via further sampling is less than \ensuremath{\delta}\ensuremath{\delta}{\textbackslash}delta , under the assumption that the probability of finding a lasso counterexample is larger than \ensuremath{\varepsilon}\ensuremath{\varepsilon}{\textbackslash}varepsilon . Extensive experimental evaluation shows that IMC2IMC2{\textbackslash}mathsf \{IMC\}{\textasciicircum}\{2\} is a fast and reliable way to find counterexamples to B{\"u}chi automata inclusion.},
	address = {Cham},
	author = {Li, Yong and Turrini, Andrea and Sun, Xuechao and Zhang, Lijun},
	booktitle = {Automated {Technology} for {Verification} and {Analysis}},
	doi = {10.1007/978-3-030-59152-6_26},
	editor = {Hung, Dang Van and Sokolsky, Oleg},
	file = {LiY et al-Proving Non-inclusion of B{\"u}chi Automata Based on Monte Carlo Sampling.pdf:/home/linusboyle/Zotero/storage/TNRESIUY/LiY et al-Proving Non-inclusion of B{\"u}chi Automata Based on Monte Carlo Sampling.pdf:application/pdf},
	isbn = {978-3-030-59152-6},
	language = {en},
	pages = {467--483},
	publisher = {Springer International Publishing},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Proving {Non}-inclusion of {B{\"u}chi} {Automata} {Based} on {Monte} {Carlo} {Sampling}},
	year = {2020}
}

@article{le_dynamite_2020,
	abstract = {There is growing interest in termination reasoning for nonlinear programs and, meanwhile, recent dynamic strategies have shown they are able to infer invariants for such challenging programs. These advances led us to hypothesize that perhaps such dynamic strategies for nonlinear invariants could be adapted to learn recurrent sets (for non-termination) and/or ranking functions (for termination). In this paper, we exploit dynamic analysis and draw termination and non-termination as well as static and dynamic strategies closer together in order to tackle nonlinear programs. For termination, our algorithm infers ranking functions from concrete transitive closures, and, for non-termination, the algorithm iteratively collects executions and dynamically learns conditions to refine recurrent sets. Finally, we describe an integrated algorithm that allows these algorithms to mutually inform each other, taking counterexamples from a failed validation in one endeavor and crossing both the static/dynamic and termination/non-termination lines, to create new execution samples for the other one. We have implemented these algorithms in a new tool called DynamiTe. For nonlinear programs, there are currently no SV-COMP termination benchmarks so we created new sets of 38 terminating and 39 non-terminating programs. Our empirical evaluation shows that we can effectively guess (and sometimes even validate) ranking functions and recurrent sets for programs with nonlinear behaviors. Furthermore, we show that counterexamples from one failed validation can be used to generate executions for a dynamic analysis of the opposite property. Although we are focused on nonlinear programs, as a point of comparison, we compare DynamiTe's performance on linear programs with that of the state-of-the-art tool, Ultimate. Although DynamiTe is an order of magnitude slower it is nonetheless somewhat competitive and sometimes finds ranking functions where Ultimate was unable to. Ultimate cannot, however, handle the nonlinear programs in our new benchmark suite.},
	author = {Le, Ton Chanh and Antonopoulos, Timos and Fathololumi, Parisa and Koskinen, Eric and Nguyen, ThanhVu},
	doi = {10.1145/3428257},
	file = {LeT et al-DynamiTe - dynamic termination and non-termination proofs-Proc. ACM Program. Lang..pdf:/home/linusboyle/Zotero/storage/DIZEEER4/LeT et al-DynamiTe - dynamic termination and non-termination proofs-Proc. ACM Program. Lang..pdf:application/pdf},
	journal = {Proceedings of the ACM on Programming Languages},
	keywords = {Read, non-termination, dynamic analysis, termination},
	month = nov,
	number = {OOPSLA},
	pages = {189:1--189:30},
	shorttitle = {{DynamiTe}},
	title = {{DynamiTe}: dynamic termination and non-termination proofs},
	url = {https://doi.org/10.1145/3428257},
	urldate = {2021-01-02},
	volume = {4},
	year = {2020}
}

@inproceedings{cook_disproving_2014,
	abstract = {When disproving termination using known techniques (e.g. recurrence sets), abstractions that overapproximate the program{\rq}s transition relation are unsound. In this paper we introduce live abstractions, a natural class of abstractions that can be combined with the recent concept of closed recurrence sets to soundly disprove termination. To demonstrate the practical usefulness of this new approach we show how programs with nonlinear, nondeterministic, and heap-based commands can be shown nonterminating using linear overapproximations.},
	address = {Lausanne, Switzerland},
	author = {Cook, Byron and Fuhs, Carsten and Nimkar, Kaustubh and O'Hearn, Peter},
	booktitle = {2014 {Formal} {Methods} in {Computer}-{Aided} {Design} ({FMCAD})},
	doi = {10.1109/FMCAD.2014.6987597},
	file = {CookB et al-Disproving termination with overapproximation.pdf:/home/linusboyle/Zotero/storage/EVA9CP7S/CookB et al-Disproving termination with overapproximation.pdf:application/pdf},
	isbn = {978-0-9835678-4-4},
	keywords = {Read},
	language = {en},
	month = oct,
	pages = {67--74},
	publisher = {IEEE},
	title = {Disproving termination with overapproximation},
	url = {http://ieeexplore.ieee.org/document/6987597/},
	urldate = {2021-01-02},
	year = {2014}
}

@inproceedings{podelski_complete_2004,
	abstract = {We present an automated method for proving the termination of an unnested program loop by synthesizing linear ranking functions. The method is complete. Namely, if a linear ranking function exists then it will be discovered by our method. The method relies on the fact that we can obtain the linear ranking functions of the program loop as the solutions of a system of linear inequalities that we derive from the program loop. The method is used as a subroutine in a method for proving termination and other liveness properties of more general programs via transition invariants; see [PR03].},
	address = {Berlin, Heidelberg},
	author = {Podelski, Andreas and Rybalchenko, Andrey},
	booktitle = {Verification, {Model} {Checking}, and {Abstract} {Interpretation}},
	doi = {10.1007/978-3-540-24622-0_20},
	editor = {Steffen, Bernhard and Levi, Giorgio},
	file = {PodelskiA&RybalchenkoA-A Complete Method for the Synthesis of Linear Ranking Functions.pdf:/home/linusboyle/Zotero/storage/5EFPGWTF/PodelskiA&RybalchenkoA-A Complete Method for the Synthesis of Linear Ranking Functions.pdf:application/pdf},
	isbn = {978-3-540-24622-0},
	keywords = {Ranking Function, Transition Relation, Linear Inequality, Logic Program, Singular Value Decomposition},
	language = {en},
	pages = {239--251},
	publisher = {Springer},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {Complete} {Method} for the {Synthesis} of {Linear} {Ranking} {Functions}},
	year = {2004}
}

@inproceedings{brockschmidt_automated_2012,
	abstract = {Recently, we developed an approach for automated termination proofs of Java Bytecode (JBC), which is based on constructing and analyzing termination graphs. These graphs represent all possible program executions in a finite way. In this paper, we show that this approach can also be used to detect non-termination or NullPointerExceptions. Our approach automatically generates witnesses, i.e., calling the program with these witness arguments indeed leads to non-termination resp. to a NullPointerException. Thus, we never obtain ``false positives''. We implemented our results in the termination prover AProVE and provide experimental evidence for the power of our approach.},
	address = {Berlin, Heidelberg},
	author = {Brockschmidt, Marc and Str{\"o}der, Thomas and Otto, Carsten and Giesl, J{\"u}rgen},
	booktitle = {Formal {Verification} of {Object}-{Oriented} {Software}},
	doi = {10.1007/978-3-642-31762-0_9},
	editor = {Beckert, Bernhard and Damiani, Ferruccio and Gurov, Dilian},
	file = {BrockschmidtM et al-Automated Detection of Non-termination and NullPointerExceptions for Java Bytecode.pdf:/home/linusboyle/Zotero/storage/3963PRRW/BrockschmidtM et al-Automated Detection of Non-termination and NullPointerExceptions for Java Bytecode.pdf:application/pdf},
	isbn = {978-3-642-31762-0},
	keywords = {Outgoing Edge, Concrete State, Loop Condition, Symbolic Evaluation, Termination Graph},
	language = {en},
	pages = {123--141},
	publisher = {Springer},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Automated {Detection} of {Non}-termination and {NullPointerExceptions} for {Java} {Bytecode}},
	year = {2012}
}

@inproceedings{li_synthesizing_2019,
	abstract = {Termination of programs is probably the most famous undecidable problem in computer science. Despite this undecidability result, a lot of effort has been spent on improving algorithms that prove termination of loops, which is one of the building blocks of software reliability analysis. These algorithms are usually focused on finding an appropriate ranking function for the loop, which proves its termination. In this paper, we consider nested ranking functions for loop programs and show that the existence problem of a nested ranking function is equivalent to the existence problem of a hyperplane separating classes of data. This allows us to leverage Support-Vector Machines (SVM) techniques for the synthesis of nested ranking functions. SVM are supervised learning algorithms that are used to classify data; they work by finding a hyperplane separating data points parted into two classes. We show how to carefully define the data points so that the separating hyperplane gives rise to a nested ranking function for the loop. Experimental results confirm the effectiveness of our SVM-based synthesis of nested ranking functions.},
	address = {Cham},
	author = {Li, Yi and Sun, Xuechao and Li, Yong and Turrini, Andrea and Zhang, Lijun},
	booktitle = {Formal {Methods} and {Software} {Engineering}},
	doi = {10.1007/978-3-030-32409-4_27},
	editor = {Ait-Ameur, Yamine and Qin, Shengchao},
	file = {LiY et al-Synthesizing Nested Ranking Functions for Loop Programs via SVM.pdf:/home/linusboyle/Zotero/storage/KDAHWNWG/LiY et al-Synthesizing Nested Ranking Functions for Loop Programs via SVM.pdf:application/pdf},
	isbn = {978-3-030-32409-4},
	language = {en},
	pages = {438--454},
	publisher = {Springer International Publishing},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Synthesizing {Nested} {Ranking} {Functions} for {Loop} {Programs} via {SVM}},
	year = {2019}
}

@inproceedings{heizmann_software_2013,
	abstract = {In this expository paper, we use automata for software model checking in a new way. The starting point is to fix the alphabet: the set of statements of the given program. We show how automata over the alphabet of statements can help to decompose the main problem in software model checking, which is to find the right abstraction of a program for a given correctness property.},
	address = {Berlin, Heidelberg},
	author = {Heizmann, Matthias and Hoenicke, Jochen and Podelski, Andreas},
	booktitle = {Computer {Aided} {Verification}},
	doi = {10.1007/978-3-642-39799-8_2},
	editor = {Sharygina, Natasha and Veith, Helmut},
	file = {HeizmannM et al-Software Model Checking for People Who Love Automata.pdf:/home/linusboyle/Zotero/storage/JSR448MA/HeizmannM et al-Software Model Checking for People Who Love Automata.pdf:application/pdf},
	isbn = {978-3-642-39799-8},
	keywords = {Read},
	language = {en},
	pages = {36--52},
	publisher = {Springer},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Software {Model} {Checking} for {People} {Who} {Love} {Automata}},
	year = {2013}
}

@inproceedings{bakhirkin_finding_2016,
	abstract = {We propose an abstract-interpretation-based analysis for recurrent sets. A recurrent set is a set of states from which the execution of a program cannot or might not (as in our case) escape. A recurrent set is a part of a program{\rq}s non-termination proof (that needs to be complemented by reachability analysis). We find recurrent sets by performing a potentially over-approximate backward analysis that produces an initial candidate. We then perform over-approximate forward analysis on the candidate to check and refine it and ensure soundness. In practice, the analysis relies on trace partitioning that predicts future paths through the program that non-terminating executions will take. Using our technique, we were able to find recurrent sets in many benchmarks found in the literature including some that, to our knowledge, cannot be handled by existing tools. In addition, we note that typically, analyses that search for recurrent sets are applied to linear under-approximations of programs or employ some form of non-approximate numeric reasoning. In contrast, our analysis uses standard abstract-interpretation techniques and is potentially applicable to a larger class of abstract domains (and therefore -- programs).},
	address = {Berlin, Heidelberg},
	author = {Bakhirkin, Alexey and Piterman, Nir},
	booktitle = {Tools and {Algorithms} for the {Construction} and {Analysis} of {Systems}},
	doi = {10.1007/978-3-662-49674-9_2},
	editor = {Chechik, Marsha and Raskin, Jean-Fran{\c c}ois},
	file = {BakhirkinA&PitermanN-Finding Recurrent Sets with Backward Analysis and Trace Partitioning.pdf:/home/linusboyle/Zotero/storage/KX7UN9YJ/BakhirkinA&PitermanN-Finding Recurrent Sets with Backward Analysis and Trace Partitioning.pdf:application/pdf},
	isbn = {978-3-662-49674-9},
	keywords = {Memory State, Abstract Domain, Abstract Interpretation, Final Location, Program Variable},
	language = {en},
	pages = {17--35},
	publisher = {Springer},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Finding {Recurrent} {Sets} with {Backward} {Analysis} and {Trace} {Partitioning}},
	year = {2016}
}

@inproceedings{bakhirkin_forward_2015,
	abstract = {Non-termination of structured imperative programs is primarily due to infinite loops. An important class of non-terminating loop behaviors can be characterized using the notion of recurrent sets. A recurrent set is a set of states from which execution of the loop cannot or might not escape. Existing analyses that infer recurrent sets to our knowledge rely on one of: the combination of forward and backward analyses, quantifier elimination, or SMT-solvers. We propose a purely forward abstract interpretation--based analysis that can be used together with a possibly complicated abstract domain where none of the above is readily available. The analysis searches for a recurrent set of every individual loop in a program by building a graph of abstract states and analyzing it in a novel way. The graph is searched for a witness of a recurrent set that takes the form of what we call a recurrent component which is somewhat similar to the notion of an end component in a Markov decision process.},
	address = {Berlin, Heidelberg},
	author = {Bakhirkin, Alexey and Berdine, Josh and Piterman, Nir},
	booktitle = {Static {Analysis}},
	doi = {10.1007/978-3-662-48288-9_17},
	editor = {Blazy, Sandrine and Jensen, Thomas},
	file = {BakhirkinA et al-A Forward Analysis for Recurrent Sets.pdf:/home/linusboyle/Zotero/storage/K5Y87LIF/BakhirkinA et al-A Forward Analysis for Recurrent Sets.pdf:application/pdf},
	isbn = {978-3-662-48288-9},
	keywords = {Abstract Domain, Loop Body, Quantifier Elimination, Recurrent State, State Formula},
	language = {en},
	pages = {293--311},
	publisher = {Springer},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {Forward} {Analysis} for {Recurrent} {Sets}},
	year = {2015}
}

@inproceedings{larraz_proving_2014,
	abstract = {We show how Max-SMT-based invariant generation can be exploited for proving non-termination of programs. The construction of the proof of non-termination is guided by the generation of quasi-invariants -- properties such that if they hold at a location during execution once, then they will continue to hold at that location from then onwards. The check that quasi-invariants can indeed be reached is then performed separately. Our technique considers strongly connected subgraphs of a program{\rq}s control flow graph for analysis and thus produces more generic witnesses of non-termination than existing methods. Moreover, it can handle programs with unbounded non-determinism and is more likely to converge than previous approaches.},
	address = {Cham},
	author = {Larraz, Daniel and Nimkar, Kaustubh and Oliveras, Albert and Rodr{\'\i}guez-Carbonell, Enric and Rubio, Albert},
	booktitle = {Computer {Aided} {Verification}},
	doi = {10.1007/978-3-319-08867-9_52},
	editor = {Biere, Armin and Bloem, Roderick},
	file = {LarrazD et al-Proving Non-termination Using Max-SMT.pdf:/home/linusboyle/Zotero/storage/TF42X9WW/LarrazD et al-Proving Non-termination Using Max-SMT.pdf:application/pdf},
	isbn = {978-3-319-08867-9},
	keywords = {Read, Transition Relation, Program Variable, Hard Constraint, Invariant Generation, Soft Constraint},
	language = {en},
	pages = {779--796},
	publisher = {Springer International Publishing},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Proving {Non}-termination {Using} {Max}-{SMT}},
	year = {2014}
}

@article{de_moura_satisfiability_2011,
	abstract = {Checking the satisfiability of logical formulas, SMT solvers scale orders of magnitude beyond custom ad hoc solvers.},
	author = {{De Moura}, Leonardo and Bj{\o}rner, Nikolaj},
	doi = {10.1145/1995376.1995394},
	file = {De MouraL&Bj{\o}rnerN-Satisfiability modulo theories - introduction and applications-Commun. ACM.pdf:/home/linusboyle/Zotero/storage/W29UDT8N/De MouraL&Bj{\o}rnerN-Satisfiability modulo theories - introduction and applications-Commun. ACM.pdf:application/pdf},
	issn = {0001-0782},
	journal = {Communications of the ACM},
	month = sep,
	number = {9},
	pages = {69--77},
	shorttitle = {Satisfiability modulo theories},
	title = {Satisfiability modulo theories: introduction and applications},
	url = {https://doi.org/10.1145/1995376.1995394},
	urldate = {2021-01-11},
	volume = {54},
	year = {2011}
}

@inproceedings{le_resource-based_2014,
	abstract = {We propose a unified logical framework for specifying and proving both termination and non-termination of various programs. Our framework is based on a resource logic which captures both upper and lower bounds on resources used by the programs. By an abstraction, we evolve this resource logic for execution length into a temporal logic with three predicates to reason about termination, non-termination or unknown. We introduce a new logical entailment system for temporal constraints and show how Hoare logic can be seamlessly used to prove termination and non-termination in our unified framework. Though this paper{\rq}s focus is on the formal foundations for a new unified framework, we also report on the usability and practicality of our approach by specifying and verifying both termination and non-termination properties for about 300 programs, collected from a variety of sources. This adds a modest 5-10\% verification overhead when compared to underlying partial-correctness verification system.},
	address = {Cham},
	author = {Le, Ton Chanh and Gherghina, Cristian and Hobor, Aquinas and Chin, Wei-Ngan},
	booktitle = {Formal {Methods} and {Software} {Engineering}},
	doi = {10.1007/978-3-319-11737-9_18},
	editor = {Merz, Stephan and Pang, Jun},
	file = {LeT et al-A Resource-Based Logic for Termination and Non-termination Proofs.pdf:/home/linusboyle/Zotero/storage/LGHJXR9V/LeT et al-A Resource-Based Logic for Termination and Non-termination Proofs.pdf:application/pdf},
	isbn = {978-3-319-11737-9},
	keywords = {Read, Call Tree, Method Call, Resource Capacity, Temporal Constraint, Temporal Logic},
	language = {en},
	pages = {267--283},
	publisher = {Springer International Publishing},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {Resource}-{Based} {Logic} for {Termination} and {Non}-termination {Proofs}},
	year = {2014}
}

@inproceedings{le_termination_2015,
	abstract = {Techniques for proving termination and non-termination of imperative programs are usually considered as orthogonal mechanisms. In this paper, we propose a novel mechanism that analyzes and proves both program termination and non-termination at the same time. We first introduce the concept of second-order termination constraints and accumulate a set of relational assumptions on them via a Hoare-style verification. We then solve these assumptions with case analysis to determine the (conditional) termination and non- termination scenarios expressed in some specification logic form. In contrast to current approaches, our technique can construct a summary of terminating and non-terminating behaviors for each method. This enables modularity and reuse for our termination and non-termination proving processes. We have tested our tool on sample programs from a recent termination competition, and compared favorably against state-of-the-art termination analyzers.},
	address = {New York, NY, USA},
	author = {Le, Ton Chanh and Qin, Shengchao and Chin, Wei-Ngan},
	booktitle = {Proceedings of the 36th {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	doi = {10.1145/2737924.2737993},
	file = {LeT et al-Termination and non-termination specification inference.pdf:/home/linusboyle/Zotero/storage/UPCEC3EP/LeT et al-Termination and non-termination specification inference.pdf:application/pdf},
	isbn = {978-1-4503-3468-6},
	keywords = {Bi-abductive inference, Hoare logic, Program termination and non-termination analysis},
	month = jun,
	pages = {489--498},
	publisher = {Association for Computing Machinery},
	series = {{PLDI} '15},
	title = {Termination and non-termination specification inference},
	url = {https://doi.org/10.1145/2737924.2737993},
	urldate = {2021-01-11},
	year = {2015}
}

@article{ohearn_separation_2019,
	abstract = {Separation logic is a key development in formal reasoning about programs, opening up new lines of attack on longstanding problems.},
	author = {O'Hearn, Peter},
	doi = {10.1145/3211968},
	file = {O'HearnP-Separation logic-Commun. ACM.pdf:/home/linusboyle/Zotero/storage/UQVPY63U/O'HearnP-Separation logic-Commun. ACM.pdf:application/pdf},
	issn = {0001-0782},
	journal = {Communications of the ACM},
	keywords = {Read},
	month = jan,
	number = {2},
	pages = {86--95},
	title = {Separation logic},
	url = {https://doi.org/10.1145/3211968},
	urldate = {2021-01-12},
	volume = {62},
	year = {2019}
}

@inproceedings{fedyukovich_syntax-guided_2018,
	abstract = {We present new algorithms for proving program termination and non-termination using syntax-guided synthesis. They exploit the symbolic encoding of programs and automatically construct a formal grammar for symbolic constraints that are used to synthesize either a termination argument or a non-terminating program refinement. The constraints are then added back to the program encoding, and an off-the-shelf constraint solver decides on their fitness and on the progress of the algorithms. The evaluation of our implementation, called Freq-Term, shows that although the formal grammar is limited to the syntax of the program, in the majority of cases our algorithms are effective and fast. Importantly, FreqTerm is competitive with state-of-the-art on a wide range of terminating and non-terminating benchmarks, and it significantly outperforms state-of-the-art on proving non-termination of a class of programs arising from large-scale Event-Condition-Action systems.},
	address = {Cham},
	author = {Fedyukovich, Grigory and Zhang, Yueling and Gupta, Aarti},
	booktitle = {Computer {Aided} {Verification}},
	doi = {10.1007/978-3-319-96145-3_7},
	editor = {Chockler, Hana and Weissenbacher, Georg},
	file = {FedyukovichG et al-Syntax-Guided Termination Analysis.pdf:/home/linusboyle/Zotero/storage/8YNZICXG/FedyukovichG et al-Syntax-Guided Termination Analysis.pdf:application/pdf},
	isbn = {978-3-319-96145-3},
	keywords = {Read, Proving Program Termination, Symbolic Encoding, Syntax-guided Synthesis (SyGuS), Term Freq, Termination Argument},
	language = {en},
	pages = {124--143},
	publisher = {Springer International Publishing},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Syntax-{Guided} {Termination} {Analysis}},
	year = {2018}
}

@inproceedings{borralleras_proving_2017,
	abstract = {We present a constraint-based method for proving conditional termination of integer programs. Building on this, we construct a framework to prove (unconditional) program termination using a powerful mechanism to combine conditional termination proofs. Our key insight is that a conditional termination proof shows termination for a subset of program execution states which do not need to be considered in the remaining analysis. This facilitates more effective termination as well as non-termination analyses, and allows handling loops with different execution phases naturally. Moreover, our method can deal with sequences of loops compositionally. In an empirical evaluation, we show that our implementation VeryMax outperforms state-of-the-art tools on a range of standard benchmarks.},
	address = {Berlin, Heidelberg},
	author = {Borralleras, Cristina and Brockschmidt, Marc and Larraz, Daniel and Oliveras, Albert and Rodr{\'\i}guez-Carbonell, Enric and Rubio, Albert},
	booktitle = {Tools and {Algorithms} for the {Construction} and {Analysis} of {Systems}},
	doi = {10.1007/978-3-662-54577-5_6},
	editor = {Legay, Axel and Margaria, Tiziana},
	file = {BorrallerasC et al-Proving Termination Through Conditional Termination.pdf:/home/linusboyle/Zotero/storage/55SEGVKJ/BorrallerasC et al-Proving Termination Through Conditional Termination.pdf:application/pdf},
	isbn = {978-3-662-54577-5},
	keywords = {Read, Ranking Function, Entry Transition, Program Component, Program Transformation, Satisfiability Modulo Theory},
	language = {en},
	pages = {99--117},
	publisher = {Springer},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Proving {Termination} {Through} {Conditional} {Termination}},
	year = {2017}
}

@inproceedings{harris_alternation_2010,
	abstract = {Proving termination of sequential programs is an important problem, both for establishing the total correctness of systems and as a component of proving more general termination and liveness properties. We present a new algorithm, TRex, that determines if a sequential program terminates on all inputs. The key characteristic of TRex is that it alternates between refining an over-approximation and an under-approximation of each loop in a sequential program. In order to prove termination, TRex maintains an over-approximation of the set of states that can be reached at the head of the loop. In order to prove non-termination, it maintains an under-approximation of the set of paths through the body of the loop. The over-approximation and under-approximation are used to refine each other iteratively, and help TRex to arrive quickly at a proof of either termination or non-termination.TRex refines the approximations in alternation by composing three different program analyses: (1) local termination provers that can quickly handle intricate loops, but not whole programs, (2) non-termination provers that analyze one cycle through a loop, but not all paths, and (3) global safety provers that can check safety properties of large programs, but cannot check liveness properties. This structure allows TRex to be instantiated using any of the pre-existing techniques for proving termination or non-termination of individual loops.We evaluated TRex by applying it to prove termination or find bugs for a set of real-world programs and termination analysis benchmarks. Our results demonstrate that alternation allows TRex to prove termination or produce certified termination bugs more effectively than previous techniques.},
	address = {Berlin, Heidelberg},
	author = {Harris, William R. and Lal, Akash and Nori, Aditya V. and Rajamani, Sriram K.},
	booktitle = {Static {Analysis}},
	doi = {10.1007/978-3-642-15769-1_19},
	editor = {Cousot, Radhia and Martel, Matthieu},
	file = {HarrisW et al-Alternation for Termination.pdf:/home/linusboyle/Zotero/storage/TX3AZZDY/HarrisW et al-Alternation for Termination.pdf:application/pdf},
	isbn = {978-3-642-15769-1},
	keywords = {Read, Termination Proof, Transition Invariant, Nest Loop, Function Call, Sequential Program},
	language = {en},
	pages = {304--319},
	publisher = {Springer},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Alternation for {Termination}},
	year = {2010}
}

@inproceedings{podelski_transition_2004,
	abstract = {Proof rules for program verification rely on auxiliary assertions. We propose a (sound and relatively complete) proof rule whose auxiliary assertions are transition invariants. A transition invariant of a program is a binary relation over program states that contains the transitive closure of the transition relation of the program. A relation is disjunctively well-founded if it is a finite union of well-founded relations. We characterize the validity of termination or another liveness property by the existence of a disjunctively well-founded transition invariant. The main contribution of our proof rule lies in its potential for automation via abstract interpretation.},
	address = {Turku, Finland},
	author = {Podelski, A. and Rybalchenko, A.},
	booktitle = {Proceedings of the 19th {Annual} {IEEE} {Symposium} on {Logic} in {Computer} {Science}, 2004.},
	doi = {10.1109/LICS.2004.1319598},
	file = {PodelskiA&RybalchenkoA-Transition invariants.pdf:/home/linusboyle/Zotero/storage/DCV4C2QP/PodelskiA&RybalchenkoA-Transition invariants.pdf:application/pdf},
	isbn = {978-0-7695-2192-3},
	language = {en},
	pages = {32--41},
	publisher = {IEEE},
	title = {Transition invariants},
	url = {http://ieeexplore.ieee.org/document/1319598/},
	urldate = {2021-01-19},
	year = {2004}
}

@inproceedings{kuwahara_predicate_2015,
	abstract = {We propose an automated method for disproving termination of higher-order functional programs. Our method combines higher-order model checking with predicate abstraction and CEGAR. Our predicate abstraction is novel in that it computes a mixture of under- and overapproximations. For non-determinism of a source program (such as random number generation), we apply underapproximation to generate a subset of the actual branches, and check that some of the branches in the abstract program is non-terminating. For operations on infinite data domains (such as integers), we apply overapproximation to generate a superset of the actual branches, and check that every branch is non-terminating. Thus, disproving non-termination reduces to the problem of checking a certain branching property of the abstract program, which can be solved by higher-order model checking. We have implemented a prototype non-termination prover based on our method and have confirmed the effectiveness of the proposed approach through experiments.},
	address = {Cham},
	author = {Kuwahara, Takuya and Sato, Ryosuke and Unno, Hiroshi and Kobayashi, Naoki},
	booktitle = {Computer {Aided} {Verification}},
	doi = {10.1007/978-3-319-21668-3_17},
	editor = {Kroening, Daniel and P{\u a}s{\u a}reanu, Corina S.},
	file = {KuwaharaT et al-Predicate Abstraction and CEGAR for Disproving Termination of Higher-Order Functional Programs.pdf:/home/linusboyle/Zotero/storage/3XZME3MT/KuwaharaT et al-Predicate Abstraction and CEGAR for Disproving Termination of Higher-Order Functional Programs.pdf:application/pdf},
	isbn = {978-3-319-21668-3},
	keywords = {Execution Path, Horn Clause, Predicate Abstraction, Predicate Variable, Source Program},
	language = {en},
	pages = {287--303},
	publisher = {Springer International Publishing},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Predicate {Abstraction} and {CEGAR} for {Disproving} {Termination} of {Higher}-{Order} {Functional} {Programs}},
	year = {2015}
}

@inproceedings{frohn_proving_2019,
	abstract = {We present the first approach to prove non-termination of integer programs that is based on loop acceleration. If our technique cannot show non-termination of a loop, it tries to accelerate it instead in order to find paths to other non-terminating loops automatically. The prerequisites for our novel loop acceleration technique generalize a simple yet effective non-termination criterion. Thus, we can use the same program transformations to facilitate both non-termination proving and loop acceleration. In particular, we present a novel invariant inference technique that is tailored to our approach. An extensive evaluation of our fully automated tool LoAT shows that it is competitive with the state of the art.},
	author = {Frohn, F. and Giesl, J.},
	booktitle = {2019 {Formal} {Methods} in {Computer} {Aided} {Design} ({FMCAD})},
	doi = {10.23919/FMCAD.2019.8894271},
	file = {FrohnF&GieslJ-Proving Non-Termination via Loop Acceleration.pdf:/home/linusboyle/Zotero/storage/ZB3SFSNP/FrohnF&GieslJ-Proving Non-Termination via Loop Acceleration.pdf:application/pdf},
	keywords = {Safety, program verification, Acceleration, Cost accounting, inference mechanisms, Informatics, integer programming, invariant inference technique, LoAT tool, loop acceleration technique, nonterminating loops, nontermination criterion, nontermination proving, program diagnostics, Program processors, program transformation, Standards, Tools},
	month = oct,
	note = {ISSN: 2642-732X},
	pages = {221--230},
	title = {Proving {Non}-{Termination} via {Loop} {Acceleration}},
	year = {2019}
}

@inproceedings{hosseini_termination_2019,
	address = {Dagstuhl, Germany},
	author = {Hosseini, Mehran and Ouaknine, Jo{\"e}l and Worrell, James},
	booktitle = {46th {International} {Colloquium} on {Automata}, {Languages}, and {Programming} ({ICALP} 2019)},
	doi = {10.4230/LIPIcs.ICALP.2019.118},
	editor = {Baier, Christel and Chatzigiannakis, Ioannis and Flocchini, Paola and Leonardi, Stefano},
	file = {HosseiniM et al-Termination of Linear Loops over the Integers.pdf:/home/linusboyle/Zotero/storage/LHI5P5DU/HosseiniM et al-Termination of Linear Loops over the Integers.pdf:application/pdf},
	isbn = {978-3-95977-109-2},
	keywords = {Affine While Loops, Linear Integer Programs, Loop Termination, Program Verification},
	note = {ISSN: 1868-8969},
	pages = {118:1--118:13},
	publisher = {Schloss Dagstuhl--Leibniz-Zentrum fuer Informatik},
	series = {Leibniz {International} {Proceedings} in {Informatics} ({LIPIcs})},
	title = {Termination of {Linear} {Loops} over the {Integers}},
	url = {http://drops.dagstuhl.de/opus/volltexte/2019/10694},
	urldate = {2021-02-16},
	volume = {132},
	year = {2019}
}

@inproceedings{cimatti_proving_2021,
	abstract = {In finite-state systems, true existential properties admit witnesses in form of lasso-shaped fair paths. When dealing with the infinite-state case (e.g. software non-termination, model checking of hybrid automata) this is no longer the case. In this paper, we propose a compositional approach for proving the existence of fair paths of infinite-state systems. First, we describe a formal approach to prove the existence of a non-empty under-approximation of the original system that only contains fair paths. Second, we define an automated procedure that, given a set of hints (in form of basic components), searches for a suitable composition proving the existence of a fair path. We experimentally evaluate the approach on examples taken from both software and hybrid systems, showing its wide applicability and expressiveness.},
	address = {Cham},
	author = {Cimatti, Alessandro and Griggio, Alberto and Magnago, Enrico},
	booktitle = {Verification, {Model} {Checking}, and {Abstract} {Interpretation}},
	doi = {10.1007/978-3-030-67067-2_6},
	editor = {Henglein, Fritz and Shoham, Sharon and Vizel, Yakir},
	file = {CimattiA et al-Proving the Existence of Fair Paths in Infinite-State Systems.pdf:/home/linusboyle/Zotero/storage/EYQJTY92/CimattiA et al-Proving the Existence of Fair Paths in Infinite-State Systems.pdf:application/pdf},
	isbn = {978-3-030-67067-2},
	keywords = {Read},
	language = {en},
	pages = {104--126},
	publisher = {Springer International Publishing},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Proving the {Existence} of {Fair} {Paths} in {Infinite}-{State} {Systems}},
	year = {2021}
}

@article{diekert_first-order_2008,
	author = {Diekert, Volker and Gastin, Paul},
	file = {DiekertV&GastinP-First-order definable languages.pdf:/home/linusboyle/Zotero/storage/6LJARMYW/DiekertV&GastinP-First-order definable languages.pdf:application/pdf},
	journal = {Logic and automata},
	pages = {261--306},
	title = {First-order definable languages.},
	volume = {2},
	year = {2008}
}

@phdthesis{bakhirkin_recurrent_2016,
	abstract = {Termination and non-termination are a pair of fundamental program properties. Arguably, the majority of code is required to terminate, e.g., dispatch routines of drivers or other event-driven code, GPU programs, etc -- and the existence of non-terminating executions is a serious bug. Such a bug may manifest by freezing a device or an entire system, or by causing a multi-region cloud service disruption. Thus, proving termination is an interesting problem in the process of establishing correctness, and proving non-termination is a complementary problem that is interesting for debugging. This work considers a sub-problem of proving non-termination -- the problem of finding recurrent sets. A recurrent set is a way to compactly represent the set of nonterminating executions of a program and is a set of states from which an execution of the program cannot or may not escape (there exist multiple definitions that differ in modalities). A recurrent set acts as a part of a non-termination proof. If we find a nonempty recurrent set and are able to show its reachability from an initial state -- then we prove the existence of a non-terminating execution. Most part of this work is devoted to automated static analyses that find recurrent sets in imperative programs. We follow the general framework of abstract interpretation and go all the way from trace semantics of programs to practical analyses that compute abstract representations of recurrent sets. In particular, we present two novel analyses. The first one is based on abstract pre-condition computation (backward analysis) and trace partitioning and focuses on numeric programs (but with some modifications it may be applicable to non-numeric ones). In popular benchmarks, it performs comparably to state-of-the-art tools. The second analysis is based on abstract post-condition computation (forward analysis) and is readily applicable to non-numeric (e.g., heap-manipulating) programs, which we demonstrate by tackling examples from the domain of shape analysis with 3-valued logic. As it turns out, recurrent sets can be used in establishing other properties as well. For example, recurrent sets are used in CTL model checking of programs. And as part of this work, we were able to apply recurrent sets in the process of establishing sufficient pre-conditions for safety.},
	author = {Bakhirkin, Alexey},
	file = {BakhirkinA-Recurrent Sets for Non-Termination and Safety of Programs.pdf:/home/linusboyle/Zotero/storage/6N4MQ6XD/BakhirkinA-Recurrent Sets for Non-Termination and Safety of Programs.pdf:application/pdf},
	language = {en},
	month = sep,
	school = {University of Leicester},
	title = {Recurrent {Sets} for {Non}-{Termination} and {Safety} of {Programs}},
	type = {Phd {Thesis}},
	url = {https://leicester.figshare.com/articles/thesis/Recurrent_Sets_for_Non-Termination_and_Safety_of_Programs/10122392},
	urldate = {2021-02-19},
	year = {2016}
}

@inproceedings{larraz_proving_2013,
	abstract = {We show how Max-SMT can be exploited in constraint-based program termination proving. Thanks to expressing the generation of a ranking function as a Max-SMT optimization problem where constraints are assigned different weights, quasi-ranking functions -functions that almost satisfy all conditions for ensuring well-foundedness- are produced in a lack of ranking functions. By means of trace partitioning, this allows our method to progress in the termination analysis where other approaches would get stuck. Moreover, Max-SMT makes it easy to combine the process of building the termination argument with the usually necessary task of generating supporting invariants. The method has been implemented in a prototype that has successfully been tested on a wide set of programs.},
	author = {Larraz, D. and Oliveras, A. and Rodr{\'\i}guez-Carbonell, E. and Rubio, A.},
	booktitle = {2013 {Formal} {Methods} in {Computer}-{Aided} {Design}},
	doi = {10.1109/FMCAD.2013.6679413},
	file = {LarrazD et al-Proving termination of imperative programs using Max-SMT.pdf:/home/linusboyle/Zotero/storage/TDP558J9/LarrazD et al-Proving termination of imperative programs using Max-SMT.pdf:application/pdf},
	keywords = {Optimization, theorem proving, Read, Computer bugs, Buildings, computability, constraint handling, constraint-based program termination proving, Generators, imperative programs, max-SMT optimization problem, optimisation, Pressing, Prototypes, quasiranking functions, Software, termination analysis, termination argument, trace partitioning},
	month = oct,
	pages = {218--225},
	title = {Proving termination of imperative programs using {Max}-{SMT}},
	year = {2013}
}

@inproceedings{chen_proving_2020-1,
	abstract = {We propose a novel approach to proving the termination of imperative programs by k-induction. By our approach, the termination proving problem can be formalized as a k-inductive invariant synthesis task. On the one hand, k-induction uses weaker invariants than that required by the standard inductive approach. On the other hand, the base case of k-induction, which unrolls the program, can provide stronger pre-condition for invariant synthesis. As a result, the termination arguments of our approach can be synthesized more efficiently than the standard method. We implement a prototype of our k-inductive approach. The experimental results show the significant effectiveness and efficiency of our approach.},
	address = {New York, NY, USA},
	author = {Chen, Jianhui and He, Fei},
	booktitle = {Proceedings of the 35th {IEEE}/{ACM} international conference on automated software engineering},
	doi = {10.1145/3324884.3418929},
	isbn = {978-1-4503-6768-4},
	keywords = {invariant synthesis, k-induction, proving termination},
	note = {Number of pages: 5 Place: Virtual Event, Australia},
	pages = {1239--1243},
	publisher = {Association for Computing Machinery},
	series = {{ASE} '20},
	title = {Proving termination by k-{Induction}},
	url = {https://doi.org/10.1145/3324884.3418929},
	year = {2020}
}

@inproceedings{giesl_termination_2019,
	abstract = {The termination and complexity competition (termCOMP) focuses on automated termination and complexity analysis for various kinds of programming paradigms, including categories for term rewriting, integer transition systems, imperative programming, logic programming, and functional programming. In all categories, the competition also welcomes the participation of tools providing certifiable output. The goal of the competition is to demonstrate the power and advances of the state-of-the-art tools in each of these areas.},
	address = {Cham},
	author = {Giesl, J{\"u}rgen and Rubio, Albert and Sternagel, Christian and Waldmann, Johannes and Yamada, Akihisa},
	booktitle = {Tools and {Algorithms} for the {Construction} and {Analysis} of {Systems}},
	doi = {10.1007/978-3-030-17502-3_10},
	editor = {Beyer, Dirk and Huisman, Marieke and Kordon, Fabrice and Steffen, Bernhard},
	file = {GieslJ et al-The Termination and Complexity Competition.pdf:/home/linusboyle/Zotero/storage/PS5GS2AZ/GieslJ et al-The Termination and Complexity Competition.pdf:application/pdf},
	isbn = {978-3-030-17502-3},
	language = {en},
	pages = {156--166},
	publisher = {Springer International Publishing},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {The {Termination} and {Complexity} {Competition}},
	year = {2019}
}

@article{albert_resource_2019,
	abstract = {When programs feature a complex control flow, existing techniques for resource analysis produce cost relation systems (CRS) whose cost functions retain the complex flow of the program and, consequently, might not be solvable into closed-form upper bounds. This paper presents a novel approach to resource analysis that is driven by the result of a termination analysis. The fundamental idea is that the termination proof encapsulates the flows of the program which are relevant for the cost computation so that, by driving the generation of the CRS using the termination proof, we produce a linearly-bounded CRS (LB-CRS). A LB-CRS is composed of cost functions that are guaranteed to be locally bounded by linear ranking functions and thus greatly simplify the process of CRS solving. We have built a new resource analysis tool, named MaxCore, that is guided by the VeryMax termination analyzer and uses CoFloCo and PUBS as CRS solvers. Our experimental results on the set of benchmarks from the Complexity and Termination Competition 2019 for C Integer programs show that MaxCore outperforms all other resource analysis tools.},
	author = {Albert, Elvira and Bofill, Miquel and Borralleras, Cristina and Martin-Martin, Enrique and Rubio, Albert},
	doi = {10.1017/S1471068419000152},
	file = {AlbertE et al-Resource Analysis driven by (Conditional) Termination Proofs.pdf:/home/linusboyle/Zotero/storage/XAIVVR22/AlbertE et al-Resource Analysis driven by (Conditional) Termination Proofs.pdf:application/pdf},
	issn = {1471-0684, 1475-3081},
	journal = {Theory and Practice of Logic Programming},
	keywords = {termination analysis, cost relation systems, resource analysis, upper bounds},
	language = {en},
	month = sep,
	note = {Publisher: Cambridge University Press},
	number = {5-6},
	pages = {722--739},
	title = {Resource {Analysis} driven by ({Conditional}) {Termination} {Proofs}},
	url = {https://www.cambridge.org/core/journals/theory-and-practice-of-logic-programming/article/resource-analysis-driven-by-conditional-termination-proofs/5677CB44260C20D72C140D0427BEB4BE#},
	urldate = {2021-03-02},
	volume = {19},
	year = {2019}
}

@misc{justin_bailey_haskell_nodate,
	abstract = {Learning Haskell is not easy. Besides the syntax, concepts, and advanced types, there is a real lack of succinct, accessible references. As I learned Haskell I frequently wanted a quick reference for syntax, keywords and other language elements. The Haskell Report, while very thorough, wasn't quite it.

For that reason I've created this cheatsheet. It's intended for beginning to intermediate Haskell programmers to use as a quick-reference guide for syntax, keywords or other language issues.},
	author = {{Justin Bailey}},
	language = {en},
	title = {Haskell {Cheatsheet}},
	url = {cheatsheet.codeslower.com}
}

@article{bozga_deciding_2014,
	abstract = {We address the problem of conditional termination, which is that of defining the set of initial configurations from which a given program always terminates. First we define the dual set, of initial configurations from which a non-terminating execution exists, as the greatest fixpoint of the function that maps a set of states into its pre-image with respect to the transition relation. This definition allows to compute the weakest non-termination precondition if at least one of the following holds: (i) the transition relation is deterministic, (ii) the descending Kleene sequence overapproximating the greatest fixpoint converges in finitely many steps, or (iii) the transition relation is well founded. We show that this is the case for two classes of relations, namely octagonal and finite monoid affine relations. Moreover, since the closed forms of these relations can be defined in Presburger arithmetic, we obtain the decidability of the termination problem for such loops.},
	author = {Bozga, Marius and Konecny, Filip and Iosif, Radu},
	file = {BozgaM et al-Deciding Conditional Termination.pdf:/home/linusboyle/Zotero/storage/L5NMYMTS/BozgaM et al-Deciding Conditional Termination.pdf:application/pdf},
	journal = {Logical Methods in Computer Science},
	language = {en},
	month = aug,
	note = {Publisher: Episciences.org},
	title = {Deciding {Conditional} {Termination}},
	url = {https://lmcs.episciences.org/737/pdf},
	urldate = {2021-03-04},
	volume = {Volume 10, Issue 3},
	year = {2014}
}

@inproceedings{ben-amram_multiphase-linear_2019,
	abstract = {Multiphase ranking functions (M\ensuremath{\Phi}\ensuremath{\Phi}{\textbackslash}varPhi RFs) are used to prove termination of loops in which the computation progresses through a number of phases. They consist of linear functions ⟨f1,{\ldots},fd⟩⟨f1,{\ldots},fd⟩{\textbackslash}langle f\_1,{\textbackslash}ldots ,f\_d {\textbackslash}rangle where fifif\_i decreases during the ith phase. This work provides new insights regarding M\ensuremath{\Phi}\ensuremath{\Phi}{\textbackslash}varPhi RFs for loops described by a conjunction of linear constraints (SLCSLC SLC loops). In particular, we consider the existence problem (does a given SLCSLC SLC loop admit a M\ensuremath{\Phi}\ensuremath{\Phi}{\textbackslash}varPhi RF). The decidability and complexity of the problem, in the case that d is restricted by an input parameter, have been settled in recent work, while in this paper we make progress regarding the existence problem without a given depth bound. Our new approach, while falling short of a decision procedure for the general case, reveals some important insights into the structure of these functions. Interestingly, it relates the problem of seeking M\ensuremath{\Phi}\ensuremath{\Phi}{\textbackslash}varPhi RFs to that of seeking recurrent sets (used to prove nontermination). It also helps in identifying classes of loops for which M\ensuremath{\Phi}\ensuremath{\Phi}{\textbackslash}varPhi RFs are sufficient, and thus have linear runtime bounds. For the depth-bounded existence problem, we obtain a new polynomial-time procedure that can provide witnesses for negative answers as well. To obtain this procedure we introduce a new representation for SLCSLC SLC loops, the difference polyhedron replacing the customary transition polyhedron. We find that this representation yields new insights on M\ensuremath{\Phi}\ensuremath{\Phi}{\textbackslash}varPhi RFs and SLCSLC SLC loops in general, and some results on termination and nontermination of bounded SLCSLC SLC loops become straightforward.},
	address = {Cham},
	author = {Ben-Amram, Amir M. and Dom{\'e}nech, Jes{\'u}s J. and Genaim, Samir},
	booktitle = {Static {Analysis}},
	doi = {10.1007/978-3-030-32304-2_22},
	editor = {Chang, Bor-Yuh Evan},
	file = {Ben-AmramA et al-Multiphase-Linear Ranking Functions and Their Relation to Recurrent Sets.pdf:/home/linusboyle/Zotero/storage/GBNB3IZ3/Ben-AmramA et al-Multiphase-Linear Ranking Functions and Their Relation to Recurrent Sets.pdf:application/pdf},
	isbn = {978-3-030-32304-2},
	language = {en},
	pages = {459--480},
	publisher = {Springer International Publishing},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Multiphase-{Linear} {Ranking} {Functions} and {Their} {Relation} to {Recurrent} {Sets}},
	year = {2019}
}

@inproceedings{cook_proving_2008,
	abstract = {We describe a method for synthesizing reasonable underapproximations to weakest preconditions for termination---a long-standing open problem. The paper provides experimental evidence to demonstrate the usefulness of the new procedure.},
	address = {Berlin, Heidelberg},
	author = {Cook, Byron and Gulwani, Sumit and Lev-Ami, Tal and Rybalchenko, Andrey and Sagiv, Mooly},
	booktitle = {Computer {Aided} {Verification}},
	doi = {10.1007/978-3-540-70545-1_32},
	editor = {Gupta, Aarti and Malik, Sharad},
	file = {CookB et al-Proving Conditional Termination.pdf:/home/linusboyle/Zotero/storage/US58FXNE/CookB et al-Proving Conditional Termination.pdf:application/pdf},
	isbn = {978-3-540-70545-1},
	keywords = {Read, Ranking Function, Transition Relation, Code Fragment, Concurrent Program, Recursive Call},
	language = {en},
	pages = {328--340},
	publisher = {Springer},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Proving {Conditional} {Termination}},
	year = {2008}
}

@inproceedings{larraz_minimal-model-guided_2014,
	abstract = {In this paper we present new methods for deciding the satisfiability of formulas involving integer polynomial constraints. In previous work we proposed to solve SMT(NIA) problems by reducing them to SMT(LIA): non-linear monomials are linearized by abstracting them with fresh variables and by performing case splitting on integer variables with finite domain. When variables do not have finite domains, artificial ones can be introduced by imposing a lower and an upper bound, and made iteratively larger until a solution is found (or the procedure times out). For the approach to be practical, unsatisfiable cores are used to guide which domains have to be relaxed (i.e., enlarged) from one iteration to the following one. However, it is not clear then how large they have to be made, which is critical.Here we propose to guide the domain relaxation step by analyzing minimal models produced by the SMT(LIA) solver. Namely, we consider two different cost functions: the number of violated artificial domain bounds, and the distance with respect to the artificial domains. We compare these approaches with other techniques on benchmarks coming from constraint-based program analysis and show the potential of the method. Finally, we describe how one of these minimal-model-guided techniques can be smoothly adapted to deal with the extension Max-SMT of SMT(NIA) and then applied to program termination proving.},
	address = {Cham},
	author = {Larraz, Daniel and Oliveras, Albert and Rodr{\'\i}guez-Carbonell, Enric and Rubio, Albert},
	booktitle = {Theory and {Applications} of {Satisfiability} {Testing} -- {SAT} 2014},
	doi = {10.1007/978-3-319-09284-3_25},
	editor = {Sinz, Carsten and Egly, Uwe},
	file = {LarrazD et al-Minimal-Model-Guided Approaches to Solving Polynomial Constraints and Extensions.pdf:/home/linusboyle/Zotero/storage/KXJIZK23/LarrazD et al-Minimal-Model-Guided Approaches to Solving Polynomial Constraints and Extensions.pdf:application/pdf},
	isbn = {978-3-319-09284-3},
	keywords = {Ranking Function, Cylindrical Algebraic Decomposition, Fresh Variable, Minimal Model, Soft Clause},
	language = {en},
	pages = {333--350},
	publisher = {Springer International Publishing},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Minimal-{Model}-{Guided} {Approaches} to {Solving} {Polynomial} {Constraints} and {Extensions}},
	year = {2014}
}

@inproceedings{slaby_checking_2012,
	abstract = {We introduce a novel technique for checking properties described by finite state machines. The technique is based on a synergy of three well-known methods: instrumentation, program slicing, and symbolic execution. More precisely, we instrument a given program with a code that tracks runs of state machines representing various properties. Next we slice the program to reduce its size without affecting runs of state machines. And then we symbolically execute the sliced program to find real violations of the checked properties, i.e. real bugs. Depending on the kind of symbolic execution, the technique can be applied as a stand-alone bug finding technique, or to weed out some false positives from an output of another bug-finding tool. We provide several examples demonstrating the practical applicability of our technique.},
	address = {Berlin, Heidelberg},
	author = {Slab{\'y}, Ji{\v r}{\'\i} and Strej{\v c}ek, Jan and Trt{\'\i}k, Marek},
	booktitle = {Formal {Methods} for {Industrial} {Critical} {Systems}},
	doi = {10.1007/978-3-642-32469-7_14},
	editor = {Stoelinga, Mari{\"e}lle and Pinger, Ralf},
	file = {Slab{\'y}J et al-Checking Properties Described by State Machines - On Synergy of Instrumentation, Slicing, and.pdf:/home/linusboyle/Zotero/storage/5Y2WCF9Y/Slab{\'y}J et al-Checking Properties Described by State Machines - On Synergy of Instrumentation, Slicing, and.pdf:application/pdf},
	isbn = {978-3-642-32469-7},
	keywords = {Execution Path, Error Report, Real Error, State Machine, Symbolic Execution},
	language = {en},
	pages = {207--221},
	publisher = {Springer},
	series = {Lecture {Notes} in {Computer} {Science}},
	shorttitle = {Checking {Properties} {Described} by {State} {Machines}},
	title = {Checking {Properties} {Described} by {State} {Machines}: {On} {Synergy} of {Instrumentation}, {Slicing}, and {Symbolic} {Execution}},
	year = {2012}
}

@article{chen_bit-precise_2017,
	abstract = {Non-termination is the root cause of a variety of program bugs, such as hanging programs and denial-of-service vulnerabilities. This makes an automated analysis that can prove the absence of such bugs highly desirable. To scale termination checks to large systems, an interprocedural termination analysis seems essential. This is a largely unexplored area of research in termination analysis, where most effort has focussed on small but difficult single-procedure problems. We present a modular termination analysis for C programs using template-based interprocedural summarisation. Our analysis combines a context-sensitive, over-approximating forward analysis with the inference of under-approximating preconditions for termination. Bit-precise termination arguments are synthesised over lexicographic linear ranking function templates. Our experimental results show the advantage of interprocedural reasoning over monolithic analysis in terms of efficiency, while retaining comparable precision.},
	author = {Chen, Hong-Yi and David, Cristina and Kroening, Daniel and Schrammel, Peter and Wachter, Bj{\"o}rn},
	doi = {10.1145/3121136},
	file = {ChenH et al-Bit-Precise Procedure-Modular Termination Analysis-ACM Trans. Program. Lang. Syst..pdf:/home/linusboyle/Zotero/storage/QPIVHJQY/ChenH et al-Bit-Precise Procedure-Modular Termination Analysis-ACM Trans. Program. Lang. Syst..pdf:application/pdf},
	issn = {0164-0925},
	journal = {ACM Transactions on Programming Languages and Systems},
	keywords = {Read, Termination analysis, bit-precise analysis, interprocedural analysis, templates},
	month = dec,
	number = {1},
	pages = {1:1--1:38},
	title = {Bit-{Precise} {Procedure}-{Modular} {Termination} {Analysis}},
	url = {https://doi.org/10.1145/3121136},
	urldate = {2021-03-09},
	volume = {40},
	year = {2017}
}

@article{gulwani_speed_2009,
	abstract = {This paper describes an inter-procedural technique for computing symbolic bounds on the number of statements a procedure executes in terms of its scalar inputs and user-defined quantitative functions of input data-structures. Such computational complexity bounds for even simple programs are usually disjunctive, non-linear, and involve numerical properties of heaps. We address the challenges of generating these bounds using two novel ideas. We introduce a proof methodology based on multiple counter instrumentation (each counter can be initialized and incremented at potentially multiple program locations) that allows a given linear invariant generation tool to compute linear bounds individually on these counter variables. The bounds on these counters are then composed together to generate total bounds that are non-linear and disjunctive. We also give an algorithm for automating this proof methodology. Our algorithm generates complexity bounds that are usually precise not only in terms of the computational complexity, but also in terms of the constant factors. Next, we introduce the notion of user-defined quantitative functions that can be associated with abstract data-structures, e.g., length of a list, height of a tree, etc. We show how to compute bounds in terms of these quantitative functions using a linear invariant generation tool that has support for handling uninterpreted functions. We show application of this methodology to commonly used data-structures (namely lists, list of lists, trees, bit-vectors) using examples from Microsoft product code. We observe that a few quantitative functions for each data-structure are usually sufficient to allow generation of symbolic complexity bounds of a variety of loops that iterate over these data-structures, and that it is straightforward to define these quantitative functions. The combination of these techniques enables generation of precise computational complexity bounds for real-world examples (drawn from Microsoft product code and C++ STL library code) for some of which it is non-trivial to even prove termination. Such automatically generated bounds are very useful for early detection of egregious performance problems in large modular codebases that are constantly being changed by multiple developers who make heavy use of code written by others without a good understanding of their implementation complexity.},
	author = {Gulwani, Sumit and Mehra, Krishna K. and Chilimbi, Trishul},
	doi = {10.1145/1594834.1480898},
	file = {GulwaniS et al-SPEED - precise and efficient static estimation of program computational complexity-SIGPLAN Not..pdf:/home/linusboyle/Zotero/storage/XVBQBSTL/GulwaniS et al-SPEED - precise and efficient static estimation of program computational complexity-SIGPLAN Not..pdf:application/pdf},
	issn = {0362-1340},
	journal = {ACM SIGPLAN Notices},
	keywords = {counter instrumentation, quantitative functions, Read, symbolic complexity bounds, termination analysis},
	month = jan,
	number = {1},
	pages = {127--139},
	shorttitle = {{SPEED}},
	title = {{SPEED}: precise and efficient static estimation of program computational complexity},
	url = {https://doi.org/10.1145/1594834.1480898},
	urldate = {2021-03-13},
	volume = {44},
	year = {2009}
}

@inproceedings{gulwani_control-flow_2009,
	abstract = {Symbolic complexity bounds help programmers understand the performance characteristics of their implementations. Existing work provides techniques for statically determining bounds of procedures with simple control-flow. However, procedures with nested loops or multiple paths through a single loop are challenging. In this paper we describe two techniques, control-flow refinement and progress invariants, that together enable estimation of precise bounds for procedures with nested and multi-path loops. Control-flow refinement transforms a multi-path loop into a semantically equivalent code fragment with simpler loops by making the structure of path interleaving explicit. We show that this enables non-disjunctive invariant generation tools to find a bound on many procedures for which previous techniques were unable to prove termination. Progress invariants characterize relationships between consecutive states that can arise at a program location. We further present an algorithm that uses progress invariants to compute precise bounds for nested loops. The utility of these two techniques goes beyond our application to symbolic bound analysis. In particular, we discuss applications of control-flow refinement to proving safety properties that otherwise require disjunctive invariants. We have applied our methodology to over 670,000 lines of code of a significant Microsoft product and were able to find symbolic bounds for 90\% of the loops. We are not aware of any other published results that report experiences running a bound analysis on a real code-base.},
	address = {{New York, NY, USA}},
	author = {Gulwani, Sumit and Jain, Sagar and Koskinen, Eric},
	booktitle = {Proceedings of the 30th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
	doi = {10.1145/1542476.1542518},
	file = {/home/linusboyle/Zotero/storage/NHX76YYV/GulwaniS et al-Control-flow refinement and progress invariants for bound analysis.pdf},
	isbn = {978-1-60558-392-1},
	keywords = {bound analysis, control-flow refinement, formal verification, program verification, progress invariants, Read, termination},
	month = jun,
	pages = {375--385},
	publisher = {{Association for Computing Machinery}},
	series = {{{PLDI}} '09},
	title = {Control-Flow Refinement and Progress Invariants for Bound Analysis},
	year = {2009}
}

@article{gulwani_speed_2016,
	abstract = {The SPEED project addresses the problem of computing symbolic computational complexity bounds of procedures in terms of their inputs. We discuss some of the challenges that arise and present various orthogonal/complementary techniques recently developed in the SPEED project for addressing these challenges.},
	author = {Gulwani, Sumit},
	file = {GulwaniS-SPEED - Symbolic Complexity Bound Analysis.pdf:/home/linusboyle/Zotero/storage/MUUEWWUM/GulwaniS-SPEED - Symbolic Complexity Bound Analysis.pdf:application/pdf},
	language = {en-US},
	month = dec,
	shorttitle = {{SPEED}},
	title = {{SPEED}: {Symbolic} {Complexity} {Bound} {Analysis}},
	url = {https://www.microsoft.com/en-us/research/publication/speed-symbolic-complexity-bound-analysis/},
	urldate = {2021-03-13},
	year = {2016}
}

@inproceedings{alias_multi-dimensional_2010,
	abstract = {Proving the termination of a flowchart program can be done by exhibiting a ranking function, i.e., a function from the program states to a well-founded set, which strictly decreases at each program step. A standard method to automatically generate such a function is to compute invariants for each program point and to search for a ranking in a restricted class of functions that can be handled with linear programming techniques. Previous algorithms based on affine rankings either are applicable only to simple loops (i.e., single-node flowcharts) and rely on enumeration, or are not complete in the sense that they are not guaranteed to find a ranking in the class of functions they consider, if one exists. Our first contribution is to propose an efficient algorithm to compute ranking functions: It can handle flowcharts of arbitrary structure, the class of candidate rankings it explores is larger, and our method, although greedy, is provably complete. Our second contribution is to show how to use the ranking functions we generate to get upper bounds for the computational complexity (number of transitions) of the source program. This estimate is a polynomial, which means that we can handle programs with more than linear complexity. We applied the method on a collection of test cases from the literature. We also show the links and differences with previous techniques based on the insertion of counters.},
	address = {Berlin, Heidelberg},
	author = {Alias, Christophe and Darte, Alain and Feautrier, Paul and Gonnord, Laure},
	booktitle = {Static {Analysis}},
	doi = {10.1007/978-3-642-15769-1_8},
	editor = {Cousot, Radhia and Martel, Matthieu},
	file = {AliasC et al-Multi-dimensional Rankings, Program Termination, and Complexity Bounds of Flowchart Programs.pdf:/home/linusboyle/Zotero/storage/EJ2JXBF5/AliasC et al-Multi-dimensional Rankings, Program Termination, and Complexity Bounds of Flowchart Programs.pdf:application/pdf},
	isbn = {978-3-642-15769-1},
	keywords = {Abstract Interpretation, Control Point, Execution Trace, Program Termination, Ranking Function, Read},
	language = {en},
	pages = {117--133},
	publisher = {Springer},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Multi-dimensional {Rankings}, {Program} {Termination}, and {Complexity} {Bounds} of {Flowchart} {Programs}},
	year = {2010}
}

@inproceedings{sinn_simple_2014,
	abstract = {We present the first scalable bound analysis that achieves amortized complexity analysis. In contrast to earlier work, our bound analysis is not based on general purpose reasoners such as abstract interpreters, software model checkers or computer algebra tools. Rather, we derive bounds directly from abstract program models, which we obtain from programs by comparatively simple invariant generation and symbolic execution techniques. As a result, we obtain an analysis that is more predictable and more scalable than earlier approaches. We demonstrate by a thorough experimental evaluation that our analysis is fast and at the same time able to compute bounds for challenging loops in a large real-world benchmark. Technically, our approach is based on lossy vector addition systems (VASS). Our bound analysis first computes a lexicographic ranking function that proves the termination of a VASS, and then derives a bound from this ranking function. Our methodology achieves amortized analysis based on a new insight how lexicographic ranking functions can be used for bound analysis.},
	address = {Cham},
	author = {Sinn, Moritz and Zuleger, Florian and Veith, Helmut},
	booktitle = {Computer {Aided} {Verification}},
	doi = {10.1007/978-3-319-08867-9_50},
	editor = {Biere, Armin and Bloem, Roderick},
	file = {SinnM et al-A Simple and Scalable Static Analysis for Bound Analysis and Amortized Complexity Analysis.pdf:/home/linusboyle/Zotero/storage/WHRYRWS4/SinnM et al-A Simple and Scalable Static Analysis for Bound Analysis and Amortized Complexity Analysis.pdf:application/pdf},
	isbn = {978-3-319-08867-9},
	keywords = {Abstract Domain, Innermost Loop, Outer Loop, Ranking Function, Read, Transition Predicate},
	language = {en},
	pages = {745--761},
	publisher = {Springer International Publishing},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {Simple} and {Scalable} {Static} {Analysis} for {Bound} {Analysis} and {Amortized} {Complexity} {Analysis}},
	year = {2014}
}

@inproceedings{urban_synthesizing_2016,
	abstract = {In this work, we present a novel approach based on recent advances in software model checking to synthesize ranking functions and prove termination (and non-termination) of imperative programs.Our approach incrementally refines a termination argument from an under-approximation of the terminating program state. Specifically, we learn bits of information from terminating executions, and from these we extrapolate ranking functions over-approximating the number of loop iterations needed for termination. We combine these pieces into piecewise-defined, lexicographic, or multiphase ranking functions.The proposed technique has been implemented in SeaHorn -- an LLVM based verification framework -- targeting C code. Preliminary experimental evaluation demonstrated its effectiveness in synthesizing ranking functions and proving termination of C programs.},
	address = {Berlin, Heidelberg},
	author = {Urban, Caterina and Gurfinkel, Arie and Kahsai, Temesghen},
	booktitle = {Tools and {Algorithms} for the {Construction} and {Analysis} of {Systems}},
	doi = {10.1007/978-3-662-49674-9_4},
	editor = {Chechik, Marsha and Raskin, Jean-Fran{\c c}ois},
	file = {UrbanC et al-Synthesizing Ranking Functions from Bits and Pieces.pdf:/home/linusboyle/Zotero/storage/GNRES396/UrbanC et al-Synthesizing Ranking Functions from Bits and Pieces.pdf:application/pdf},
	isbn = {978-3-662-49674-9},
	keywords = {Affine Function, Entry Transition, Loop Iteration, Ranking Function, Read, Safety Property},
	language = {en},
	pages = {54--70},
	publisher = {Springer},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Synthesizing {Ranking} {Functions} from {Bits} and {Pieces}},
	year = {2016}
}

@unpublished{leino_this_2016,
	abstract = {Boogie is an intermediate verification language, designed to make the prescription of verification conditions natural and convenient. It serves as a common intermediate representation for static program verifiers of various source languages, and it abstracts over the interfaces to various theorem provers. Boogie can also be used as a shared input and output format for [{\ldots}]},
	address = {Microsoft},
	author = {Leino, K. Rustan M.},
	file = {LeinoK-This is Boogie 2.pdf:/home/linusboyle/Zotero/storage/IGBURHUR/LeinoK-This is Boogie 2.pdf:application/pdf},
	keywords = {⛔ No DOI found},
	language = {en-US},
	month = dec,
	title = {This is {Boogie} 2},
	type = {working draft},
	url = {https://www.microsoft.com/en-us/research/publication/this-is-boogie-2-2/},
	urldate = {2021-04-06},
	year = {2016}
}

@article{chatterjee_proving_2021,
	abstract = {We present a new approach to proving non-termination of non-deterministic integer programs. Our technique is rather simple but efficient. It relies on a purely syntactic reversal of the program's transition system followed by a constraint-based invariant synthesis with constraints coming from both the original and the reversed transition system. The latter task is performed by a simple call to an off-the-shelf SMT-solver, which allows us to leverage the latest advances in SMT-solving. Moreover, our method offers a combination of features not present (as a whole) in previous approaches: it handles programs with non-determinism, provides relative completeness guarantees and supports programs with polynomial arithmetic. The experiments performed with our prototype tool RevTerm show that our approach, despite its simplicity and stronger theoretical guarantees, is at least on par with the state-of-the-art tools, often achieving a non-trivial improvement under a proper configuration of its parameters.},
	author = {Chatterjee, Krishnendu and Goharshady, Ehsan Kafshdar and Novotn{\'y}, Petr and {\v Z}ikeli{\'c}, {\DJ}or{\dj}e},
	file = {ChatterjeeK et al-Proving Non-termination by Program Reversal_annotated.pdf:/home/linusboyle/Zotero/storage/WG7JLTCI/ChatterjeeK et al-Proving Non-termination by Program Reversal_annotated.pdf:application/pdf; ChatterjeeK et al-Proving Non-termination by Program Reversal_Note.pdf:/home/linusboyle/Zotero/storage/MNKK3WNP/ChatterjeeK et al-Proving Non-termination by Program Reversal_Note.pdf:application/pdf; ChatterjeeK et al-Proving Non-termination by Program Reversal.pdf:/home/linusboyle/Zotero/storage/PAF4J23E/ChatterjeeK et al-Proving Non-termination by Program Reversal.pdf:application/pdf},
	journal = {arXiv:2104.01189 [cs]},
	keywords = {⛔ No DOI found, Read, Computer Science - Programming Languages},
	month = apr,
	note = {arXiv: 2104.01189 version: 1},
	title = {Proving {Non}-termination by {Program} {Reversal}},
	url = {http://arxiv.org/abs/2104.01189},
	urldate = {2021-04-11},
	year = {2021}
}

@misc{jane_mcalevey_blowout_2021,
	author = {{Jane McAlevey}},
	file = {Jane McAlevey-Blowout in Bessemer - 亞馬遜倉儲工人工會選舉後記 失敗警訊無所不在.pdf:/home/linusboyle/Zotero/storage/99RVVC2H/Jane McAlevey-Blowout in Bessemer - 亞馬遜倉儲工人工會選舉後記 失敗警訊無所不在.pdf:application/pdf},
	month = apr,
	title = {Blowout in {Bessemer}: 亞馬遜倉儲工人工會選舉後記 失敗警訊無所不在},
	year = {2021}
}

@misc{moritz_sinn_loopus_2010,
	author = {{Moritz Sinn}},
	file = {Moritz Sinn-Loopus - A Tool for Computing Loop Bounds for C Programs.pdf:/home/linusboyle/Zotero/storage/MKFJL7H8/Moritz Sinn-Loopus - A Tool for Computing Loop Bounds for C Programs.pdf:application/pdf},
	keywords = {Read},
	title = {Loopus - {A} {Tool} for {Computing} {Loop} {Bounds} for {C} {Programs}},
	year = {2010}
}

@inproceedings{sinn_difference_2015,
	abstract = {Difference constraints have been used for termination analysis in the literature, where they denote relational inequalities of the form x' \ensuremath{\leq} y + c, and describe that the value of x in the current state is at most the value of y in the previous state plus some constant c \ensuremath{\in} Z. In this paper, we argue that the complexity of imperative programs typically arises from counter increments and resets, which can be modeled naturally by difference constraints. We present the first practical algorithm for the analysis of difference constraint programs and describe how C programs can be abstracted to difference constraint programs. Our approach contributes to the field of automated complexity and (resource) bound analysis by enabling automated amortized complexity analysis for a new class of programs and providing a conceptually simple program model that relates invariant- and bound analysis. We demonstrate the effectiveness of our approach through a thorough experimental comparison on real world C code: our tool Loopus computes the complexity for considerably more functions in less time than related tools from the literature.},
	address = {Austin, Texas},
	author = {Sinn, Moritz and Zuleger, Florian and Veith, Helmut},
	booktitle = {Proceedings of the 15th {Conference} on {Formal} {Methods} in {Computer}-{Aided} {Design}},
	file = {SinnM et al-Difference constraints - an adequate abstraction for complexity analysis of imperative programs.pdf:/home/linusboyle/Zotero/storage/KSFCNQ6Z/SinnM et al-Difference constraints - an adequate abstraction for complexity analysis of imperative programs.pdf:application/pdf},
	isbn = {978-0-9835678-5-1},
	month = sep,
	pages = {144--151},
	publisher = {FMCAD Inc},
	series = {{FMCAD} '15},
	shorttitle = {Difference constraints},
	title = {Difference constraints: an adequate abstraction for complexity analysis of imperative programs},
	urldate = {2021-05-10},
	year = {2015}
}

@article{sinn_complexity_2017,
	abstract = {Difference constraints have been used for termination analysis in the literature, where they denote relational inequalities of the form \$\$x' {\textbackslash}le y + c\$\$, and describe that the value of x in the current state is at most the value of y in the previous state plus some constant \$\$c {\textbackslash}in {\textbackslash}mathbb \{Z\}\$\$. We believe that difference constraints are also a good choice for complexity and resource bound analysis because the complexity of imperative programs typically arises from counter increments and resets, which can be modeled naturally by difference constraints. In this article we propose a bound analysis based on difference constraints. We make the following contributions: (1) our analysis handles bound analysis problems of high practical relevance which current approaches cannot handle: we extend the range of bound analysis to a class of challenging but natural loop iteration patterns which typically appear in parsing and string-matching routines. (2) We advocate the idea of using bound analysis to infer invariants: our soundness proven algorithm obtains invariants through bound analysis, the inferred invariants are in turn used for obtaining bounds. Our bound analysis therefore does not rely on external techniques for invariant generation. (3) We demonstrate that difference constraints are a suitable abstract program model for automatic complexity and resource bound analysis: we provide efficient abstraction techniques for obtaining difference constraint programs from imperative code. (4) We report on a thorough experimental comparison of state-of-the-art bound analysis tools: we set up a tool comparison on (a) a large benchmark of real-world C code, (b) a benchmark built of examples taken from the bound analysis literature and (c) a benchmark of challenging iteration patterns which we found in real source code. (5) Our analysis is more scalable than existing approaches: we discuss how we achieve scalability.},
	author = {Sinn, Moritz and Zuleger, Florian and Veith, Helmut},
	doi = {10.1007/s10817-016-9402-4},
	file = {SinnM et al-Complexity and Resource Bound Analysis of Imperative Programs Using Difference Constraints-J Autom Reasoning.pdf:/home/linusboyle/Zotero/storage/AT7BLSUJ/SinnM et al-Complexity and Resource Bound Analysis of Imperative Programs Using Difference Constraints-J Autom Reasoning.pdf:application/pdf; Soundness Proof:/home/linusboyle/Zotero/storage/WL8YBA4L/Soundness Proof.pdf:application/pdf},
	issn = {1573-0670},
	journal = {Journal of Automated Reasoning},
	language = {en},
	month = jun,
	number = {1},
	pages = {3--45},
	title = {Complexity and {Resource} {Bound} {Analysis} of {Imperative} {Programs} {Using} {Difference} {Constraints}},
	url = {https://doi.org/10.1007/s10817-016-9402-4},
	urldate = {2021-05-11},
	volume = {59},
	year = {2017}
}

@inproceedings{zuleger_bound_2011,
	abstract = {The size-change abstraction (SCA) is an important program abstraction for termination analysis, which has been successfully implemented in many tools for functional and logic programs. In this paper, we demonstrate that SCA is also a highly effective abstract domain for the bound analysis of imperative programs. We have implemented a bound analysis tool based on SCA for imperative programs. We abstract programs in a pathwise and context dependent manner, which enables our tool to analyze real-world programs effectively. Our work shows that SCA captures many of the essential ideas of previous termination and bound analysis and goes beyond in a conceptually simpler framework.},
	address = {Berlin, Heidelberg},
	author = {Zuleger, Florian and Gulwani, Sumit and Sinn, Moritz and Veith, Helmut},
	booktitle = {Proceedings of the 18th international conference on {Static} analysis},
	isbn = {978-3-642-23701-0},
	month = sep,
	pages = {280--297},
	publisher = {Springer-Verlag},
	series = {{SAS}'11},
	title = {Bound analysis of imperative programs with the size-change abstraction},
	urldate = {2021-05-20},
	year = {2011}
}

@article{alpern_recognizing_1987,
	author = {Alpern, Bowen and Schneider, Fred B.},
	doi = {10.1007/BF01782772},
	file = {AlpernB&SchneiderF-Recognizing safety and liveness-Distrib Comput.pdf:/home/linusboyle/Zotero/storage/4A22DERU/AlpernB&SchneiderF-Recognizing safety and liveness-Distrib Comput.pdf:application/pdf},
	issn = {0178-2770, 1432-0452},
	journal = {Distributed Computing},
	keywords = {Classic},
	language = {en},
	month = sep,
	number = {3},
	pages = {117--126},
	title = {Recognizing safety and liveness},
	url = {http://link.springer.com/10.1007/BF01782772},
	urldate = {2021-05-25},
	volume = {2},
	year = {1987}
}

@inproceedings{brockschmidt_better_2013,
	abstract = {One of the difficulties of proving program termination is managing the subtle interplay between the finding of a termination argument and the finding of the argument{\rq}s supporting invariant. In this paper we propose a new mechanism that facilitates better cooperation between these two types of reasoning. In an experimental evaluation we find that our new method leads to dramatic performance improvements.},
	address = {Berlin, Heidelberg},
	author = {Brockschmidt, Marc and Cook, Byron and Fuhs, Carsten},
	booktitle = {Computer {Aided} {Verification}},
	doi = {10.1007/978-3-642-39799-8_28},
	editor = {Sharygina, Natasha and Veith, Helmut},
	file = {BrockschmidtM et al-Better Termination Proving through Cooperation.pdf:/home/linusboyle/Zotero/storage/AJ3CLUNJ/BrockschmidtM et al-Better Termination Proving through Cooperation.pdf:application/pdf},
	isbn = {978-3-642-39799-8},
	keywords = {Termination Proof, Model Check, Dependency Pair, Proof Search, Strongly Connect Component},
	language = {en},
	pages = {413--429},
	publisher = {Springer},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Better {Termination} {Proving} through {Cooperation}},
	year = {2013}
}

@inproceedings{brockschmidt_alternating_2014,
	abstract = {We present a modular approach to automatic complexity analysis. Based on a novel alternation between finding symbolic time bounds for program parts and using these to infer size bounds on program variables, we can restrict each analysis step to a small part of the program while maintaining a high level of precision. Extensive experiments with the implementation of our method demonstrate its performance and power in comparison with other tools.},
	address = {Berlin, Heidelberg},
	author = {Brockschmidt, Marc and Emmes, Fabian and Falke, Stephan and Fuhs, Carsten and Giesl, J{\"u}rgen},
	booktitle = {Tools and {Algorithms} for the {Construction} and {Analysis} of {Systems}},
	doi = {10.1007/978-3-642-54862-8_10},
	editor = {{\'A}brah{\'a}m, Erika and Havelund, Klaus},
	file = {BrockschmidtM et al-Alternating Runtime and Size Complexity Analysis of Integer Programs.pdf:/home/linusboyle/Zotero/storage/AHXUSTZ6/BrockschmidtM et al-Alternating Runtime and Size Complexity Analysis of Integer Programs.pdf:application/pdf},
	isbn = {978-3-642-54862-8},
	keywords = {Complexity Bound, Integer Program, Program Part, Program Variable, Topological Order},
	language = {en},
	pages = {140--155},
	publisher = {Springer},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Alternating {Runtime} and {Size} {Complexity} {Analysis} of {Integer} {Programs}},
	year = {2014}
}

@article{wilhelm_worst-case_2008,
	abstract = {The determination of upper bounds on execution times, commonly called worst-case execution times (WCETs), is a necessary step in the development and validation process for hard real-time systems. This problem is hard if the underlying processor architecture has components, such as caches, pipelines, branch prediction, and other speculative components. This article describes different approaches to this problem and surveys several commercially available tools1 and research prototypes.},
	author = {Wilhelm, Reinhard and Engblom, Jakob and Ermedahl, Andreas and Holsti, Niklas and Thesing, Stephan and Whalley, David and Bernat, Guillem and Ferdinand, Christian and Heckmann, Reinhold and Mitra, Tulika and Mueller, Frank and Puaut, Isabelle and Puschner, Peter and Staschulat, Jan and Stenstr{\"o}m, Per},
	doi = {10.1145/1347375.1347389},
	file = {WilhelmR et al-The worst-case execution-time problem&#x2014\; overview of methods and survey of tools-ACM Trans. Embed. Comput. Syst..pdf:/home/linusboyle/Zotero/storage/UCKYJ8YN/WilhelmR et al-The worst-case execution-time problem&#x2014\; overview of methods and survey of tools-ACM Trans. Embed. Comput. Syst..pdf:application/pdf},
	issn = {1539-9087},
	journal = {ACM Transactions on Embedded Computing Systems},
	keywords = {Hard real time, worst-case execution times},
	month = may,
	number = {3},
	pages = {36:1--36:53},
	title = {The worst-case execution-time problem : overview of methods and survey of tools},
	url = {https://doi.org/10.1145/1347375.1347389},
	urldate = {2021-05-31},
	volume = {7},
	year = {2008}
}

@article{turing_computable_1937,
	author = {Turing, A. M.},
	doi = {10.1112/plms/s2-42.1.230},
	issn = {0024-6115},
	journal = {Proceedings of the London Mathematical Society},
	month = jan,
	number = {1},
	pages = {230--265},
	title = {On {Computable} {Numbers}, with an {Application} to the {Entscheidungsproblem}},
	url = {https://doi.org/10.1112/plms/s2-42.1.230},
	urldate = {2021-05-31},
	volume = {s2-42},
	year = {1937}
}

@inproceedings{smith_foundations_2009,
	abstract = {There is growing interest in quantitative theories of information flow in a variety of contexts, such as secure information flow, anonymity protocols, and side-channel analysis. Such theories offer an attractive way to relax the standard noninterference properties, letting us tolerate ``small'' leaks that are necessary in practice. The emerging consensus is that quantitative information flow should be founded on the concepts of Shannon entropy and mutual information. But a useful theory of quantitative information flow must provide appropriate security guarantees: if the theory says that an attack leaks x bits of secret information, then x should be useful in calculating bounds on the resulting threat. In this paper, we focus on the threat that an attack will allow the secret to be guessed correctly in one try. With respect to this threat model, we argue that the consensus definitions actually fail to give good security guarantees---the problem is that a random variable can have arbitrarily large Shannon entropy even if it is highly vulnerable to being guessed. We then explore an alternative foundation based on a concept of vulnerability (closely related to Bayes risk) and which measures uncertainty using R{\'e}nyi{\rq}s min-entropy, rather than Shannon entropy.},
	address = {Berlin, Heidelberg},
	author = {Smith, Geoffrey},
	booktitle = {Foundations of {Software} {Science} and {Computational} {Structures}},
	doi = {10.1007/978-3-642-00596-1_21},
	editor = {de Alfaro, Luca},
	file = {SmithG-On the Foundations of Quantitative Information Flow.pdf:/home/linusboyle/Zotero/storage/7HC262IV/SmithG-On the Foundations of Quantitative Information Flow.pdf:application/pdf},
	isbn = {978-3-642-00596-1},
	keywords = {Equivalence Class, Mutual Information, Probabilistic Program, Shannon Entropy, Threat Model},
	language = {en},
	pages = {288--302},
	publisher = {Springer},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {On the {Foundations} of {Quantitative} {Information} {Flow}},
	year = {2009}
}

@article{kirchner_frama-c_2015,
	author = {Kirchner, Florent and Kosmatov, Nikolai and Prevosto, Virgile and Signoles, Julien and Yakobowski, Boris},
	doi = {10.1007/s00165-014-0326-7},
	issn = {0934-5043, 1433-299X},
	journal = {Formal Aspects of Computing},
	language = {en},
	month = may,
	number = {3},
	pages = {573--609},
	shorttitle = {Frama-{C}},
	title = {Frama-{C}: {A} software analysis perspective},
	url = {http://link.springer.com/10.1007/s00165-014-0326-7},
	urldate = {2021-05-31},
	volume = {27},
	year = {2015}
}

